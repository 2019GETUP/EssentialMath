{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "6673c556-9250-4f2e-b2cc-6b7d821a0d2a",
    "_uuid": "ce40e46efe0115d00094a094423aabc3edc80766"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../input/Melbourne_housing_extra_data-18-08-2017.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4fa2d81e6e7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Melbourne_housing_extra_data-18-08-2017.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#%% Using NaN data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../input/Melbourne_housing_extra_data-18-08-2017.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "path = '../data/'\n",
    "filename = \"Melbourne_housing_FULL.csv\"\n",
    "\n",
    "data = pd.read_csv(path + filename)\n",
    "\n",
    "#%% Using NaN data\n",
    "## about 22% of the data contains missing values. How do we use them \n",
    "## nonetheless? This approach replaces their value with the mean of the\n",
    "## column in which they are; better would be to also do it according to\n",
    "## regions and other parameters of influence\n",
    "\n",
    "# only keep prices that we know of\n",
    "data = data.dropna(axis = 0, how = 'any', subset = [\"Price\"])\n",
    "\n",
    "\n",
    "\n",
    "X = data[\"Bathroom\"].values.reshape(-1, 1)\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "data[\"Bathroom\"] = X\n",
    "\n",
    "X = data[\"Car\"].values.reshape(-1, 1)\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "data[\"Car\"] = X\n",
    "\n",
    "X = data[\"Bedroom2\"].values.reshape(-1, 1)\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "data[\"Bedroom2\"] = X\n",
    "\n",
    "X = data[\"Propertycount\"].values.reshape(-1, 1)\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "data[\"Propertycount\"] = X\n",
    "\n",
    "X = data[\"BuildingArea\"].values.reshape(-1, 1)\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "data[\"BuildingArea\"] = X\n",
    "\n",
    "X = data[\"Landsize\"].values.reshape(-1, 1)\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "data[\"Landsize\"] = X\n",
    "\n",
    "X = data[\"Lattitude\"].values.reshape(-1, 1)\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "data[\"Lattitude\"] = X\n",
    "\n",
    "X = data[\"Longtitude\"].values.reshape(-1, 1)\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "data[\"Longtitude\"] = X\n",
    "\n",
    "X = data[\"Distance\"].values.reshape(-1, 1)\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "data[\"Distance\"] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c1956b8d-0627-419a-a9ee-333e5dd5e945",
    "_uuid": "65c5301cc830868da50412a6ef6cd9f4b96786e0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% Using proper data types\n",
    "## some features have incorrect data types, such as Bathroom, Bedroom2, Car\n",
    "## \n",
    "\n",
    "data[\"Bathroom\"] = data[\"Bathroom\"].astype('int64')\n",
    "data[\"Bedroom2\"] = data[\"Bedroom2\"].astype('int64')\n",
    "data[\"Car\"] = data[\"Car\"].astype('int64')\n",
    "data[\"Propertycount\"] = data[\"Propertycount\"].astype('int64')\n",
    "\n",
    "\n",
    "#%% Feature selection\n",
    "## some features are moslty NaN and would be difficult to use for\n",
    "## prediction; CouncilArea, YearBuilt\n",
    "## some others have many NaNs but should influence price - BuildingArea for ins.\n",
    "## some have too many features to be meaningful and we can drop them\n",
    "## like Suburb, PostCode and Address (encoded in Lattittude and Longtitude)\n",
    "## dates span only over one year so we could not take them into account\n",
    "## or choose to scale prices accordingly - we can try to do so by looking\n",
    "## at global housing trends, for now we leave it out\n",
    "\n",
    "## Note that we can leave SellerG out because of all the additional features it adds\n",
    "## and still get reasonable results. \n",
    "\n",
    "new_features = ['Rooms', 'Type', 'Price', 'Method',\n",
    "        'Distance', 'Bedroom2', 'Bathroom', 'Car',\n",
    "       'Landsize', 'BuildingArea','Lattitude',\n",
    "       'Longtitude', 'Regionname', 'Propertycount']\n",
    "       \n",
    "\n",
    "data = data[new_features]\n",
    "\n",
    "\n",
    "data = pd.get_dummies(data,columns = [\"Type\",\"Method\",\"Regionname\"],\n",
    "               prefix = [\"type\",\"method\",\"region_name\"])\n",
    "\n",
    "\n",
    "#%% Data transformations\n",
    "## We change the data to reflect the fact price distributions seem to \n",
    "## be log-normal\n",
    "\n",
    "data[\"LogPrice\"] = np.log(data[\"Price\"])\n",
    "\n",
    "#%% Outliers\n",
    "## We could remove outliers from the data, which would likely produce better performances\n",
    "## especially in regions or building areas with little information. For the sake of using\n",
    "## as much data as possible, we keep those for now\n",
    "\n",
    "\n",
    "#%% Scale the data\n",
    "\n",
    "float_keys = [\"LogPrice\",\"Distance\",\n",
    "\"Landsize\",\"BuildingArea\",\"Lattitude\",\"Longtitude\"]\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(data[float_keys])\n",
    "\n",
    "unique_scaler = preprocessing.StandardScaler()\n",
    "unique_scaler.fit(data[\"LogPrice\"].values.reshape(-1,1))\n",
    "\n",
    "data[float_keys] = scaler.transform(data[float_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "32850a73-e407-4425-9d8f-906b6fe5d516",
    "_uuid": "a4f9b0e7bffe171c3727637c7d907370485405d7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b6d409f-4a54-40aa-a479-0ed489286816",
    "_uuid": "e3aae47ad9faf95ee44ce12d2923d7c3ebe55198",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7180da16-348e-4517-8279-49bf6800227b",
    "_uuid": "73ca5c41e53cdf4df4d68b3a2b81d58de2e82660",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model.stochastic_gradient import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "names = data.drop([\"LogPrice\",\"Price\"],axis = 1).keys()\n",
    "y = np.asarray(data[\"LogPrice\"])\n",
    "X = data.drop([\"LogPrice\",\"Price\"],axis = 1)\n",
    "X = np.asarray(X)\n",
    "\n",
    "## Templates taken from scikit-learn website and modified for our purposes\n",
    "\n",
    "#%% First model : Lasso\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LassoCV, LassoLarsCV\n",
    "from sklearn.linear_model import LassoLarsIC, RidgeCV, Ridge\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# #############################################################################\n",
    "# LassoLarsIC: least angle regression with BIC/AIC criterion\n",
    "\n",
    "model_bic = LassoLarsIC(criterion='bic')\n",
    "t1 = time.time()\n",
    "model_bic.fit(X, y)\n",
    "t_bic = time.time() - t1\n",
    "alpha_bic_ = model_bic.alpha_\n",
    "\n",
    "model_aic = LassoLarsIC(criterion='aic')\n",
    "model_aic.fit(X, y)\n",
    "alpha_aic_ = model_aic.alpha_\n",
    "\n",
    "\n",
    "def plot_ic_criterion(model, name, color):\n",
    "    alpha_ = model.alpha_\n",
    "    alphas_ = model.alphas_\n",
    "    criterion_ = model.criterion_\n",
    "    plt.plot(-np.log10(alphas_), criterion_, '--', color=color,\n",
    "             linewidth=3, label='%s criterion' % name)\n",
    "    plt.axvline(-np.log10(alpha_), color=color, linewidth=3,\n",
    "                label='alpha: %s estimate' % name)\n",
    "    plt.xlabel('-log(alpha)')\n",
    "    plt.ylabel('criterion')\n",
    "\n",
    "plt.figure()\n",
    "plot_ic_criterion(model_aic, 'AIC', 'b')\n",
    "plot_ic_criterion(model_bic, 'BIC', 'r')\n",
    "plt.legend()\n",
    "plt.title('Information-criterion for model selection (training time %.3fs)'\n",
    "          % t_bic)\n",
    "\n",
    "# #############################################################################\n",
    "# LassoCV: coordinate descent\n",
    "\n",
    "# Compute paths\n",
    "print(\"Computing regularization path using the coordinate descent lasso...\")\n",
    "t1 = time.time()\n",
    "model = LassoCV(alphas = np.logspace(0,2,num = 20),cv=20).fit(X, y)\n",
    "t_lasso_cv = time.time() - t1\n",
    "\n",
    "# Display results\n",
    "m_log_alphas = -np.log10(model.alphas_)\n",
    "\n",
    "plt.figure()\n",
    "ymin, ymax = 0., 0.6\n",
    "plt.plot(m_log_alphas, model.mse_path_, ':')\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='Average across the folds', linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha: CV estimate')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('Mean square error')\n",
    "plt.title('Mean square error on each fold: coordinate descent '\n",
    "          '(train time: %.2fs)' % t_lasso_cv)\n",
    "plt.axis('tight')\n",
    "plt.ylim(ymin, ymax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1a864c18-6758-4c0c-869a-58cbecea4f7b",
    "_uuid": "ace527984b1c669e0ee2c193d343bb261f347d55",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RidgeCV: coordinate descent\n",
    "# Display results\n",
    "\n",
    "# Compute paths, only viable with Leave One Out validation\n",
    "print(\"Computing regularization path using the ridge regression...\")\n",
    "t1 = time.time()\n",
    "m_log_alphas = np.logspace(0,2,num = 20)\n",
    "\n",
    "model = RidgeCV(cv=None,store_cv_values = True,alphas = m_log_alphas).fit(X, y)\n",
    "t_lasso_cv = time.time() - t1\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "ymin, ymax = 0, 0.5\n",
    "for k in range(len(model.cv_values_)):\n",
    "  if (k%int((len(model.cv_values_)/10)) == 0):\n",
    "    plt.plot(-np.log(m_log_alphas), model.cv_values_[k,:], ':')\n",
    "plt.plot(-np.log(m_log_alphas), model.cv_values_.mean(axis=0), 'k',\n",
    "         label='Average across the folds', linewidth=2)\n",
    "plt.axvline(-np.log(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha: CV estimate')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('Mean square error')\n",
    "plt.title('Mean square error on each fold: coordinate descent '\n",
    "          '(train time: %.2fs)' % t_lasso_cv)\n",
    "plt.axis('tight')\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8bb42b9c-16be-40bf-acbf-79e1ec84efb0",
    "_uuid": "219d2932f317d510f0acbedc0c6a4da69df6776f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = model.alpha_\n",
    "\n",
    "RidgeModel = Ridge(alpha = alpha)\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "y_pred_ = np.array([0.0] * len(data))\n",
    "\n",
    "LOO = LeaveOneOut()\n",
    "LOO.get_n_splits(X)\n",
    "compt = 1\n",
    "for train_index, test_index in LOO.split(X):\n",
    "  if compt % int(len(data)/10) == 0:\n",
    "    print(\"progress (in %) = \",int(100 * compt/len(data)))\n",
    "  compt += 1\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  RidgeModel.fit(X_train,y_train)\n",
    "  y_pred = model.predict(X_test)\n",
    "  y_pred_[test_index] = y_pred\n",
    "\n",
    "data[\"LogPricePredicted\"] = y_pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f2e4fb38-4012-4a6b-9cba-e7064e95feeb",
    "_uuid": "ec07996a4850c7e0d1cddb73eb61d7ff7c00125e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% Descale data and compute objective functions\n",
    "\n",
    "float_keys = [\"LogPrice\",\"Distance\",\n",
    "\"Landsize\",\"BuildingArea\",\"Lattitude\",\"Longtitude\"]\n",
    "\n",
    "predicted_data = data\n",
    "predicted_data[float_keys] = scaler.inverse_transform(predicted_data[float_keys])\n",
    "\n",
    "data[\"LogPricePredicted\"] = unique_scaler.inverse_transform(data[\"LogPricePredicted\"])\n",
    "data[\"PricePredicted\"] = np.exp(data[\"LogPricePredicted\"])\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "y = data[\"Price\"]\n",
    "y_pred = data[\"PricePredicted\"]\n",
    "\n",
    "print(\"MAE:\", metrics.mean_absolute_error(y, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y, y_pred)))\n",
    "print('RLMSE:',np.sqrt(metrics.mean_squared_error(np.log(y),np.log(y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b0faa31e-3c5d-478d-adaf-a62c075234a5",
    "_uuid": "623755b5b851e10ef4013c4492ca34be5f050643",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relative_error = abs((y - y_pred)/y_pred)\n",
    "relative_error.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "de221037-b001-4d53-af3b-7462e01d0a39",
    "_uuid": "333a7a86cc2a6d5967c1a4675c5e6b85940edc93",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = np.linspace(0.25,0.9,num = 50)\n",
    "quantiles = relative_error.quantile(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c37050fe-850d-4ade-b1b1-0aded863cda2",
    "_uuid": "2459efd7a7879909e27ecd50b08a947f1c0c88ee",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(100*accuracy,quantiles)\n",
    "plt.title(\"Accuracy as a function of quantiles\")\n",
    "plt.xlabel(\"quantity of data in the confidence interval [x(1-a),x(1+a)] (in %)\")\n",
    "plt.ylabel(\"confidence interval length a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "094516f4-a466-4bb9-97ed-4247bc4f23bf",
    "_uuid": "560ec5545562a2b55333ec386dc1621dbda6d071",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef_names = [[RidgeModel.coef_[k],names[k]] for k in range(len(RidgeModel.coef_))]\n",
    "coef_names = pd.DataFrame(coef_names)\n",
    "coef_names = coef_names.sort_values(by = 0, ascending = False)\n",
    "coef_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc5d4e53-3cb0-44a2-8cd1-cc92e3098889",
    "_uuid": "e4010b63f1b65f4667b06372687df249f3d58bca",
    "collapsed": true
   },
   "source": [
    "# Conclusions:\n",
    "\n",
    "1) Ridge regression improves on linear regression (as performed by Tony Pino) in MAE and notably in MSE, while still offering decent performance with few predictors.\n",
    "\n",
    "2) Very few parameters have strong influence, as shown by the coefficients of the ridge regression: those are\n",
    "     - Region\n",
    "     - Type of property\n",
    "     - Number of rooms and bathrooms\n",
    "     - Distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
