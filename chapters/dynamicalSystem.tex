
\begin{refsection}
\startcontents[chapters]
\chapter{Differential Equations I \& Dynamical Systems}\label{ch:dynamical-systems}
	
\printcontents[chapters]{}{1}{}
\section{Difference equation}\index{difference equation}
\subsection{Existence and uniqueness of solutions}
\begin{definition}[difference equation]
A difference equation of $y$ of $n$th order is usually of the form 
$$y(k+n)+f[y(k+n-1),y(k+n-2),...,y(k),k] = 0$$
where $f$ is an arbitrary real-valued function, be defined over \textbf{a finite or infinite sequence of consecutive values of $k=\{k_0,k_0+1,...\}$}.
\end{definition}

\begin{remark}
One key aspect of difference equation is that the domain is $\Z$ or a subset of $\Z$(consecutive). 
\end{remark}


\begin{theorem}[existence and uniqueness of solutions]
\cite[19]{luenberger1979introduction}Let a difference equation of the form 
$$y(k+n)+f[y(k+n-1),y(k+n-2),...,y(k),k] = 0$$
where $f$ is an arbitrary real-valued function, be defined over a finite or infinite sequence of consecutive values of $k$. The equation has one and only one solution corresponding to each arbitrary specification of the $n$ initial values $y(k_0),y(k_0+1)...,y(k_0+n-1)$.
\end{theorem}

\begin{remark}
The theorem essentially says as long as there is no future dependence, we can iterate the equation to get the solution, even if $f$ can be arbitrary(except for taking $\infty$ values).
\end{remark}

\subsection{Linear difference equations}
\begin{definition}[nth order linear difference equation]\cite[26]{luenberger1979introduction}
An nth order linear difference equation is given as the following form:
$$z(k+n)+a_{n-1}(k)z(k+n-1)+...+ a_0(k)z(k)=g(k)$$
If $g = 0$, then this equation is said to be homogeneous, otherwise it is said to be non-homogeneous.
\end{definition}

\begin{lemma}[linearity of solutions to homogeneous equation]
If $z_1(k),z_2(k),...,z_m(k)$ are solutions to the homogeneous linear difference equations(\textbf{without considering the initial conditions}), then any linear combination of these solutions is still a solution:
$$z(k)=\sum_{i=1}^m c_i z_i(k),c_i\in \R$$
\end{lemma}
\begin{proof}
direct plug in.
\end{proof}


\begin{definition}[linear Independence of solutions]
Given a set of solutions $z_1,z_2,...,z_n$  defined for a set of consecutive integers, say $k=1,2,...,N$, to the homogeneous linear difference equation, we say they are linearly independent if
$$\sum_{i=1}^n c_i z_i(k) = 0, \forall k$$
only hold when all $c_i = 0$. 
\end{definition}


\begin{remark}
We can view each solution as a vector of $N$ dimension, with each dimension/component associated with an integer time.
\end{remark}

\begin{theorem}[the vector space of solutions to homogeneous equation]
Given a $K$th order linear difference homogeneous equation defined on $N$ consecutive integer, the solutions form a linear space of dimension $K$.
\end{theorem}
\begin{proof}
The linearity of solution has been proved. We can view each solution is the $N$  component vector, and there are $N-K$ equations(each equation is the 1-step sliding version of the other) for each vector to satisfy. Therefore, the null space of the linear equation is $K$.
\end{proof}


\begin{definition}[fundamental set of solutions]
\cite{luenberger1979introduction}Define $\bar{z}_i(k)$ to be the solution to $n$th order linear homogeneous difference equation with a special initial condition: $z(k_0+i-1)=1$ and others equal 0. Then this set of solutions $\bar{z}_1(k),\bar{z}_2(2),...,\bar{z}_n(k)$ is called fundamental set of solutions, which is also linearly independent set. 
\end{definition}

\begin{remark}
The linear independence can be proved as: the linear combination of any element in the set except $i$ cannot satisfy the $i$th initial condition.
\end{remark}


\begin{remark}\hfill
\begin{itemize}
    \item since we can view the solutions to homogeneous equation as a vector space, then the role of fundamental set is analog as the standard basis. When a initial condition is given, the solution can be immediately constructed from fundamental set.
    \item not any solution to the difference equation itself is an element of the fundamental set, because this solution might not satisfy the special initial condition.
    \item Calculations of the fundamental set is easy: plug in the special initial condition and iterate forward. 
\end{itemize}
\end{remark}


\begin{theorem}[construct solution to arbitrary initial conditions]
Given the fundamental set of solutions to a homogeneous linear difference equation, then the solution to a homogeneous linear difference equation with initial conditions:$z(k_0+i-1)=c_i,\forall i=1,2,...,N$ can be expressed as 
$$z = \sum_{i=1}^n c_i \bar{z}_i$$
\end{theorem}
\begin{proof}
Since the dimension of the solution space is $n$, these solutions form a basis.
\end{proof}


\begin{theorem}[linearly independent set of solutions]
Suppose $z_1(k),z_2(k),...,z_n(k)$ is a linearly independent set of solutions to the homogeneous equations. Then any solution can be decomposed as
$$z(k) = \sum_{i=1}^n c_i z_i$$
\end{theorem}
\begin{proof}
Since the dimension of the solution space is $n$, these solutions form a basis.
\end{proof}








\subsection{Solution to non-homogeneous equation}
\begin{mdframed}
The procedures to find a solution to a non-homogeneous equation are:\cite{luenberger1979introduction}
\begin{itemize}
    \item First find a set of $n$ linearly independent solutions $z_1,z_2,...,z_n$ to the homogeneous equation
    \item Find a particular solution $\bar{y}$ which does not satisfy the initial condition.
    \item Construct the unique solution:
    $$y = \bar{y} + \sum_{i=1}^n c_i z_i$$
    where $c_i$ are coefficients make $y$ satisfy the initial conditions.
\end{itemize}
\end{mdframed}


\begin{remark}
The coefficients can always be found, its existence can be obtained by setting $z_i$ as the fundamental set, and the initial conditions given by $y-\bar{y}$, from above, we know $c_i$ can be calculated. 
\end{remark}

\subsection{Linear equations with constant coefficients}


\begin{definition}[linear difference equation with constant coefficients]\index{linear difference equation with constant coefficients}
A linear difference equation with constant coefficients is defined as
\begin{equation}\label{ch:dynamical-systems:eqn:lineardifferentconst}
    z(k+n) + a_{n-1}z(k+n-1) + ... + a_0z(k) = 0
\end{equation}

\end{definition}


\begin{definition}[characteristic polynomial]\index{characteristic polynomial}
The polynomial 
$$\lambda^n + a_{n-1}\lambda^{n-1} + ... + a_0 = 0$$
is called characteristic equation associated with the linear difference constant coefficients equation\autoref{ch:dynamical-systems:eqn:lineardifferentconst}. 
\end{definition}


\begin{theorem}\cite[32]{luenberger1979introduction}
A necessary and sufficient condition for the geometric sequence $z(k) = \lambda^k$ to be a solution to the linear difference constant coefficients equation \autoref{ch:dynamical-systems:eqn:lineardifferentconst} is that the constant $\lambda$ satisfies the associated characteristic equation.
\end{theorem}
\begin{proof}
Note that plug in $z(k) = \lambda^k$ into \autoref{ch:dynamical-systems:eqn:lineardifferentconst}, we have
$$(\lambda^n + a_{n-1}\lambda^{n-1} + ... + a_0)\lambda^k = 0$$
\end{proof}


\begin{definition}[Casoratian determinant]\cite[149]{jerri2013linear}
The Casoratian determinant $C$ of $n$ sequence function $z_1(k),...,z_n(k)$ is defined as $$C(k) = det(W)$$
where $W$ is given as
$$W=\begin{pmatrix}
z_1(k) & z_2(k) & \dots & z_n(k))\\ 
z_1(k+1) & z_2(k+1) & \dots & z_n(k+n))\\ 
\vdots & \vdots &  & \vdots \\ 
z_1(k+n) & z_2(k+n) & \dots & z_n(k+n)
\end{pmatrix}$$
\end{definition}




\begin{lemma}[linear independence criterion]\cite[149]{jerri2013linear}
Consider $n$ sequence function $z_1(k),...,z_n(k)$ defined on consecutive integers $K_1\leq k \leq K_2$. Then these $n$ sequence functions are linearly independent if and only 
$$C(k) \neq 0,\forall K_1\leq k\leq K_2-n$$
\end{lemma}





\begin{lemma}[linear dependence of geometric sequence]
$z_1(k) = \lambda_1^k$ and $z_2(k) = \lambda_2^k$ are linearly independent.
\end{lemma}




\begin{definition}[forward operator]
We define forward operator $E$ on the function $z(k)$, which is defined on consecutive interge as
$$Ez(k) = z(k+1)$$
and $E^0 = I$.
\end{definition}


\begin{definition}[characteristic polynomial]
The linear difference equation with constant coefficients can be rewritten as
$\Phi(E)z(k) = (E^n + a_{n-1}E^{n-1} + ... + E^0)z(k) = 0$
where $\Phi(E) = E^n + a_{n-1}E^{n-1} + ... + E^0$ is known as \textbf{characteristic polynomial}.
\end{definition}


\begin{lemma}[key identity]
$$\Phi(E)x^k  = x^k\Phi(x)$$
\end{lemma}
\begin{proof}
$\Phi(E)x^k = (x^{n+k} + a_{n-1}x^{n+k-1} + ... + a_0x^k) = x^k\Phi(x)$
\end{proof}



\begin{theorem}
If the polynomial $\Phi(x)$ has a repeated root $\lambda$ with multiplicity of $m$. Then for functions defined as 
\begin{align*}
z(k) &= \lambda^k,\\
z(k) &= k\lambda^{k},\\ z(k)=k(k-1)\lambda^k,\\
&...,\\
z(k) &= k(k-1)...(k-m_i+1)\lambda^k
\end{align*}
all satisfies
$$\Phi(E) z(k) = 0$$
\end{theorem}
\begin{proof}
Using above lemma we have:
\begin{align*}
    \frac{d}{dx}\Phi(E)x^k &= \frac{d}{dx}x^k\Phi(x)\\
    \Phi(E)kx^{k-1} &= kx^{k-1}\Phi(x) + x^k\Phi'(x)
\end{align*}
if $x=\lambda$ and $\lambda$ is the root to $\Phi(x)$ with multiplicity greater then 1, then $\Phi(\lambda)=0,\Phi'(\lambda) = 0$ and therefore $\Phi(E)kx^{k-1} = 0$ \\

We can generalize this procedure easily to roots with other multiplicities.

\end{proof}


\begin{theorem}[solutions to linear difference equation with constant coefficients]\label{ch:dynamical-systems:th:solutiontolineardifferenceequationwithconstantcoefficients}
Let $\lambda_1,\lambda_2,...,\lambda_k$ be the eigenvalues with algebraic multiplicity of $m_i,i=1,2,...,k$ to the characteristic equation such that $\sum_i m_i = n$, then the following functions form a linearly independent set
\begin{align*}
z(k) &= \lambda_i^k,\\
z(k) &= k\lambda_i^{k},\\ z(k)=k(k-1)\lambda_i^k,\\
&...,\\
z(k) &= k(k-1)...(k-m_i+1)\lambda_i^k,\\
for~&i=1,2,...,k.
\end{align*}
\end{theorem}








\begin{example}\cite[35]{luenberger1979introduction}
Consider the difference equation 
$$y(k+2)=y(k+1) + y(k)$$ with initial condition $y(1)=y(2)=1$. 
The characteristic equation is 
$$\lambda^2 - \lambda - 1 = 0$$
we have the solution of
$\lambda_1 = 1+\sqrt{5}/2, \lambda_2 = 1-\sqrt{5}/2$.\\
Then we assume the solution will take the form 
$$y(k)=A\lambda_1^k + B\lambda_2^k$$
From initial condition we can get
$$y(k)= (\lambda_1^k - \lambda_2^k)\frac{1}{\sqrt{5}}$$
\end{example}



\section{Differential equations}
\subsection{Existence \& uniqueness of solution}
\begin{theorem}
(Local existence and uniqueness \cite[91]{khalil1996nonlinear}
Let $f(t,x)$ be a piece-wise continuous function and satisfies the Lipschitz condition:
$$\abs{f(t,x)-f(t,y)} \leq L\abs{x-y}$$
$\forall x,y\in B=\{x\in \R^n| \norm{x-y} \leq b\},\forall t\in[t_0,t_1]$. Then there exist some $\delta > 0$ such that the equation $\dot{x}=f(t,x)$ with $x(0)=x_0$ has a unique solution over $[t_0,t_0+\delta]$.
\end{theorem}
\begin{example}\hfill
\begin{itemize}
    \item the ODE $\dot{x}=x^{1/3}$ with $x(0)=0$ has non-unique solutions: $x(t) = 0$ and $x(t)=(2t/3)^{3/2}$. It can be verified that at $x=0$ it does not satisfy above Lipschitz condition.($f(x)=x^{1/3},f'=1/3(x^{-2/3})$, therefore $f(x)$ has unbounded derivative at $x=0$, and therefore it is not locally Lipschitz, since the necessary condition is bounded derivatives.)
\end{itemize}
\end{example}


\begin{example}
	Consider the differential equation 
	$$\frac{dy}{dt} = \frac{4y}{t}, y(1) = 1.$$
	
	Since $t$ cannot equal 0, we first consider the situation of $t \in (0,+\infty)$. By assuming $y\neq 0$ for $t\in (0,+\infty)$, we have
	$$\frac{dy}{y} = \frac{4dt}{t} \implies \ln y = 4 \ln t + C \implies y = ke^{4\ln t}=kt^4.$$
	The initial condition implies $k=1$, also $y\neq 0 \forall t > 0$.
	
	For $t < 0$, we have
	$$\frac{dy}{y} = \frac{4d-t}{-t} \implies \ln y = 4 \ln (-t) + C \implies y = ke^{4\ln (-t)}=k(-t)^4.$$
	
	However, no initial condition can be used to determine $k$ when $t < 0$. That is, when $t<0$, there is no unique solution.
	
	\textbf{Note that for any $k$, $y$ is continuous at $t=0$, but may take different form for $t\in (-\infty,0)$. This nonuniqueness is due to the fact that at$t=0$, $dy/dt $ not well defined.}
\end{example}

\begin{theorem}[alternative, for linear differential equations]
\cite[40]{luenberger1979introduction} Given a linear differential equation
$$\frac{\partial^n y}{\partial t^n} + a_{n-1}(t)\frac{\partial^{n-1} y}{\partial t^{n-1}} + ... + a_0(t) y = g(t)$$
satisfying the initial conditions 
$$\frac{\partial^i y}{\partial t^i} = b_i, \forall i = 0,1,...,n-1$$
If all $a_i$s and $g(t)$ are continuous on an interval $0 \leq t \leq T$, then there exists an unique solution.
\end{theorem}


\subsection{Linear differential equations}
\subsubsection{Concepts}
\begin{definition}[order n linear differential equation]
An order n linear differential equation is given by $$\frac{\partial^n y}{\partial t^n} + a_{n-1}(t)\frac{\partial^{n-1} y}{\partial t^{n-1}} + ... + a_0(t) y = g(t)$$
If $g(t) = 0$, then it is called homogeneous; otherwise it is called non-homogeneous.
\end{definition}

\begin{remark}[caution!]
Note that the coefficients $a_0,a_1,...,a_{n-1}$ can be a constant or a function of $t$; however, it cannot be a function of $y$, which is call \textbf{nonlinear differential equation}.	
\end{remark}

\begin{example}\hfill
\begin{itemize}
	\item The ODE
	$$y'' - y' -2y = 0$$
	is an second order linear homogeneous differential equation.
	\item The ODE
	$$y'' - y' -2y = \exp(2t)$$
	is an second order linear non-homogeneous differential equation.
	\item The ODE
	$$(1+t^2)y'' - 2ty' + 2y = 0$$
	is an second order linear homogeneous differential equation.
\end{itemize}	
\end{example}


\begin{theorem}[linearity of solutions]\label{ch:dynamical-systems:th:linearityOfSolutionsLinearHomogeneousODE}
If $z_1(t),z_2(t),...z_m(t)$ are all solutions of a linear homogeneous differential equation, then the linear combination 
$$z(t) = \sum_{i=1}^m c_i z_i(t),c_i\in \C,$$
is also the solution.

Note that the coefficients can be complex constants.
\end{theorem}
\begin{proof}
direct plug in to verify.
\end{proof}


\subsubsection{Wronskian and linear independence}

\begin{definition}[linear independence of solutions]\cite[40]{luenberger1979introduction}
	Given a set of solutions $y_1,y_2,...,y_n$ on the interval $[t_0,t_1]$  to the homogeneous linear differential equation, we say they are linearly independent if
	$$\sum_{i=1}^n c_i y_i(t) = 0, \forall t\in[t_0,t_1]$$
	only hold when all $c_i = 0$. 
\end{definition}

\begin{remark}
	To check linear independence, we need to check the condition holds for all $t \in  [t_0,t_1]$, not just at a single $t$.
\end{remark}





\begin{definition}[Wronskian of a set of solutions]\index{Wronskian}
Given $n$ solutions $z_1(t),z_2(t),...,z_n(t)$, the \textbf{Wronskian} associated with the n solutions is defined to be a determinant of the fundmental matrix 
$$W[z_1,z_2,...,z_n](t) = det~Z = det \begin{pmatrix}
z_1(t) & z_2(t) & \cdots & z_n(t)\\ 
z_1'(t) & z_2'(t) & \cdots & z_n'(t)\\ 
\vdots & \vdots & \ddots & \vdots\\ 
z_1^{(n-1)}(t) & z_2^{(n-1)}(t) & \cdots & z_n^{(n-1)}(t)
\end{pmatrix}$$
where $Z_{ij} = z_j^{i-1}$(the $i-1$ derivative of $j$th solution).
\end{definition}

\begin{note}[motivation of Wronskians and the initial value problem]
Suppose we have n different solutions $z_1,z_2,...,z_n$ to the order n linear homogeneous ODE on the interval $[t_L,t_R]$. We want the solution $z(t) = \sum_{i=1}^{n} c_iz_i(t)$ to satisfy the initial value conditions given by
$z(t_I) = b_0, z'(t_I) = b_1,...,z^{(n-1)}(t_I) = b_{n-1},t_I\in [t_L,t_R]$. 
To solve for the coefficients $c_1,c_2,...,c_n$, we have	
	\begin{align*}
	b_0 &= c_1z_1(t_I) + c_2z_2(t_I) + \cdots + c_nz_n(t_I) \\
	b_1 &= c_1z_1'(t_I) + c_2z_2'(t_I) + \cdots + c_nz_n'(t_I) \\
	\vdots &= \vdots\\
	b_{n-1} &= c_1z_1^{(n-1)}(t_I) + c_2z_2^{(n-1)}(t_I) + \cdots + c_nz_n^{(n-1)}(t_I). 
	\end{align*}

$c_1,c_2,...,c_n$ can be uniquely solved if and only if 
$$det \begin{pmatrix}
	z_1(t_I) & z_2(t_I) & \cdots & z_n(t_I)\\ 
	z_1'(t_I) & z_2'(t_I) & \cdots & z_n'(t_I)\\ 
	\vdots & \vdots & \ddots & \vdots\\ 
	z_1^{(n-1)}(t_I) & z_2^{(n-1)}(t_I) & \cdots & z_n^{(n-1)}(t_I)
\end{pmatrix} \neq 0.$$	
\end{note}


\begin{theorem}[Abel's Wronskian theorem and its properties]\label{ch:dynamical-systems:th:AbelWronskianTheoremAndItsProperties}
\cite[32]{sanchez1968ordinary}\hfill
\begin{itemize}
	\item If $z_1,z_2,...,z_n$ are solutions to the order n linear homogeneous ODE, then the Wronskian satisfies the following first order linear system:
	$$\frac{d}{dt}W[z_1,z_2,...,z_n](t) + a_{n-1}(t)W[z_1,z_2,...,z_n](t) = 0.$$
	\item The Wronskian can be solved to get
	$$W(t) = W(t_0)\exp[-\int_{t_0}^t a_{n-1}(s)ds].$$
	\item The Wronskian is either always zero or never zero. 
\end{itemize}
\end{theorem}
\begin{proof}
(1) see reference. (2) solve ODE in (1). (3)Since $\exp[\int_{t_0}^t a_{n-1}(s)ds] \neq 0$, the Wronskian has the property of \textbf{once zero at a point, all zero; once nonzero at a point, all nonzero}.	
\end{proof}

\begin{example}
$z_1(t) = \exp(2t)$ and $z_2(t) = \exp(-t)$ are solutions of 
$$z''-z'-2z = 0.$$
Their Wronskian is given by
$$W[z_1,z_2](t) = det \begin{pmatrix}
z_1(t)& z_2(t) \\
z_1'(t)& z_2'(t)
\end{pmatrix} = det \begin{pmatrix}
\exp(2t)& \exp(-t) \\
2\exp(2t)& -\exp(-t)
\end{pmatrix} = -3\exp(t)\neq 0.$$	
\end{example}

\begin{theorem}[linear independence and nonzero Wronskian]\label{ch:dynamical-systems:th:LinearIndependenceAndNonzeroWronskianForlinearHomogeneousODE}Let $z_1,z_2,...,z_n$ be the solutions of a linear homogeneous ODE of order $n$ within the interval $(t_L,t_R)$. Then the following properties are equivalent.
\begin{itemize}
	\item the Wronskian $W[z_1,z_2,...,z_n]$ is nonzero everywhere in $(t_L,t_R)$.
	\item the Wronskian $W[z_1,z_2,...,z_n]$ is nonzero somewhere in $(t_L,t_R)$.
	\item $z_1,z_2,...,z_n$ are linearly independent.
\end{itemize}	
\end{theorem}
\begin{proof}
(1) is equivalent to (2): directly from \autoref{ch:dynamical-systems:th:AbelWronskianTheoremAndItsProperties}. (1) implies (3): suppose at $t_I\in (t_L,t_R)$,  $W[z_1,z_2,...,z_n](t_I)\neq 0$. This implies that the following linear algebraic system
 	\begin{align*}
 0 &= c_1z_1(t_I) + c_2z_2(t_I) + \cdots + c_nz_n(t_I) \\
 0 &= c_1z_1'(t_I) + c_2z_2'(t_I) + \cdots + c_nz_n'(t_I) \\
 \vdots &= \vdots\\
 0 &= c_1z_1^{(n-1)}(t_I) + c_2z_2^{(n-1)}(t_I) + \cdots + c_nz_n^{(n-1)}(t_I). 
 \end{align*}
have solution $c_1=c_2=...=c_n= 0$.  That is, the equation
$$\sum_{i=1}^n c_i z_i(t) = 0, \forall t\in[t_L,t_R]$$
only hold when all $c_i = 0$.
(3)implies (1): suppose $z_1,z_2,...,z_n$ are linearly independent but $W[z_1,z_2,...,z_n](t_I) = 0$ for some $t_I\in (t_L,t_R)$.
This implies that
that the following linear algebraic system
\begin{align*}
0 &= c_1z_1(t_I) + c_2z_2(t_I) + \cdots + c_nz_n(t_I) \\
0 &= c_1z_1'(t_I) + c_2z_2'(t_I) + \cdots + c_nz_n'(t_I) \\
\vdots &= \vdots\\
0 &= c_1z_1^{(n-1)}(t_I) + c_2z_2^{(n-1)}(t_I) + \cdots + c_nz_n^{(n-1)}(t_I). 
\end{align*}
have a nonzero solution $c_1,c_2,...,c_n$ such that
$$z(t) = \sum_{i=1}^n c_i z_i(t).$$
The algebraic system also implies that $z(t)$ satisfies the following initial conditions
$$z(t_I) = 0,z'(t_I) = 0,...,z^{(n-1)}(t_I) = 0.$$
In other words, $z(t)$ is a constant value function of zero. Or equivalently,
$$z(t) = 0 = \sum_{i=1}^n c_i z_i(t).$$
This contradicts the fact that $z_1,z_2,...,z_n$ are linear independent.
\end{proof}

\begin{corollary}[linear independence and nonzero Wronskian]Let $z_1,z_2,...,z_n$ be the solutions of a linear homogeneous ODE of order $n$ within the interval $(t_L,t_R)$. Then the following properties are equivalent.
	\begin{itemize}
		\item the Wronskian $W[z_1,z_2,...,z_n]$ is zero everywhere in $(t_L,t_R)$.
		\item the Wronskian $W[z_1,z_2,...,z_n]$ is zero somewhere in $(t_L,t_R)$.
		\item $z_1,z_2,...,z_n$ are linearly dependent.
	\end{itemize}	
\end{corollary}

\begin{remark}[Wronskian and linear independence of functions might not hold for general functions]
\begin{itemize}
	\item Only when the functions are solutions of the order n linear homogeneous ODE, \autoref{ch:dynamical-systems:th:LinearIndependenceAndNonzeroWronskianForlinearHomogeneousODE} holds.
	\item For general functions, above theorem might not hold. For example $Y_1(t) = t^2,Y_2(t) = \abs{t}t, t\in(-\infty,\infty)$. We can show that
	\begin{align*}
	W[Y_1,Y_2](t) &= det \begin{pmatrix}
	Y_1(t) & Y_2(t) \\
	Y_1'(t) & Y_2'(t)
	\end{pmatrix} \\
	&= det \begin{pmatrix}
	t^2 & \abs{t}t \\
	2t & 2\abs{t}
	\end{pmatrix} \\
	&= 0
	\end{align*}
	However, it is clearly that $Y_1(t)$ and $Y_2(t)$ are not proportional to each other;i.e., they are linearly independent.
\end{itemize}	
\end{remark}


\subsubsection{General solution theory}


\begin{definition}[fundamental systems]
	\cite[30]{sanchez1968ordinary} A collection of $n$ linearly independent solutions $z_1(t),z_2(t),...,z_n(t)$ is called \textbf{fundamental system.} 
\end{definition}

\begin{theorem}[construct arbitrary solution from fundamental set]
Suppose $z_1(t),z_2(t),...,z_n(t)$ is a linearly independent set of solutions to the linear homogeneous differential equations of order $n$, then \textbf{any} solution can be decomposed as the linear combination of the $$F(t) = [z_1(t)~z_2(t)~...~z_n(t)].$$ 
\end{theorem}
\begin{proof}	
Consider an arbitrary solution denoted by $Y(t)$. We can generate $n$ initial conditions at $t_I$ by differentiation; that is
$$Z(t_I) = Y(t_I),Z'(t_I) = Y'(t_I),...,Z^{(n-1)}(t_I) = Y^{(n-1)}(t_I).$$
To show that $Y(t)$ can be actually decomposed via
$$Y(t) = \sum_{i=1}^{n} c_i z_i(t),$$
we want to solve for $c$ from the linear system
	\begin{align*}
Y(t_I) &= c_1z_1(t_I) + c_2z_2(t_I) + \cdots + c_nz_n(t_I) \\
Y'(t_I) &= c_1z_1'(t_I) + c_2z_2'(t_I) + \cdots + c_nz_n'(t_I) \\
\vdots &= \vdots\\
Y(t_I)^{(n-1)} &= c_1z_1^{(n-1)}(t_I) + c_2z_2^{(n-1)}(t_I) + \cdots + c_nz_n^{(n-1)}(t_I). 
\end{align*}

Because \textbf{linear independence implies nonzero  Wronskian}(\autoref{ch:dynamical-systems:th:LinearIndependenceAndNonzeroWronskianForlinearHomogeneousODE}), then $c_1,c_2,...,c_n$ can be uniquely solved.	
\end{proof}


\begin{remark}[implication for initial value problem]
	The solution to an initial value problem is just one solution satisfies both the ODE and the initial conditions; therefore it can be constructed via linear combination of the fundamental set. 
\end{remark}

\begin{theorem}[necessary and sufficient conditions for fundamental system and the solution method]The necessary and sufficient conditions for $z_1,z_2,...,z_n$ to be a fundamental system is
	that there exists a $t_0 \in (r_1,r_2)$, such that $W(t_0)\neq 0$.
	Note that $(t_L,t_R)$ is the interval where the conditions for existence and uniqueness of solutions are satisfied. 
\end{theorem}
\begin{proof}
(a) forward: use the property of Wronskian that \textbf{ nonzero Wronskian implies linear independence}.(b) converse: a fundamental system is linearly independent and the Wronskian will be nonzero.
\end{proof}

\begin{theorem}[natural fundamental set of solution]
Let $N_i(t),i=0,1,...,n-1$ be a set of solutions to the linear homogeneous differential equations with special initial conditions given as
$$\frac{\partial^n y}{\partial t^n} = \begin{cases} 
1, n=i-1\\
0, otherwise\\
\end{cases}.$$
This set of solutions are called \textbf{natural fundamental set of solutions}, and the solution to linear homogeneous differential equations with arbitrary initial conditions can be constructed via
$$z = \sum_{i=1}^{n} c_iN_i(t)$$
with $c_i=b_i$. \\
Moreover, \textbf{the natural fundamental set are linearly independent.} 
\end{theorem}
\begin{proof}
directly use the existence and uniqueness theorem plus the vector space nature of the solutions. 
We can calculate the Wronskian for the natural fundamental set of solutions given by	
\begin{align*}
W[N_1,N_2,...,N_n](t_I) &= det \begin{pmatrix}
N_1(t_I) & N_2(t_I) & \cdots & N_n(t_I)\\ 
N_1'(t_I) & N_2'(t_I) & \cdots & N_n'(t_I)\\ 
\vdots & \vdots & \ddots & \vdots\\ 
N_1^{(n-1)}(t_I) & N_2^{(n-1)}(t_I) & \cdots & N_n^{(n-1)}(t_I)
\end{pmatrix} \\
&=det \begin{pmatrix}
1 & 0 & \cdots & 0\\ 
0 & 1 & \cdots & 0\\ 
\vdots & \vdots & \ddots & \vdots\\ 
0 & 0 & \cdots & 1
\end{pmatrix} = 1 \neq 0.
\end{align*}	
therefore, the natural fundamental set is linearly independent.
\end{proof}

\begin{example}
Consider the ODE
$$y'' - y' - 2y =0,$$
with initial conditions $y(0) = b_0, y'(0) = b_1.$

The natural fundamental set of solutions are
$$N_0(t) = \frac{\exp(2t)+2\exp(-t)}{3},N_1(t) = \frac{\exp(2t)-\exp(-t)}{3}.$$

It can be verify that $N_0(t)$ solves
$$y'' - y' - 2y =0,$$
with initial conditions $y(0) = b_0, y'(0) = 0.$

And $N_1(t)$ solves
$$y'' - y' - 2y =0,$$
with initial conditions $y(0) = 0, y'(0) = b_1.$
\end{example}



\subsection{Linear homogeneous differential equations with constant coefficients}
\subsubsection{The key identity}
\begin{definition}[differential operators and polynomials]\hfill
\begin{itemize}
	\item The \textbf{differential operator} $D:C^\infty \to C^{\infty}$ is defined as
	$$D[\phi] = \phi'$$
	where $\phi\in C^\infty$.
	\item Let $$L(D)=a_0D^n + a_1D^{n-1} + ... + a_n$$ with polynomial $L$ defined as
	$$L(x) = a_0x^n + a_1x^{n-1} + ... + a_n $$
	then $L(D)$ is called a \textbf{differential operator polynomial.}
\end{itemize}	
\end{definition}

\begin{definition}[characteristic polynomial]
Consider a linear homogeneous differential equation with constant coefficients given by
\end{definition}


\begin{theorem}[key identity for linear homogeneous ODE]\label{ch:dynamical-systems:th:keyIdentityLinearHomogeneousODE}
Consider a differential operator polynomial given by $$L(D)=a_0D^n + a_1D^{n-1} + ... + a_n.$$
Then we have the \textbf{key identity} $$L(D)e^{rx} = L(r)e^{rx},$$
where $$L(r) = (a_0r^n + a_1r^{n-1} + ... + a_n)$$
is known as the characteristic polynomial.
\end{theorem}
\begin{proof}
Direct differentiate $e^{rx}$ and can verify the correctness.
\end{proof}


\begin{theorem}[characteristic solution to linear homogeneous differential equation]\label{ch:dynamical-systems:th:characteristicSolutiontolinearhomogeneousODE}
Let $\lambda \in \C$ be a root to the characteristic polynomial $L(x) = 0$, then
$$L(D)e^{\lambda x} = 0.$$
That is, $e^{\lambda x}$ is one solution to the differential equation $L(D)e^{rx} = 0$. Particularly if $\lambda = 0$, then constant function $1$ is the solution.
\end{theorem}
\begin{proof}
 $L(D)e^{\lambda x} = L(\lambda)e^{\lambda x} = 0$ from above lemma.
\end{proof}

\subsubsection{The case of real roots}
\begin{theorem}[solutions associated with characteristic polynomial roots with multiplicity]\cite[49]{sanchez1968ordinary}
Let $\lambda$ be a root to polynomial $L(x) = 0$ with multiplicity $m$, then the function 
$$x^re^{\lambda x}, r\in \{0,1,2,...,m-1\}$$
satisfies
$L(D)x^re^{\lambda x} = 0$.
That is, $x^re^{\lambda x}$ is the solution to the differential equation $L(D)f(x) = 0$
\end{theorem}
\begin{proof}
Consider the key identity(\autoref{ch:dynamical-systems:th:keyIdentityLinearHomogeneousODE}) $$ L(D)e^{sx} = e^{sx}L(s)$$( where $D$ is the differential operator with respect to $x$). Take the derivative with respect to $s$ we have
\begin{align*}
    \frac{d}{ds}L(D)e^{sx} &= \frac{d}{ds}e^{sx}L(s) \\
    sL(D)e^{st} &= e^{st}(sL(s) + dL(s)/ds)
\end{align*}
where we have used the fact that exchange the partial differential operator is legitimate. 
We can see then if $s=\lambda$ is root to $L(x)$ with multiplicity of 2, then we can write $L(x)=(x-\lambda)^2Q(x)$, and we have $$L(\lambda) = 0, \frac{dL}{dx}|_{x=\lambda} = 0.$$
Therefore, 
$$L(D)t\exp(\lambda t) = e^{\lambda t}(\lambda L(\lambda) + dL(\lambda)/ds) = 0,$$
and $x\exp(\lambda x)$ is a solution. 

More generally, we have
\begin{align*}
   \frac{d^r(L(D)e^{sx}}{ds^r} &= L(D)x^re^{sx} =  \frac{d^r e^{st}L(s)}{ds^r} \\
    &= e^{st}(L^{(r)}(s) + rtL^{(r-1)}(s) + ... + t^rL(s))
\end{align*}
where we have the binomial expression for differentiate products(general leibniz rule).If $s=\lambda$ is root to $L(x)$ with multiplicity greater than $r$, then
$L(D)x^re^{\lambda x} = 0$.
That is, $x^r\exp(\lambda x)$ is a solution.
\end{proof}

\subsubsection{The case of complex roots}

\begin{lemma}[complex roots comes as conjugate pairs for real-valued ODE]
Consider a linear homogeneous differential equation given by
$$L(D)y = 0$$	
where $L(x)$ is a polynomial with real-valued coefficients. 
If $a+bi,a,b\in \R$ is a root to $L(x) = 0$ such that $\exp((a+bi)t)$ is a solution, then
 $a-bi,a,b\in \R$ is also a root to $L(x) = 0$ such that $\exp((a-bi)t)$ is also a solution.
\end{lemma}
\begin{proof}
Use	
\autoref{ch:dynamical-systems:th:characteristicSolutiontolinearhomogeneousODE} and \autoref{ch:topics-in-abstract-algebra:th:PairedComplexRootsForRealCoefficientPolynomials}
\end{proof}



\begin{lemma}[conversion between trigonometric functions and complex exponential functions]\label{ch:dynamical-systems:th:ConversionTrigonometricFunctionsAndComplexExponentialsHomogeneousEquations} 
Let
$$y_1(t)=\exp((a+ib)t),y_2=\exp((a-ib)t),a,b\in \R$$	
be the solutions to 
$$Ly = 0,$$
then
$$u_1(t) =\Re(y_1(t))= \exp(at)\cos(bt), u_2(t)= \Im(y_2(t)) = \exp(at)\sin(bt),$$
are also the solutions.	
\end{lemma}
\begin{proof}
Use the linearity of solutions in \autoref{ch:dynamical-systems:th:linearityOfSolutionsLinearHomogeneousODE}, we have
$$u_1(t) = \frac{y_1(t) + y_2(t)}{2},u_2(t) = \frac{y_1(t) - y_2(t)}{2i}.$$
\end{proof}

\subsubsection{The complete solution set}
\begin{theorem}[construct the complete solution set]\cite[49]{sanchez1968ordinary}
Let $\lambda_1,\lambda_2,...,\lambda_k \in \C$,with multiplicity $\mu_1,\mu_2,...,\mu_k, \sum \mu_i = n$, be the roots to order $n$ polynomial $L(x) = 0$, then the complete linear independent solution set is given by
\begin{align*}
	&x^re^{\lambda_1 x}, r\in \{0,1,2,...,\mu_1-1\} \\
	&x^re^{\lambda_2 x}, r\in \{0,1,2,...,\mu_2-1\} \\
	&\cdots \\
	&x^re^{\lambda_k x}, r\in \{0,1,2,...,\mu_k-1\} \\
\end{align*} 
The solution to the differential equation $L(D)y = 0$ can be written by
$$y = \sum_{i=0}^{\mu_1-1} c_{1i} x^r\exp(\lambda_1 x) + \sum_{i=0}^{\mu_2-1} c_{2i} x^r\exp(\lambda_2 x) + \cdots + \sum_{i=0}^{\mu_k-1} c_{2i} x^r\exp(\lambda_k x).$$
\end{theorem}

\begin{example}[two distinct real roots]
The general solution of 
$$y'' - y' -2y = 0$$
is given by $$y = c_1\exp(-t) + c_2\exp(2t).$$
This is because the characteristic polynomial is given by
$$p(r) = r^2-r-2 = (r+1)(r-2).$$
Its two roots are $-1$ and $2$, with associated solutions $\exp(-t)$ and $\exp(2t)$.	
\end{example}

\begin{example}[real roots with multiplicity]
	The general solution of 
	$$u'' + 6u + 9 = 0$$
	is given by $$u = c_1\exp(-3t) + c_2t\exp(-3t).$$
	This is because the characteristic polynomial is given by
	$$p(r) = r^2+6r+9 = (r+3)^2.$$
	Its root is -3 with multiplicity of 2, with associated solutions $\exp(-3t)$ and $t\exp(-3t)$.	
\end{example}

\begin{example}[distinct complex roots]
	The general solution of 
	$$u'' +  9u = 0$$
	is given by $$u = c_1\exp(-3it) + c_2t\exp(-3it) = c3\cos(3t) + c4\sin(3t).$$
	This is because the characteristic polynomial is given by
	$$p(r) = r^2+9 = (r+3i)(r-3i).$$
	Its root is -3i and 3i, with associated solutions $\exp(-3it)$ and $t\exp(3it)$.	
\end{example}

\begin{example}[complex roots with multiplicity]
	The general solution of 
	$$u^{(4)} + 8u^{(2)} + 16u = 0$$
	is given by 
	\begin{align*}
	u &= c_1\exp(-2it) + c_2\exp(2it) + c_3t\exp(-2it) + c_4t\exp(2it) \\
	&= c_5\cos(-2t) + c_6\sin(2it) + c_7t\cos(-2t) + c_8t\sin(2t) 
	\end{align*}
	This is because the characteristic polynomial is given by
	$$p(r) = r^4+8r^2+16 = (r^2+4)^2.$$
	Its roots are $\pm 2i,\pm 2i$, with associated solutions $\exp(-2t),\exp(2t)$ and $t\exp(2it),t\exp(-2it)$.	
\end{example}

\begin{example}[a complete example]
	The general solution of 
	$$(D^3-2D^2)(D^2-2D+10)^3(D^2+4D+29)y = 0, D \triangleq \frac{d}{dt},$$
	is given by 
	\begin{align*}
	u &= c_1 + c_2t + c_3\exp(2t) + c_4\exp(t)\cos(3t)+c_5\exp(t)\sin(3t) + c_6t\exp(t)\cos(3t) + c_7t\exp(t)\sin(3t)\\
	&+ c_8t^2\exp(t)\cos(3t) + c_9t^2\exp(t)\sin(3t) + c_{10}\exp(-2t)\cos(5t) + c_{11}\exp(-2t)\sin(5t) 
	\end{align*}
	This is because the characteristic polynomial is given by
	\begin{align*}
	p(r) & = (r^3-2r^2)(r^2-2r+10)^3(r^2+4r+29)\\
	& = r^2(r-2)((r-1)^2+9)^3((r+2)^2+25)
	\end{align*}
	Its 11 roots are $0,0,2,1\pm 3i,1\pm3i,1\pm 3i,-2\pm i5$.	
\end{example}


\subsection{Solution to non-homogeneous ODEs}
\subsubsection{General principles}
\begin{theorem}[general principle for linear non-homogeneous ODEs]
	Consider a linear differential equation
	$$\frac{\partial^n y}{\partial t^n} + a_{n-1}(t)\frac{\partial^{n-1} y}{\partial t^{n-1}} + ... + a_0(t) y = g(t).$$
The solution space is given by the vector space of the solution $\{c_1y_1+c_2y_2 + ... + c_ny_n, c_1,...,c_n\in \R^n \}$to the associated homogeneous solution plus a particular solution $y_p$ satisfy
	$$\frac{\partial^n y_p}{\partial t^n} + a_{n-1}(t)\frac{\partial^{n-1} y_p}{\partial t^{n-1}} + ... + a_0(t) y_p = g(t).$$

Moreover, when given the initial conditions 
$$\frac{\partial^i y}{\partial t^i} = b_i, \forall i = 0,1,...,n-1,$$
we can uniquely determine $c_1,...,c_n$.
\end{theorem}
\begin{proof}
	Directly plug in to verify.
\end{proof}

\begin{remark}\hfill
\begin{itemize}
	\item The particular solution $y_P$ does not need to satisfy the initial condition. The coefficients associated with the homogeneous solution needs to satisfy the initial condition.
\end{itemize}	

\end{remark}



\begin{lemma}[decomposition of particular solution]
Let $y_{P,1}(t),y_{P,2}(t),...,y_{P,k}(t)$ be the particular solutions such that
$$Ly_{P,i} = g_i(t), i=1,2,...,k.$$
Then
$$L(y_{P,1} + y_{P,2} + ... + y_{P,k}) = g_1(t) + g_2(t) + ... + g_k(t).$$	
\end{lemma}
\begin{proof}
Using the linearity of the operator $L$.
\end{proof}

\subsubsection{Key identity approach}
\begin{lemma}
	If $z$ is a root to $L(x) = 0$ with multiplicity $m$, then
	$$L[t^m\exp(zt)] = L(z)^{(m)}\exp(zt).$$	
	and
	$$L[t^r\exp(zt)] = 0, \forall r<m.$$	
	$$L[t^s\exp(st)] = L(z)^{(m)}t^{s-m}\exp(zt), \forall s>m.$$
\end{lemma}
\begin{proof}
If $z$ is a root to $L(x) = 0$ with multiplicity $m$, then we can write
$$L(x) = (x-z)^m Q(x).$$

For $r = m$, we have 
$$L[t^r\exp(zt)] = \frac{d}{dx}L[\exp(xt)]_{x=z} = [\frac{d}{dx}L(x)\exp(xt)]_{x=z} = L(z)^{(m)}\exp(zt).$$
\end{proof}

\begin{example}
	Consider $$y''-6y'+9y = 4\exp(3t).$$
	3 is the root to $L(z)=z^2-6z+9$ with multiplicity of 2.
	We have
	\begin{align*}
	L[\exp(3t)] &= L(3)\exp(3t) = 0 \\
	L[t\exp(3t)] &= L'(3)\exp(3t) + 3L(3)\exp(3t) = 0 \\
	L[t^2\exp(3t)] &= L''(3)\exp(3t) + 2tL'(3)\exp(3t) + t^2L(3)\exp(3t) = L''(3)\exp(3t) =  2\exp(3t) \\
	L[t^3\exp(3t)] &= tL''(3)\exp(3t) = 2t\exp(3t) 
	\end{align*}
\end{example}


\begin{theorem}[key identity approach to particular solution for non-homogeneous linear ODE]\label{ch:dynamical-systems:th:KeyIdentityApproachParticularSolutionNonhomogeneouslinearODE}
Consider linear non-homogeneous equation given by $$L(D)y = f(t),$$
where $f(t)$ has the form of
$$f(t)=(h_0 t^d+h_1t^{d-1}+...+h_d)\exp(u t)\cos(vt) + (g_0 t^d+g_1t^{d-1}+...+g_d)\exp(u t)\sin(vt).$$
We say $f$ has characteristic form with degree $d$ and characteristic $u+i v$.

Then we can find coefficients $\beta_0,\beta_1,\beta_2,... \in \C$ such that
$$f(t) = L[\sum_{i=1} \beta_i t^{i}\exp((u+iv)t)]$$ and the particular solution is given by
$$y_P = \sum_{i=1} \beta_i  t^{i}\exp((u+iv)t).$$
\end{theorem}
\begin{proof}
It is straight forward to see that $$L[y_P] = f(t).$$
\end{proof}

\begin{remark}
The key identity approach only works for the cases where $f(t)$ take the particular forms.
\end{remark}




\begin{lemma}[conversion between trigonometric functions and complex exponential functions]\label{ch:dynamical-systems:th:ConversionTrigonometricFunctionsAndComplexExponentialsForcingTerms} 
	Let
	$$y_1(t)=\exp((a+ib)t),y_2=\exp((a-ib)t),a,b\in \R$$	
	be the solutions to 
	$$Ly = 0,$$
	then
	$$u_1(t) =\Re(y_1(t))= \exp(at)\cos(bt), u_2(t)= \Im(y_2(t)) = \exp(at)\sin(bt),$$
	are also the solutions.	
\end{lemma}
\begin{proof}
	Use the linearity of solutions in \autoref{ch:dynamical-systems:th:linearityOfSolutionsLinearHomogeneousODE}, we have
	$$u_1(t) = \frac{y_1(t) + y_2(t)}{2},u_2(t) = \frac{y_1(t) - y_2(t)}{2i}.$$
\end{proof}

\begin{example}
Consider the ODE given by	
	$$Ly = y''-2y'+5y = te^t = f(t).$$
The characteristic polynomial roots are $1\pm 2$, and therefore we have homogeneous solution
$$y_H(t) = c_1\exp(t)\cos(2t) + c_2\exp(t)\sin(2t).$$

We then use key identity approach to find the particular solution. We have
\begin{align*}
L(\exp(zt)) &= (z^2-2z+5)\exp(zt)\\
L(t\exp(zt)) &= \frac{d}{dt}[(z^2-2z+5)\exp(zt)]=(z^2-2z+5)t\exp(zt) + (2z-2)\exp(zt)
\end{align*}
Plug in $z = 1$, we have
$$L(\exp(t)) =  4\exp(t), L(t\exp(t)) = 4t\exp(t).$$

Therefore, we can construct a particular solution as 
$$y_P(t) = \frac{1}{4}t\exp(t),$$
such that $$L(\frac{1}{4}t\exp(t)) = t\exp(t)= f(t).$$
\end{example}



\begin{example}
	Consider the ODE given by	
	$$Ly = y''-2y'+5y = te^t = f(t).$$
	The characteristic polynomial roots are $1\pm 2$, and therefore we have homogeneous solution
	$$y_H(t) = c_1\exp(t)\cos(2t) + c_2\exp(t)\sin(2t).$$
	
	We then use key identity approach to find the particular solution. We have
	\begin{align*}
	L(\exp(zt)) &= (z^2-2z+5)\exp(zt)\\
	L(t\exp(zt)) &= \frac{d}{dt}[(z^2-2z+5)\exp(zt)]=(z^2-2z+5)t\exp(zt) + (2z-2)\exp(zt)
	\end{align*}
	Plug in $z = 1$, we have
	$$L(\exp(t)) =  4\exp(t), L(t\exp(t)) = 4t\exp(t).$$
	
	Therefore, we can construct a particular solution as 
	$$y_P(t) = \frac{1}{4}t\exp(t),$$
	such that $$L(\frac{1}{4}t\exp(t)) = t\exp(t)= f(t).$$
\end{example}


\begin{example}
	Consider the ODE given by	
	$$Ly = y''-3y'-10 = te^{-2t} = f(t).$$
	The characteristic polynomial roots are $5,-2$, and therefore we have homogeneous solution
	$$y_H(t) = c_1\exp(5t) + c_2\exp(-2t).$$
	
	We then use key identity approach to find the particular solution. We have
	\begin{align*}
	L(\exp(zt)) &= (z^2-3z-10)\exp(zt)\\
	L(t\exp(zt)) &= \frac{d}{dt}[(z^2-3z-10)\exp(zt)]=(z^2-3z-10)t\exp(zt)+(2z-3)\exp(t)\\
	L(t^2\exp(zt)) &= \frac{d}{dt}[(z^2-3z-10)\exp(zt)]=(z^2-3z-10)t^2\exp(zt)+2(2z-3)t\exp(t)+2\exp(t)
	\end{align*}
	Plug in $z = -2$, we have
	$$L(\exp(-2t)) = 0, L(t\exp(-2t)) =  -7\exp(-2t), L(t\exp(-2t)) = -14t\exp(-2t)+2\exp(-2t).$$
	
	Therefore, we can construct a particular solution $y_P$ such that 
	$$L(y_P(t)) = L(-\frac{1}{14}t^2\exp(-2t)-\frac{2}{98}t\exp(-2t)) =  te^{-2t} = f(t),$$
	where 
	$$y_P(t)=-\frac{1}{14}t^2\exp(-2t)-\frac{2}{98}t\exp(-2t). $$
\end{example}

\begin{corollary}
	Given $$y'' + by' + cy = G(x)$$ with $G(x)$ given by $P_n(x)$, a polynomial of degree $n$. Denote $L(x) = x^2+bx+c$.
It follows that
\begin{itemize}
	\item if $0$ is not a root to $L(x)$, then the particular solution will also be a polynomial of degree $n$. The coefficients of the solution can be determined by substituting. 
	\item if $0$ is a root to $L(x)$ with multiplicity of 1, then the particular solution will be a polynomial of degree $n+1$. The coefficients of the solution can be determined by substituting.
	\item if $0$ is a root to $L(x)$ with multiplicity of 2, then the particular solution will be a polynomial of degree $n+2$. The coefficients of the solution can be determined by substituting.
\end{itemize}
\end{corollary}
\begin{proof}
(1)	Use \autoref{ch:dynamical-systems:th:KeyIdentityApproachParticularSolutionNonhomogeneouslinearODE} with characteristic equals 0.
(2) Note that  
\end{proof}


\begin{example}\hfill
\begin{itemize}
	\item The equation
	$$y'' + y' + y = 1$$
	has a particular solution of 1.
	\item The equation
	$$y'' + y' + y = x$$
	has a particular solution of the form $ax + b$.
	We can determine $a,b$ by
	$$a + b = 0, a = 1.$$
	\item The equation
	$$y'' + y' + y = dx^2 + ex + f$$
	has a particular solution of the form $ax^2 + bx + c$.
	We can determine $a,b,c$ by
	$$a= d, 2a+b = e, 2a+b + c = f.$$
\end{itemize}	
\end{example}

\begin{corollary}
	Given $$L(D)y = y'' + by' + cy = G(x)$$ with $G(x)$ given by $e^{kx}P_n(x)$, a polynomial of degree $n$.
	 Denote $L(x) = x^2+bx+c$.
	 and $k$ is not a solution of the characteristic equation.It follows that
	\begin{itemize}
		\item if $k$ is not a root to $L(x)$, then the particular solution will be the form $e^{kx}Q(x)$, $Q(x)$ is a polynomial of degree $n$. The coefficients of the solution can be determined by substituting. 
		\item if $k$ is a root to $L(x)$ with multiplicity 1, then the particular solution will be the form $e^{kx}Q(x)$, $Q(x)$ is a polynomial of degree $n+1$. The coefficients of the solution can be determined by substituting. 
		\item if $k$ is not a root to $L(x)$ with multiplicity 2, then the particular solution will be the form $e^{kx}Q(x)$, $Q(x)$ is a polynomial of degree $n+2$. The coefficients of the solution can be determined by substituting. 
	\end{itemize}
\end{corollary}

\begin{example}
	The equation
	$$y'' + 4y = e^{3x}$$
	has a particular solution of the form $ae^{3x}$.
	We can determine $a$ by
	$$9a + 4a = 1, a = 1/13.$$
\end{example}



\subsection{First order linear differential equation}
\begin{theorem}[general solution to first order linear differential equation]\label{ch:dynamical-systems:th:GeneralSolutionFirstOrderLinearODE}
The equation of the form
$$\frac{dy}{dx} + p(x)y = Q(x),y(0) = y_0$$
has the solution
$$y(x) = \exp(\int_0^x -p(t)dt)y_0 + \int_0^x \exp(\int_t^x -p(\tau)d\tau) Q(t)dt.$$
\begin{itemize}
	\item If we introduce $\Phi(t,\tau) = \exp(-\int_{\tau}^{t}p(u)du)$, then
	$$y = \Phi(x,0)y_0 + \int_0^x \Phi(x,t)Q(t)dt.$$
	\item If $p(x) = p$, then
	$$y(x) = \exp(-p(t)x)y_0 + \int_0^x \exp(-p(x-t)) Q(t)dt.$$
\end{itemize}
\end{theorem}
\begin{proof}
This is a special case of \autoref{ch:dynamical-systems:th:generalsolutionlinearsystem}.
\end{proof}

\begin{example}
Given the example
$$y' + \frac{y}{x} = \frac{1}{x^2},y(1)=1,$$
we can calculate the state transition function 
$$\Phi(t_1,t_2) = \exp(-\int_{t_1}^{t_2}\frac{1}{x}dx) = \frac{t_1}{t_2}.$$
Then
$$y(x) = \Phi(x,1)y(1) + \int_1^x \Phi(x,\tau)\frac{1}{\tau^2}d\tau = \frac{1}{x} + \frac{\ln x}{x}. $$
\end{example}


\begin{example}
Consider the differential equation 
$$\frac{dy}{dt} = \frac{4y}{t}, y(1) = 1.$$

Since $t$ cannot equal 0, we first consider the situation of $t \in (0,+\infty)$. By assuming $y\neq 0$ for $t\in (0,+\infty)$, we have
$$\frac{dy}{y} = \frac{4dt}{t} \implies \ln y = 4 \ln t + C \implies y = ke^{4\ln t}=kt^4.$$
The initial condition implies $k=1$, also $y\neq 0 \forall t > 0$.

For $t < 0$, we have
$$\frac{dy}{y} = \frac{4d-t}{-t} \implies \ln y = 4 \ln (-t) + C \implies y = ke^{4\ln (-t)}=k(-t)^4.$$

However, no initial condition can be used to determine $k$ when $t < 0$. That is, when $t<0$, there is no unique solution.

	
\end{example}


\section{Linear system}
\begin{definition}[discrete-time linear system]
A discrete-time linear system is given as:
$$x(k+1) = A(k)x(k) + w(k)$$
where $x(k),w(k) \in \R^n, A\in \R^{n\times n}$.
If $w(k) = 0$, then it is called \textbf{homogeneous system};otherwise it is called \textbf{non-homogeneous system}. 
\end{definition}



\subsection{Solution space for linear homogeneous system}
\begin{mdframed}
\textbf{Note}\\
In this subsection, we discuss the solution property for the linear homogeneous equation of dimension $n$:
$$\dot{x} = A(t)x, x\in \R^n, A(t)\in \R^{n\times n}.$$
\end{mdframed}
\begin{theorem}[Existence and uniqueness of solutions]\cite[8]{sanchez1968ordinary}\label{ch:dynamical-systems:th:existenceUniquenessLinearHomogeneousSystems} \hfill
\begin{itemize}
	\item Let the equation $$\dot{x}=f(t,x),x\in\R^n$$ be given, where $f(t,x)$ is defined in some domain $B$ in $\R^{n=1}$. Suppose both $f$ and $\Pa f/\Pa x_i,i=1,2...,n$ are \textbf{defined and continuous} in $B$. Then for every point $(t_0,x_0)\in B$, there exists a unique solution $x=\phi(t)$ satisfying $\phi(t_0) = x_0$ and defined in some neighborhood of $(t_0,x_0)$.
	\item Particularly for $$\dot{x}=f(t,x) = A(t)x(t),, x\in \R^n, A(t)\in \R^{n\times n},$$
	if $A(t)$ is \textbf{continuous} over some domain $B$ in $\R^{n=1}$,Then for every point $(t_0,x_0)\in B$, there exists a unique solution $x=\phi(t)$ satisfying $\phi(t_0) = x_0$ and defined in some neighborhood of $(t_0,x_0)$.
\end{itemize}	
\end{theorem}


\begin{remark}[consequences]\hfill
\begin{itemize}
    \item Suppose we have two solutions satisfying $\dot{x}=f(t,x)$ and initial conditions, then two the solution must be equal. This property can be summarized as: \textbf{once agreeing on a point, they will agree on a neighborhood.}
    \item If we have two solutions \textbf{only} satisfying $\dot{x}=f(t,x)$, and the two solutions need not be equal.
\end{itemize}
\end{remark}



\begin{lemma}[linearity of solutions to linear homogeneous systems]\label{ch:dynamical-systems:th:linearityOfSolutionToLinearHomogeneousSystems}
Suppose $z_1(t),z_2(t),...,z_k(t)$ is a linearly dependent set of solutions to the linear homogeneous differential equations,given by,
$$\dot{x}=f(t,x) = A(t)x(t), x\in \R^n, A(t)\in \R^{n\times n},$$
then $$z(t) \triangleq \sum_{i=1}^k c_iz_i(t)$$ is also a solution. 
\end{lemma}
\begin{proof}
Directly from the linearity of the differential operator. 
\begin{align*}
\frac{d}{dt}z(t) &= \sum_{i=1}^k c_i \frac{d}{dt}z_i(t) \\
&= \sum_{i=1}^k c_i Az_i(t) \\
&= A\sum_{i=1}^k c_i z_i(t) \\
&= Az
\end{align*}
\end{proof}

\begin{theorem}[fundamental solution property of linear homogeneous system]\label{ch:dynamical-systems:th:FundamentalSolutionPropertyLinearHomogeneousSystem}
\cite[18]{sanchez1968ordinary}There exists a set $F$ of $n$ linearly independent solution to the linear homogeneous system of dimension $n$ given by
$$\dot{x}=f(t,x) = A(t)x(t), x\in \R^n, A(t)\in \R^{n\times n},$$
Moreover, any solution can be decomposed as the linear combination of $F$. 
\end{theorem}
\begin{proof}
(1) Consider n 1-of-n hot spot initial conditions, each initial condition combined with the differential equation itself will have a unique solution $\phi_i$, as guaranteed by the uniqueness and existences theorem(\autoref{ch:dynamical-systems:th:existenceUniquenessLinearHomogeneousSystems}). Moreover, this set of solution $\{\phi_1,\phi_2,...,\phi_n\}$ must be linearly independent of each other since they disagree on the point $t=t_0$(i.e. the initial conditions). (2) For any other solution $\psi$ to the equation, let $c=(c_1,c_2,...,c_n)$ be the value of $\psi$ at $t'$, then we can find a linear combination of $\{\phi_1,\phi_2,..,\phi_n\}$ that agree with $\psi$ at $t'$. Again the uniqueness theorem(\autoref{ch:dynamical-systems:th:existenceUniquenessLinearHomogeneousSystems}) will guarantee that this linear combination and $\psi$ are equal to each other. 
\end{proof}

\begin{remark}[implications]\hfill \\
This theorem implies that if we can find $n$ linearly independent solutions, we can construct any other solutions to satisfy any initial conditions. In otherwise, once found $n$ linearly independent solutions, we completely solve the problem.
\end{remark}

\begin{corollary}
The solutions to the linear homogeneous equation form a $n$ dimensional linear space.
\end{corollary}


\subsection{Linear independence and the Wronskian}

\begin{definition}[linear independence of solutions]\cite[40]{luenberger1979introduction}
	Given a set of solutions $x_1,x_2,...,x_k \in \R$ on the interval $[t_0,t_1]$  to the homogeneous linear differential equation $\dot{x}= A(t)x(t)$, we say they are linearly independent if
	$$\sum_{i=1}^k c_i y_i(t) = 0, \forall t\in[t_0,t_1]$$
	only hold when all $c_i = 0$. 
\end{definition}

\begin{definition}[fundamental system/matrix]\label{ch:dynamical-systems:def:fundamentalmatrix}
Let $\phi_1,...\phi_n$ be the solutions to the linear homogeneous system $\dot{x}= A(t)x(t)$ . If they are linearly independent , then they are called the \textbf{fundamental system}. The matrix $\Phi=[\phi_1,...\phi_n]$ is called \textbf{fundamental matrix}.
\end{definition}



\begin{definition}[The Wronskian]\index{The Wronskian}
\cite[22]{sanchez1968ordinary}The Wronskian $W$ is defined as a scalar function for the fundamental matrix given by
$$W[\Phi](t) = det(\Phi(t)).$$
\end{definition}

\begin{lemma}[Wronskian and linear independence]
Give  a set of solutions $x_1,x_2,...,x_k \in \R$ on the interval $[t_0,t_1]$  to the homogeneous linear differential equation $\dot{x}= A(t)x(t)$, we say they are linearly independent if and only if
its Wronskian $W[x_1,x_2,...,x_k](t)\neq 0$, for all $t\in[t_0,t_1]$.
\end{lemma}
\begin{proof}
Based on the definition of linear independence, they are linearly independent if
$$\sum_{i=1}^k c_i y_i(t) = 0, \forall t\in[t_0,t_1]$$
only hold when all $c_i = 0$. This is equivalent to say that 
$$W\triangleq det [x_1,x_2,...,x_k] \neq 0$$
for all $t\in[t_0,t_1]$.
\end{proof}

\begin{remark}
This theorem is not easy to use; the following Liouvilles' formula provides an easy way to check linear independence among solutions.
\end{remark}

\begin{lemma}[Liouvilles' formula for Wronskian]\index{Liouvilles' formula}
\cite[22]{sanchez1968ordinary}\cite[481]{meyer2000matrix}\hfill
\begin{itemize}
	\item Let $W[\Phi]$ be the Wronskian associated with the fundamental matrix of a linear homogeneous system, then
	$$W(t) = W(t_0)\exp[\int_{t_0}^t Tr(A(s))]ds.$$
	\item The Wronskian has the property of: once zero, always zero; once nonzero, always nonzero.
\end{itemize}
\end{lemma}
\begin{proof}
(1) see reference; (2) Since $\exp[\int_{t_0}^t Tr(A(s)ds] \neq 0$, the Wronskian has the property of \textbf{once zero at a point, all zero; once nonzero at a point, all nonzero}.
\end{proof}



\begin{theorem}[necessary and sufficient conditions for fundamental system]
A necessary and sufficient condition for $\phi_1,\phi_2,...,\phi_n$ to be a \textbf{fundamental system} is
that there exists a $t_0 \in (r_1,r_2)$, such that $W(t_0)\neq 0$.
where $(r_1,r_2)$ is the interval where the conditions for existence and uniqueness of solutions are satisfied. 
\end{theorem}
\begin{proof}
Use the above two lemma.
\end{proof}

\begin{example}
The vector-valued function
$$x_1(t) = \exp(5t)\begin{bmatrix}
1\\
1
\end{bmatrix},x_2(t) = \exp(t)\begin{bmatrix}
1\\
-1
\end{bmatrix},$$
are solutions of the differential system
$$\frac{d}{dt}\begin{bmatrix}
x_1\\
x_2
\end{bmatrix} = \begin{bmatrix}
3 & 2\\
2 & 3
\end{bmatrix}\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}.$$
The associated Wronskian is given by
$$W(t)=det\begin{bmatrix}
\exp(5t) & \exp(t) \\
\exp(5t) & -\exp(t) 
\end{bmatrix} = -2\exp(6t) \neq 0.$$	
Therefore, $x_1$ and $x_2$ constitute a fundamental system.
\end{example}



\subsection{The fundamental system and solution method}

\begin{theorem}[solution to initial value problem via fundamental system/matrix property]\label{ch:dynamical-systems:th:propertyoffundamentalmatrixLinearHomogeneousSystem}\hfill
	\begin{itemize}
		\item Let $\Phi(t)$ be the fundamental system, then $$\dot{\Phi}(t) = A(t)\Phi(t), A(t) = \dot{\Phi}(t)\Phi(t)^{-1}.$$
		\item Further more, any solution $x(t)$ satisfying the initial condition $x(t_0)=x_0$ is given as
		$$x(t) = \Phi(t)\Phi^{-1}(t_0)x_0.$$
		\item In particular, if $\Phi(t_0)=I$(i.e., the solution of $\phi_i$ satisfies initial condition of $e_i$), then
		$$x(t) = \Phi(t)x_0.$$
	\end{itemize}	
\end{theorem}
\begin{proof}
	(1) Each column of $\Phi$ is a solution satisfies the equation. (2) First, any solution is a linear combination of the basis solutions(\autoref{ch:dynamical-systems:th:FundamentalSolutionPropertyLinearHomogeneousSystem}), i.e., $x(t) = \Phi(t)c,c\in \R^n$, note that $$x_0 = \Phi(t_0)c\Rightarrow c = \Phi^{-1}(t_0)x_0$$.
\end{proof}

\begin{example}
	The vector-valued function
	$$x_1(t) = \exp(5t)\begin{bmatrix}
	1\\
	1
	\end{bmatrix},x_2(t) = \exp(t)\begin{bmatrix}
	1\\
	-1
	\end{bmatrix},$$
	are solutions of 
Consider the differential system
	$$\frac{d}{dt}\begin{bmatrix}
	x_1\\
	x_2
	\end{bmatrix} = \begin{bmatrix}
	3 & 2\\
	2 & 3
	\end{bmatrix}\begin{bmatrix}
	x_1\\
	x_2
	\end{bmatrix},$$
	with initial condition $x_0 = (4,-2)^T.$
	It has the fundamental matrix given by
	$$\Phi = \begin{bmatrix}
	\exp(5t) & \exp(t) \\
	\exp(5t) & -\exp(t) 
	\end{bmatrix}.$$
Its solution can be written by
\begin{align*}
x(t) &= \Phi(t)\Phi(0)^{-1}x_0 \\
&= \begin{bmatrix}
\exp(5t) & \exp(t) \\
\exp(5t) & -\exp(t) 
\end{bmatrix}\begin{bmatrix}
1 & 1 \\
1 & -1 
\end{bmatrix}^{-1}\begin{bmatrix}
4\\
-2
\end{bmatrix}
\end{align*}
\end{example}



\subsection{The non-homogeneous linear equation}
\begin{mdframed}
\textbf{Note}\\
In this subsection, we discuss the solutions for the linear non-homogeneous equation of dimension $n$:
$$\dot{x} = A(t)x + B(t), x\in \R^n, A(t)\in \R^{n\times n},B(t) \in \R^n.$$
\end{mdframed}


\begin{definition}[state transition matrix]\index{state transition matrix}\cite[114]{luenberger1979introduction}\label{ch:dynamical-systems:def:statetransitionmatrix}
	The state transition matrix for linear system
	$$\dot{x}(t) = A(t)x(t)$$
	is a $n\times n$ matrix function satisfying
	$$\frac{d}{dt}\Phi(t,\tau) = A(t)\Phi(t,\tau), \Phi(\tau,\tau) = I.$$
\end{definition}

\begin{lemma}[state transition matrix derived from fundamental matrix]\label{ch:dynamical-systems:th:linearSystemStateTransitionMatrixViaFundamentalMatrix}
	The state transition matrix for linear system
	$$\dot{x}(t) = A(t)x(t)$$
	is 
	$$\Phi(t,\tau) = X(t)X(\tau)^{-1}$$
	where $X(t)$ is the fundamental matrix.
	Moreover, let $x(t)$ be the solution, then
	$$x(t) = \Phi(t,\tau) x(\tau).$$ 
\end{lemma}
\begin{proof}
(1) We can verify that $\Phi(\tau,\tau) = I$ and 
$$\frac{d}{dt}\Phi(t,\tau) = \frac{d}{dt}X(t)X(\tau)^{-1} = AX(t)X(\tau)^{-1}  = A\Phi(t,\tau).$$
(2)	See \autoref{ch:dynamical-systems:th:propertyoffundamentalmatrixLinearHomogeneousSystem}.
\end{proof}

\begin{example}[state transition matrix for linear system with constant coefficients]
	The linear system with constant coefficients
	$$\dot{x} = Ax$$
	has state transition matrix given as
	$$\Phi(t,\tau) = e^{A(t-\tau)}.$$

It can be verified that $$\Phi(\tau,\tau) = I, \frac{d}{dt}\Phi(t,\tau) =\frac{d}{dt}e^{A(t-\tau)} = Ae^{A(t-\tau)}  = A(t)\Phi(t,\tau).$$	
\end{example}

\begin{theorem}[general solution principle for linear non-homogeneous systems]
	Consider a linear system
$$\dot{x} = A(t)x + B(t), x\in \R^n, A(t)\in \R^{n\times n},B(t) \in \R^n,$$
	The solution is given by the  homogeneous solution $y_H(t)$ satisfying
	$$\dot{y}_H = A(t)y_H$$
	 plus a particular solution $y_P$ satisfy
	$$\dot{y}_P = A(t)y_P + B(t).$$
\end{theorem}
\begin{proof}
	Directly plug in to verify.
\end{proof}

\begin{theorem}[solution to linear non-homogeneous systems via state transition matrix]\label{ch:dynamical-systems:th:generalsolutionlinearsystem}
The solution $x(t)$ to
$$\dot{x} = A(t)x + B(t), x\in \R^n, A(t)\in \R^{n\times n},B(t) \in \R^n,$$
satisfying $x(t) = x_0, r_1<t_0<r_2$, is given by
$$x(t) = \Phi(t,0)x_0 + \int_{t_0}^t \Phi(t,s)B(s)ds, r_1<t<r_2$$
where $\Phi(t,\tau)$ is the state transition matrix.

Particularly, $$y_H(t) = \Phi(t,0)x_0$$ is homogeneous solution, and $$y_P(t) =\int_{t_0}^t \Phi(t,s)B(s)ds .$$
\end{theorem}
\begin{proof}
Take the time derivative, and we get
\begin{align*}
\dot{y}_H(t) &= \frac{d}{dt}\Phi(t,0)x_0 \\
 &= A\Phi(t,0)x_0 \\
 &= Ax(t)
\end{align*}
and
\begin{align*}
\dot{y}_P(t) &= \frac{d}{dt}(\int_{0}^t \Phi(t,\tau)B(\tau)d\tau), r_1<t<r_2 \\
&= \int_0^t \frac{d}{dt} \Phi(t,\tau)B(\tau)d\tau + \Phi(t,t)B(t), r_1<t<r_2 \\
&= \int_0^t  A\Phi(t,\tau)B(\tau)d\tau + \Phi(t,t)B(t), r_1<t<r_2 \\
&= A(t)\int_0^t \Phi(t,\tau)B(\tau)d\tau + \Phi(t,t)B(t), r_1<t<r_2 \\
&= A(t)y_P(t) + B(t), r_1<t<r_2 
\end{align*}
where we use the properties of state transition matrix(\autoref{ch:dynamical-systems:def:statetransitionmatrix}) that
$$\frac{d}{dt}\Phi(t,0)x_0 = A(t)\Phi(t,0)x_0.$$
\end{proof}


\begin{example}
	Consider the differential system
	$$\frac{d}{dt}\begin{bmatrix}
	x_1\\
	x_2
	\end{bmatrix} = \begin{bmatrix}
	2 & -1\\
	3 & -2
	\end{bmatrix}\begin{bmatrix}
	x_1\\
	x_2
	\end{bmatrix} + \begin{bmatrix}
	\exp(t)\\
	\exp(-t)
	\end{bmatrix},$$
	with initial condition $x_0 = (4,-2)^T.$
	It has the fundamental matrix given by
	$$X(t) = \begin{bmatrix}
	\exp(t) & \exp(-t) \\
	\exp(t) & 3\exp(-t) 
	\end{bmatrix}.$$
	Its transition matrix is given by
	$$\Phi(t,0)=X(t)X(0)^{-1} =\begin{bmatrix}
	\exp(t) & \exp(-t) \\
	\exp(t) & 3\exp(-t) 
	\end{bmatrix}\begin{bmatrix}
	1 & 1 \\
	1 & 3 
	\end{bmatrix}^{-1}$$
	The
	solution can be written by
	\begin{align*}
	x(t) = \Phi(t,0)^{-1}x_0 + \int_0^t \Phi(t,\tau)\begin{bmatrix}
	\exp(\tau)\\
	\exp(-\tau)
	\end{bmatrix}d\tau. 
	\end{align*}
\end{example}

\begin{remark}
\textbf{Note on the practical solutions}\\
In practice, the fundamental matrix and state transition matrix are usually difficult to obtain since $A(t),B(t)$ depends on time. If independent of time, then it is possible to solve, as showed in the following sections. 
\end{remark}


\subsection{Conversion of linear differential/difference equation to linear systems}
\begin{note}[conversion of linear difference equation]
Given high order linear difference equation, given as
$z(k+n)+a_{n-1}y(k+n-1)+...+ a_0(k)z(k)=g(k),k=0,1,2,...$,we can use the following procedure to convert to linear system. In particular, let
$$x_i(k) = z(k+i-1), i=1,2,...,n-1$$ and $$x_n = -\sum_{i=0}^{n-1}a_i x_{i+1} + g(k)$$
Then the state vector has $n$ components. 
See \cite[96]{luenberger1979introduction}.
Also, the resulting matrix $A$ is known as \emph{companion matrix}

The matrix $A$ will be
$$\begin{pmatrix}
0 & 1 & 0 & \dots &  & 0\\ 
0 & 0 & 1 & \dots &  & 0\\ 
\vdots &  &  &  &  & \\ 
 &  &  &  &  & \\ 
0 &  &  & \dots & 0 & 1\\ 
-a_n(t)) & -a_{n-1}(t) &  &  &  & -a_1(t)) 
\end{pmatrix}$$
\end{note}


\begin{note}[conversion of linear differential equation]
	Given high order linear differential equation, given as
$$\frac{d^n y}{dt^n} + a_1(t)\frac{d^{n-1} y}{dt^{n-1}} + \cdots + a_1(t)\frac{d^{n-1} y}{dt^{n-1}} + a_n(t)y = f(t) $$	
	,we can use the following procedure to convert to linear system. In particular, let
	$$x = \begin{bmatrix}
	x_1\\
	x_2\\
	\vdots
	x_n
	\end{bmatrix} = \begin{bmatrix}
	y\\
	y'\\
	\vdots
	y^{(n-1)}
	\end{bmatrix}, g(t) = \begin{bmatrix}
	0\\
	0'\\
	\vdots
	f(t)
	\end{bmatrix}$$
	and
	$$A(t)=\begin{bmatrix}
	0 & 1 & 0 & \cdots & 0\\ 
	0 & \ddots & \ddots & \ddots & \vdots\\ 
	\vdots & \ddots & 0 & 1 & 0\\ 
	0 & \cdots & 0 & 0 & 1\\ 
	-a_n(t) & \cdots & -a_3(t) & -a_2(t) & -a_1(t)
	\end{bmatrix}.$$
	then we have
	$$\frac{dx}{dt} = A(t)x + g(t).$$
\end{note}


\subsection{Solution method for discrete system}
\begin{definition}[free/homogeneous system]
A free system is given as 
$$x(k+1)=A(k)x(k)$$
which can be solved recursively once an initial value of the state is given.
\end{definition}

\begin{definition}[state-transition matrix]
\cite[100]{luenberger1979introduction} The state-transition matrix of the homogeneous system is
$$\Phi(k,l) = A(k-1)A(k-2)...A(l),k>l$$
$$\Phi(k,k) = I$$
\end{definition}

\begin{remark}
With the state-transition matrix, the solution to the homogeneous system can be written as $x(k) = \Phi(k,0)x(0)$.
\end{remark}

\begin{theorem}[solution to forced system]
\cite[109]{luenberger1979introduction}Consider the forced system $x(k+1)=A(k)x(k)+B(k)u(k)$, the general solution can be written as
$$x(k) = \Phi(k,0)x(0) + \sum_{i=1}^{k-1} \Phi(k,i+1)B(i)u(i)$$
\end{theorem}
\begin{proof}
It is easy to see that $x(0)=x(0)$, also it can be verified the solution satisfies the original equation by
$$x(k+1)-x(k) = A(k)x(k)-x(k) + B(k)u(k)$$
via subtracting.
\end{proof}


\section{Linear system with constant coefficients}
\subsection{General solutions}
\begin{theorem}[general solution to linear system]\label{ch:dynamical-systems:th:generalsolutionlinearsystemconstantcoeff}
The linear system with constant coefficients
$$\dot{x} = Ax$$
has state transition matrix given as
$$\Phi(t,\tau) = e^{A(t-\tau)}.$$
Then the general solution to  linear system with constant coefficients
$$\dot{x} = Ax + B(t)$$
is given as
$$x(t) = \Phi(t,0)x(0) + \int_0^t \Phi(t,\tau)B(\tau)d\tau.$$	
\end{theorem}
\begin{proof}
(1) It can be showed that $$\frac{d}{dt}e^{A(t-\tau)} = Ae^{A(t-\tau)},\Phi(\tau,\tau)=I,$$ which satisfies the definition of state transition matrix(\autoref{ch:dynamical-systems:def:statetransitionmatrix}).(2) See \autoref{ch:dynamical-systems:th:generalsolutionlinearsystem}.
\end{proof}


\subsection{System eigenvector method: continuous-time system}
\subsubsection{Diagonalizable system}

\begin{lemma}[eigenpair and solutions to linear systems]\label{ch:dynamical-systems:th:EigenPairAndSolutionToLinearSystem}
Consider the system
$$\frac{dx}{dt} = Ax, x\in \R^n, A\in \R^{n\times n}.$$
If $(\lambda, v),v\in \C^n, \lambda\in \C$ is an eigenpair of $A$, then 
$$x(t) = \exp(\lambda t) v$$	
is a solution to the system.
\end{lemma}
\begin{proof}
Note that	
$$\frac{dx}{dt} = \frac{d}{dt}(\exp(\lambda t)v)= \exp(\lambda t)\lambda v = \exp(\lambda t)Av = A\exp(\lambda t)v = Ax,$$
where we use the fact that $\lambda v = Av$.
\end{proof}

\begin{lemma}[conjugate pair for real-value systems]
Consider the system
$$\frac{dx}{dt} = Ax, x\in \R^n, A\in \R^{n\times n}.$$
\begin{itemize}
	\item If $(\lambda, v),v\in \C^n, \lambda\in \C$ is an eigenpair of $A$, then 
	$(\cong{\lambda}, \cong{v})$ is also an eigenpair of $A$.
	\item Denote $\lambda = a + bi, a,b\in \R$, then
	$$x_1 = \exp(at+bit), x_2=\exp(at - bit)$$	
	are both solutions to the system.
	\item Denote $\lambda = a + bi, a,b\in \R$, then
	$$u_1 = \exp(at)\cos(bt), u_2=\exp(at)\sin(bt)$$	
	are both solutions to the system.
\end{itemize}
\end{lemma}
\begin{proof}


(3) Use linearity of solutions(\autoref{ch:dynamical-systems:th:linearityOfSolutionToLinearHomogeneousSystems}). Note that
$$u_1 = \frac{1}{2}(x_1+x_2),u_1 = \frac{1}{2i}(x_1-x_2).$$
\end{proof}

\begin{theorem}[construct complete solution set for diagonalizable system]
	Consider the system
	$$\frac{dx}{dt} = Ax, x\in \R^n, A\in \R^{n\times n},$$
	with $A$ being diagonalizable, i.e.,
	$$A = V\Lambda V^{-1}.$$
It follows that
\begin{itemize}
	\item The fundamental matrix is given by
	$$X(t) = V\exp(t\Lambda).$$
	\item The transition matrix is given by
	$$\Phi(t,\tau) = \exp((t-\tau)A)= V\exp((t-\tau)\Lambda)V^{-1},$$
	particularly, $\Phi(t,0) = V\exp((t)\Lambda)V^{-1}.$
	\item The solution to the system with initial condition $x(\tau)$ is given by
	$$x(t) =\Phi(t,\tau)x(\tau) =V\exp((t-\tau)\Lambda)V^{-1} x(\tau).$$
\end{itemize}	
\end{theorem}
\begin{proof}
(1) Note that
$$\frac{d}{dt}X(t) = \frac{d}{dt}V\exp(t\Lambda) = V\Lambda\exp(t\Lambda) = AV\exp(t\Lambda) = AX(t),$$
plus the fact that $X(t)$ are linearly independent, we have that $X(t)$ is the fundamental matrix.
(2) Note that transition matrix is related to fundamental matrix(\autoref{ch:dynamical-systems:th:linearSystemStateTransitionMatrixViaFundamentalMatrix}) via
$$\Phi(t,\tau) = X(t)X(\tau)^{-1}.$$
(3)	Directly from the property of state transition matrix(\autoref{ch:dynamical-systems:th:linearSystemStateTransitionMatrixViaFundamentalMatrix}).
\end{proof}

\begin{note}[the solution recipe summary]
By solving the characteristic equation associated with the linear system $Ly = 0$, we have
\begin{itemize}
	\item each real simple root $r$ yields the solution 
	$$\exp(rt).$$
	\item each real root $r$ of multiplicity $m$ yields $m$ solutions 
	$$\exp(rt), t\exp(rt), t^2\exp(rt),...,t^{m-1}\exp(rt).$$
	\item each conjugate pair of complex simple root $a\pm bi$ yields two solutions 
	$$\exp(at)\cos(bt), \exp(at)\sin(bt).$$
	\item each conjugate pair of complex root $a\pm bi$ yields $2m$ solutions
	\begin{align*}
	&\exp(at)\cos(bt),t\exp(at)\cos(bt),\cdots,t^{m-1}exp(at)\cos(bt) \\
	&\exp(at)\sin(bt),t\exp(at)\sin(bt),\cdots,t^{m-1}exp(at)\sin(bt)
	\end{align*} 
\end{itemize}	
\end{note}


\begin{example}
Consider the linear system 
$$\dot{x} = Ax, A = \begin{bmatrix}
3 & 2\\
2 & 3
\end{bmatrix}.$$
We have the following decomposition for $A$:
$$A = V\Lambda V^{-1}, V=\begin{bmatrix}
1 & 1\\
-1 & 1
\end{bmatrix},\Lambda=\begin{bmatrix}
1 & 0\\
0 & 5
\end{bmatrix},V^{-1}=\frac{1}{2}\begin{bmatrix}
1 & -1\\
1 & 1
\end{bmatrix}.$$

The transition matrix is given by
$$\Phi(t,0) = \exp(tA) = V\exp(t\Lambda)V^{-1} = \frac{1}{2}\begin{bmatrix}
\exp(5t)+\exp(t) & \exp(5t)-\exp(t)\\
\exp(5t)-\exp(t) & \exp(5t) + \exp(t)
\end{bmatrix}$$	
\end{example}


\begin{example}
	Consider the linear system 
	$$\dot{x} = Ax, A = \begin{bmatrix}
	3 & 2\\
	-2 & 3
	\end{bmatrix}.$$
	We have the following decomposition for $A$:
	$$A = V\Lambda V^{-1}, V=\begin{bmatrix}
	1 & 1\\
	i & -i
	\end{bmatrix},\Lambda=\begin{bmatrix}
	3+i2 & 0\\
	0 & 3-i2
	\end{bmatrix},V^{-1}=\frac{1}{2}\begin{bmatrix}
	1 & -i\\
	1 & i
	\end{bmatrix}.$$
	Note that
	$$\exp(t\Lambda) = \begin{bmatrix}
	\exp((3+i2)t) & 0\\
	0 & \exp((3-i2)t)
	\end{bmatrix} = \exp(3t)\begin{bmatrix}
	\exp(i2t) & 0\\
	0 & \exp(i2t)
	\end{bmatrix}.$$
	The transition matrix is given by
	\begin{align*}
	\Phi(t,0) &= \exp(tA) \\
	& = V\exp(t\Lambda)V^{-1} \\
	& = \frac{1}{2}\begin{bmatrix}
	1 & 1\\
	i & -i
	\end{bmatrix}\exp(3t)\begin{bmatrix}
	\exp(i2t) & 0\\
	0 & \exp(i2t)
	\end{bmatrix}\begin{bmatrix}
	1 & -i\\
	1 & i
	\end{bmatrix} \\
	&=\frac{1}{2}\exp(3t)\begin{bmatrix}
	\exp(i2t)+\exp(-i2t) & -i\exp(i2t)+i\exp(-i2t)\\
	i\exp(i2t)-i\exp(-i2t) & \exp(i2t)+\exp(-i2t)
	\end{bmatrix} \\
	&=\exp(3t)\begin{bmatrix}
	\cos(2t) & \sin(2t)\\
	-\sin(2t) & \cos(2t)
	\end{bmatrix}
	\end{align*}
\end{example}


\begin{theorem}[transformation to uncoupled system for homogeneous system]	Consider the system
	$$\frac{dx}{dt} = Ax, x\in \R^n, A\in \R^{n\times n},$$
	with $A$ being diagonalizable, i.e.,
	$$A = V\Lambda V^{-1}.$$
Let $y = V^{-1}x,$, then
$$\frac{dy}{dt} = \Lambda y,$$
or explicitly in each component,
\begin{align*}
\dot{y}_1 &= \lambda_1 y_1\\
\dot{y}_2 &= \lambda_2 y_2\\
\vdots&\\
\dot{y}_n &= \lambda_n y_n
\end{align*}

The solution to the transformed system is given by
$$y_i(t) = \exp(\lambda_it)y_i(0),i=1,2,...,n.$$

The solution to the original system is given by	
$$x=Vy.$$	
\end{theorem}
\begin{proof}
Note that
\begin{align*}
\frac{dx}{dt} &= Ax \\
\frac{dx}{dt} &= V\Lambda V^{-1}x \\
V^{-1}\frac{dx}{dt} &= \Lambda V^{-1}x \\
\frac{dV^{-1}x}{dt} &= \Lambda V^{-1}x \\
\frac{dy}{dt} &= \Lambda y 
\end{align*}
\end{proof}

\begin{theorem}[confining dynamics in eigenspaces]
	\cite[136]{luenberger1979introduction}
Consider the system
$$\frac{dx}{dt} = Ax, x\in \R^n, A\in \R^{n\times n},$$
with $A$ being diagonalizable, i.e.,
$$A = V\Lambda V^{-1}.$$
\begin{itemize}
	\item If the state vector $x(0)$ is initially lying within subspace of $span(v_1,v_2,...,v_k)$, then it continuous to be the subspace in subsequent time periods.
	\item If the state vector $x(0)$ is initially lying within one-d subspace of $span(v_1)$, then it continuous to be the subspace in subsequent time periods.The solution is given by
	$$x(t) = \exp(\lambda_1 t)y_1(0)v_1.$$
\end{itemize}
\end{theorem}
\begin{proof}
(1) Note that if $x(0)$ is lying within	$span(v_1,v_2,...,v_k)$, then $x(0) = \sum_{i=1}^{k} y_i(0)v_i$. And $x(t) = \sum_{i=1}^{k} y_i(t)v_i$.
(2) Note that
$x(0) =  y_1(0)v_1$. And $$x(t) = y_1(t)v_1 = \exp(\lambda_1 t)y_1(0)v_1.$$
\end{proof}

\begin{theorem}[transformation to uncoupled system for non-homogeneous system]	Consider the system
	$$\frac{dx}{dt} = Ax + g(t), x\in \R^n, A\in \R^{n\times n}, g(t)\in\R^n,$$
	with $A$ being diagonalizable, i.e.,
	$$A = V\Lambda V^{-1}.$$
	Let $y = V^{-1}x, h = V^{-1}g,$ then
	$$\frac{dy}{dt} = \Lambda y,$$
	or explicitly in each component,
	\begin{align*}
	\dot{y}_1 &= \lambda_1 y_1 + h_1(t)\\
	\dot{y}_2 &= \lambda_2 y_2 + h_2(t)\\
	\vdots&\\
	\dot{y}_n &= \lambda_n y_n + h_n(t)
	\end{align*}
	
	The solution to the transformed system is given by
	$$y_i(t) = \exp(\lambda_i t)y_i(0) + \int_0^t \exp(\lambda_i (t-s))h_i(s) ds .$$
	
	The solution to the original system is given by	
	$$x=Vy.$$	
\end{theorem}
\begin{proof}
Note that
\begin{align*}
\frac{dx}{dt} &= Ax + g(t)\\
\frac{dx}{dt} &= V\Lambda V^{-1}x + g(t)\\
V^{-1}\frac{dx}{dt} &= \Lambda V^{-1}x + V^{-1}g(t)\\
\frac{dV^{-1}x}{dt} &= \Lambda V^{-1}x + h(t)\\
\frac{dy}{dt} &= \Lambda y + h(t). 
\end{align*}	
To solve the first order ODE, we use \autoref{ch:dynamical-systems:th:GeneralSolutionFirstOrderLinearODE}.
\end{proof}


\subsubsection{two-by-two non-diagonalizable system }
\begin{lemma}[two-by-two linear system with Jordan matrix]
	The general solution of the system
	$$\frac{dy}{dt} = Jy, J = \begin{bmatrix}
	\lambda & 1\\
	0 & \lambda
	\end{bmatrix}$$	
	is given by	
	$$y = c_1\exp(\lambda t)(tv_1 + v_2) + c_2\exp(\lambda t) v_2,$$
	where
	$$v_1 = \begin{bmatrix}
	1\\
	0
	\end{bmatrix},v_2 = \begin{bmatrix}
	0\\
	1
	\end{bmatrix}.$$
Note that $A$ is a non-diagonalizable matrix with eigenvalue $\lambda$.	
\end{lemma}
\begin{proof}
Note that $v_1$ is the eigenvector associated with the eigenvalue $\lambda$. From \autoref{ch:dynamical-systems:th:EigenPairAndSolutionToLinearSystem}, we know that
$$c_1\exp(\lambda t) v_1$$ will constitute one solution.
 	
To show  $c_1\exp(\lambda t)(tv_1 + v_2)$ is also the solution, we have
\begin{align*}
\frac{d}{dt}(c_1\exp(\lambda t)(tv_1 + v_2)) &= (c_1\exp(\lambda t)\lambda(tv_1 + v_2)) + c_1\exp(\lambda t)\lambda v_1 \\
& = c_1\exp(\lambda t)(\lambda t v_1 + \lambda v_2 +  v_1 ) \\
& = c_1\exp(\lambda t)(\lambda t v_1 + Av_2 - Av_1 + \lambda v_1 ) \\
& = c_1\exp(\lambda t)(A t v_1 + Av_2) \\
& = Ac_1\exp(\lambda t)(t v_1 + v_2)
\end{align*}	
where we use the relation 
$$(A - \lambda I )v_2 = v_1 \implies \lambda v_2 = Av_2 - v_1$$
\end{proof}


\begin{theorem}[two-by-two linear system fundamental matrix]
	Consider the system
	$$\frac{dy}{dt} = Ay, A\in\R^{2\times 2}.$$
	Suppose we have eigenvalue $\lambda$ and vector $v_1$ and $v_2$ satisfying
\begin{align*}
(A-\lambda I)v_1 &= 0 \\
(A-\lambda I)v_2 &= v_1, 
\end{align*}	
then the solution is given by
	$$y = c_1\exp(\lambda t)(tv_1 + v_2) + c_2\exp(\lambda t) v_1.$$	
\end{theorem}
\begin{proof}
	Note that $v_1$ is the eigenvector associated with the eigenvalue $\lambda$. From \autoref{ch:dynamical-systems:th:EigenPairAndSolutionToLinearSystem}, we know that
	$$c_1\exp(\lambda t) v_1$$ will constitute one solution.
	
	To show  $c_1\exp(\lambda t)(tv_1 + v_2)$ is also the solution, we have
	\begin{align*}
	\frac{d}{dt}(c_1\exp(\lambda t)(tv_1 + v_2)) &= (c_1\exp(\lambda t)\lambda(tv_1 + v_2)) + c_1\exp(\lambda t)\lambda v_1 \\
	& = c_1\exp(\lambda t)(\lambda t v_1 + \lambda v_2 +  v_1 ) \\
	& = c_1\exp(\lambda t)(\lambda t v_1 + Av_2 - Av_1 + \lambda v_1 ) \\
	& = c_1\exp(\lambda t)(A t v_1 + Av_2) \\
	& = Ac_1\exp(\lambda t)(t v_1 + v_2)
	\end{align*}	
	where we use the relation 
	$$(A - \lambda I )v_2 = v_1 \implies \lambda v_2 = Av_2 - v_1$$
\end{proof}

\subsubsection{Non-diagonalizable system}








\begin{remark}
	For non-diagonalizable system, see \cite[148]{luenberger1979introduction} and below.
\end{remark}
\begin{note}
	\textbf{Procedures for non-diagonalizable system}\\
	\begin{itemize}
		\item Find the eigenvalues (we are assuming they are real).
		\item For each eigenvalue $\lambda_i$ with multiplicity $\mu_i$, look for $\mu_i$ generalized eigenvectors $v_0,v_1,...,v_{\mu_i}$ by solving $(A-\lambda_i)^\mu_i v = 0$.(The theory on generalized eigenspace guarantees that $\mu_i$ eigenvectors can be found.)
		\item For each generalized eigenvector, we have corresponding solutions of
		$$e^{\lambda_i t}v_0,e^{\lambda_i t}(v_0 t + v_1), e^{\lambda_i t}(v_0 t^2/2! + v_1 t + v_2)...$$
		\item note that by construction, these $n$ solutions are linearly independent, and therefore will span the solution space.
	\end{itemize}
\end{note}

\begin{theorem}[generalized eigenpair as a solution]\label{ch:dynamical-systems:th:generalizedEigenpairSoluitontoLinearSystems}
Consider the system
$$\frac{dx}{dt} = Ax, x\in \R^n, A\in \R^{n\times n},$$
where $A$ might not be \textbf{diagonalizable}. Let $(\lambda, v)$ be a generalized eigenpair of degree $k$ such that
$$(A - \lambda I)^k v = 0.$$

Then $$x(t) = \exp(\lambda t)(v + t(A-\lambda I)v + \cdots + \frac{t^{k-1}}{(k-1)!}(A-\lambda I)^{k-1}v)$$
is a solution.
\end{theorem}
\begin{proof}
Note that
\begin{align*}
(A-\lambda I)x(t) &=\exp(\lambda t)((A-\lambda I)v + t(A-\lambda I)^2v + \cdots + \frac{t^{k-1}}{(k-1)!}(A-\lambda I)^{k}v) \\
&=\exp(\lambda t)((A-\lambda I)v + t(A-\lambda I)^2v + \cdots + \frac{t^{k-2}}{(k-1)!}(A-\lambda I)^{k-1}v) 
\end{align*}

To show it is the solution, we have
\begin{align*}
\frac{dx}{dt} &= \frac{d}{dt}(\exp(\lambda t)((A-\lambda I)v + t(A-\lambda I)^2v + \cdots + \frac{t^{k-2}}{(k-1)!}(A-\lambda I)^{k-1}v)) \\
&=\lambda\exp(\lambda t)((A-\lambda I)v + t(A-\lambda I)^2v + \cdots + \frac{t^{k-2}}{(k-1)!}(A-\lambda I)^{k-1}v) \\
&+ \exp(\lambda t)((A-\lambda I)v + t(A-\lambda I)^2v + \cdots + \frac{t^{k-2}}{(k-1)!}(A-\lambda I)^{k-1}v) \\
&=\lambda x(t) + (A - \lambda I) x(t) \\
&= Ax(t).
\end{align*}
\end{proof}

\begin{theorem}[construct a solution set using a chain of generalized eigenpairs]
	Consider the system
	$$\frac{dx}{dt} = Ax, x\in \R^n, A\in \R^{n\times n},$$
	where $A$ might not be \textbf{diagonalizable}. Let the vectors $v_1,v_2,...,v_k,v_1\neq 0$ be a \textbf{chain of generated eigenvectors of length k} such that
	\begin{align*}
	v_{r-1} &= (A - \lambda I)v_r\\
	v_{r-2} &= (A - \lambda I)v_{r-1}\\
	&\vdots\\
	v_{1} &= (A - \lambda I)v_{2}\\
	0 &= (A - \lambda I)v_{1}.
	\end{align*}	
	
	Then 
	\begin{align*}
	x_1(t) &= \exp(\lambda t)v_1 \\
	x_2(t) &= \exp(\lambda t)(tv_1+v_2) \\
	\vdots & \\
	x_k(t) &= \exp(\lambda t)(v_k + tv_{k-1} + \cdots + \frac{t^{k-1}}{(k-1)!}v_1)
	\end{align*}
	form $k$ linearly independent solutions.
\end{theorem}
\begin{proof}
\autoref{ch:dynamical-systems:th:generalizedEigenpairSoluitontoLinearSystems}	shows that $x_i$ is a solution. And \autoref{linearIndependenceChainofGeneralizedEigenvectors} shows the independence.
\end{proof}


\subsection{System eigenvector method: discrete-time system}
\subsubsection{Discrete-time system}
\begin{theorem}[discrete-time dynamic in one-d eigenspace]
\cite[136]{luenberger1979introduction}Given a homogeneous discrete-time system
$$x(k+1) = Ax(k)$$
If the state vector $x(0)$ is initially aligned with an eigenvector $e_i,x(0)=z_i(0) $of $A$, it continuous to be aligned in subsequent time periods. And evolves as
$$z_i(k+1) = \lambda_i z_i(k) $$
\end{theorem}
\begin{proof}
$x(k+1) = Az_i(k)e_i$ therefore, once initiated in the eigenvector state, it will continue to do so.	
\end{proof}


\begin{remark}
This will hold no matter $A$ can be diagonalized or not. For any matrix, there always exist at least one eigenvector. 
\end{remark}

\begin{theorem}
Given a homogeneous discrete-time system
$$x(k+1) = Ax(k)$$
If $A$ can be diagonalized as
$$A = M\Sigma M^{-1}$$
then
$$x(k) = A^k x(0) = M\Sigma^k M^{-1}x(0)$$
\end{theorem}





\subsection{Equilibrium point}
\subsubsection{Discrete-time system}
\begin{definition}[equilibrium point]\index{equilibrium point}
A vector $\bar{x}$ is an \textbf{equilibrium point} for a dynamic system if
$$\bar{x}(k+1) = f(\bar{x}(k),k)$$
for discrete time system or
$$d\bar{x}/dt = f(\bar{x}(k),k) = 0$$
\end{definition}

\begin{lemma}[equilibrium of linear homogeneous system]
\cite[151]{luenberger1979introduction}
The discrete time system $x(k+1)=Ax(k)$ always has the $x=0$ as an equilibrium point. And if $A$ has eigenvalue of 1, the associated eigenspace are all equilibrium points. 
\end{lemma}
\begin{proof}
(1) $0 = A0$ (2) $Ax = \lambda x,\lambda = 1$.	
\end{proof}


\begin{corollary}[equilibrium of non-homogeneous system]
\cite[151]{luenberger1979introduction}
The dynamical system
$$x(k+1) = Ax(k) + b$$
has equilibrium point $\bar{x}$ given by
$$\bar{x} = (I-A)^{-1}b.$$
If $I-A$ is non-singular, then the equilibrium point is unique; otherwise, there are infinitely many equilibrium point forming an affine subspace.
\end{corollary}

\begin{remark}
We can easily extend to continuous-time system(	\cite[151]{luenberger1979introduction}).
\end{remark}

\subsubsection{Continuous-time system}
\begin{definition}[equilibrium point]\index{equilibrium point}
	A vector $\bar{x}$ is an \textbf{equilibrium point} for a dynamic system if
	$$\bar{x}(k+1) = f(\bar{x}(k),k)$$
	for discrete time system or
	$$d\bar{x}/dt = f(\bar{x}(k),k) = 0$$
\end{definition}

\begin{lemma}[equilibrium of linear homogeneous system]
	\cite[151]{luenberger1979introduction}
	The discrete time system $x(k+1)=Ax(k)$ always has the $x=0$ as an equilibrium point. And if $A$ has eigenvalue of 1, the associated eigenspace are all equilibrium points. 
\end{lemma}
\begin{proof}
	(1) $0 = A0$ (2) $Ax = \lambda x,\lambda = 1$.	
\end{proof}


\begin{corollary}[equilibrium of non-homogeneous system]
	\cite[151]{luenberger1979introduction}
	The dynamical system
	$$x(k+1) = Ax(k) + b$$
	has equilibrium point $\bar{x}$ given by
	$$\bar{x} = (I-A)^{-1}b.$$
	If $I-A$ is non-singular, then the equilibrium point is unique; otherwise, there are infinitely many equilibrium point forming an affine subspace.
\end{corollary}




\subsection{Stability}
\begin{definition}[stability of equilibrium point]\index{stability}
\cite[155]{luenberger1979introduction}
Consider a linear time-invariant system 
$$x(k+1) = Ax(k) + b$$ 
or 
$$\dot{x}(t) = Ax(t) + b.$$
\begin{itemize}
	\item An equilibrium point $\hat{x}$ of 
	is \textbf{stable} if, for \textbf{any} solution $\phi(t)$ and any $\epsilon > 0$, there exists a $\delta > 0$ satisfying: if 
	$$\norm{\phi(0) - \hat{x}} < \delta,$$
	then for all $t>0$,
	$$\norm{\phi(t) - \hat{x}} <\epsilon.$$
	\item An equilibrium point $\hat{x}$ of 
	is \textbf{asymptotically stable} if, for \textbf{any} solution $\phi(t)$, there exists a $\delta > 0$ satisfying: if 
	$$\lim_{t\to \infty}\norm{\phi(t) - \hat{x}} = 0.$$
	\item The equilibrium  point $\hat{x}$ is \textbf{unstable} if there exists a solution $\phi(t)$ such that
	$$\lim_{t\to \infty}\norm{\phi(t)} = \infty.$$ 
\end{itemize}
\end{definition}

\begin{remark}[interpretation]\hfill
\begin{itemize}
	\item 'stable' means that if a solution starts off close to $\hat{x}$, then stays close to $\hat{x}$ for all positive $t$.
	\item 'asymptotic stable' means that if a solution starts off close to $\hat{x}$, then it will eventually stay arbitrarily close to $\hat{x}$ for sufficiently large $t$.
	\item 'asymptotic stable' and 'stable' are neither inclusive nor mutual exclusive concepts. 
\end{itemize}	
\end{remark}





\begin{theorem}[stability condition]
\cite[156]{luenberger1979introduction}A necessary and sufficient condition for an equilibrium point of the system $x(k+1) = Ax(k) + b$ to be asymptotically stable is that the eigenvalues of $A$ all have magnitude less than 1(that is, the eigenvalues must all lie inside the unit circle in the complex plane). If \textbf{at least one eigenvalue has magnitude greater than 1, the equilibrium point is unstable.}
\end{theorem}
\begin{proof}
Let $\hat{x}$ be the equilibrium point,i.e., $\hat{x} = A\hat{x} + b$. We write the dynamical equation as $x(k+1)-\hat{x} = A(x(k) - \hat{x})$, then
$x(k+1)-\hat{x} = A^k(x(0) - \hat{x}).$ From Jordan decomposition and its related theorems, we know that if all eigenvalues of $A$ have $\abs{\lambda_i} < 1$, then $A^m \to 0$, as $m \to \infty$.	
\end{proof}



\begin{theorem}
\cite[157]{luenberger1979introduction}A necessary and sufficient condition for an equilibrium point of the system $\dot{x}(t) = Ax(t) + b$ to be asymptotically stable is that the eigenvalues of $A$ all have negative real parts. If \textbf{at least one eigenvalue has positive real part, the equilibrium point is unstable.}
\end{theorem}
\begin{proof}
Same from Jordan decomposition and its related theorems.
because $e^{tA}$ can be written as power series of $tA$, if every ${tA}^m \to 0$ as $t\to \infty$ (\autoref{ch:linearalgebra:th:convergenceofmatrixpower}). 	
\end{proof}

\begin{theorem}[stability analysis of $2\times 2$ linear homogeneous system]
Consider a $2\times 2$ linear homogeneous system with constant coefficients given by
$$\dot{x} = Ax.$$
It follows that
\begin{itemize}
	\item $\hat{x} = 0$ is the only equilibrium point.
	\item If $A$ has two real eigenvalues $r_1,r_2$, then $r_1,r_2<0$ ensures that $\hat{x} = 0$ be the asymptotically stable point.
	\item If $A$ has two complex eigenvalues $a\pm bi$, then $a<0$ ensures that $\hat{x} = 0$ be the asymptotically stable point.
	\item If $A$ is non-diagonalizable with a single eigenvalue $r\in \R$, then $r<0$ ensures that $\hat{x} = 0$ be the asymptotically stable point.
\end{itemize}	
\end{theorem}
\begin{proof}
(1)  (2) The general solution takes the form
$$x(t) = c_1\exp(r_1t)v_1 + c_2\exp(r_2t)v_2.$$
Only when both $r_1,r_2<0$, we have asymptotically stable $\hat{x}= 0$.
(3) The general solution takes the form
$$x(t) = c_1\exp(at)\cos(bt) + c_2\exp(at)\sin(bt).$$
Only when both $a<0$, we have asymptotically stable $\hat{x}= 0$.
(4) The general solution takes the form
$$x(t) = c_1\exp(rt)v_1 + c_2(t\exp(rt)v_1 + \exp(rt)v_2 ),$$
where $v_1$ is eigenvector associated with the eigenvalue $r$, and $v_2$ is the solution of 
$$(A - rI)v_2 = v_1.$$
Only when $r<0$, we have asymptotically stable $\hat{x}= 0$.
\end{proof}



\subsection{Complex eigenvalues/eigenvectors}
For diagonalizable \textbf{real-valued }system, it might happens that there are complex eigenvalues and complex eigenvectors. We can convert them to real-valued basis vectors and $2\times 2$ block matrix. See the linear algebra chapter for more details.


\subsection{Boundedness  of linear systems}


\begin{lemma}[boundedness of linear systems]\cite[344]{chirikjianstochastic}
Let $A\in \R^{n\times n}$ be a constant matrix and $x(t),g(t)\in\R^n$ be vector-valued function of time. Then,
\begin{itemize}
	\item The solution to 
	$$\frac{dx}{dt} = Ax + g(t), x(0) = x_0$$
	is
	$$x(t) = \exp(At)x_0 + \int_0^t \exp(A(t-\tau))g(\tau)d\tau.$$
	\item If $0>-a>Re(\lambda_i(A)),\forall i$, then there exists some positive constant scalar $c$ such that
	$$\norm{x(t)} \leq ce^{-at}\norm{x_0} + c\int_0^t e^{-a(t-\tau)}\norm{g(\tau)}d\tau.$$
	\item If $\norm{g(\tau)}\leq \gamma,\forall \tau$ for some scalar constant $\gamma$, then $\norm{x(t)}$ will be bounded.
	\item If $$f(t) = \int_0^t e^{a\tau} \norm{g(\tau)} d\tau$$
	is bounded by a constant, then $\norm{x(t)}$ will decay to zero as $t\to \infty$.
\end{itemize}
\end{lemma}
\begin{proof}
(1) see \autoref{ch:dynamical-systems:th:generalsolutionlinearsystemconstantcoeff}. (2)
	\begin{align*}
	\norm{x(t)} &= \norm{\exp(At)x_0 + \int_0^t \exp(A(t-\tau))g(\tau)d\tau} \\
	&\leq \norm{\exp(At)x_0} + \norm{\int_0^t \exp(A(t-\tau))g(\tau)d\tau} \\
	&\leq \norm{\exp(At)x_0} + \int_0^t\norm{ \exp(A(t-\tau))}\norm{g(\tau)}d\tau \\
    &\leq ce^{-at}\norm{x_0} + c\int_0^t e^{-a(t-\tau)}\norm{g(\tau)}d\tau
	\end{align*}
where we have use properties of norm \autoref{ch:functional-analysis:th:operatornormproperties}. 
(3)(4) easy.
\end{proof}

\section{Nonlinear system analysis}


\subsection{One dimensional dynamical system analysis}


\begin{definition}[fixed point, equilibrium point and stability classification]\cite[18]{strogatz2014nonlinear}
Consider a one-dimensional dynamical system given by
$$\dot{x} = f(x).$$
The solution $x^*$ such that $f(x^*) = 0$ is called the \textbf{fixed point} or \textbf{equilibrium point}.

An equilibrium point $x^*$ is \textbf{locally stable} if a small deviation from $x^*$ will decay to zero;  An equilibrium point $x^*$ is \textbf{globally stable} if arbitrary deviation from $x^*$ will decay to zero; An equilibrium point is not locally stable is called \textbf{unstable}.
\end{definition}


\begin{lemma}[linear stability analysis]\cite[18]{strogatz2014nonlinear}\label{ch:dynamical-systems:th:linearStabilityAnalysisNonlinearOneDimensional}
	Consider a one-dimensional dynamical system given by
	$$\dot{x} = f(x).$$
Let $x^*$ be an equilibrium point. Assume $f'(x^*) \neq 0$. 
Then if $f'(x^*) < 0$, $x^*$ is locally stable; if $f'(x^*) > 0$, $x^*$ is unstable.

The quantity $1/\abs{f'(x^*)}$ is called \textbf{characteristic time scale}, describing how fast a deviation decays when the system being disturbed from $x^*$.
\end{lemma}
\begin{proof}
Consider the dynamics of deviation $y$ such that
$$d(x^* + y)/dt = dy / dt= f(x^* + y) = f(x^*) + f'(x^*)y + O(y^2) = f'(x^*)y + O(y^2).$$
When $y$ is small, we have approximation 
$$\dot{y} \approx f'(x^*)y,$$
with solution $y(0) = \exp(f'(x^*)t)y(0)$.
If $f'(x^*) > 0$, the small deviation will grow; if $f'(x^*) < 0$, vice versa.
\end{proof}

\begin{remark}[the vanishing first order derivative]
If the first order derivative is zero, then we need to conduct higher order nonlinear stability analysis to determine the stability of the equilibrium point. 
\end{remark}


\begin{example}\cite[327]{luenberger1979introduction}
Consider the 1D system
$$\dot{x}(t) = ax(t) + cx(t)^2.$$
$x = 0$ is an equilibrium point. Linearize about $x = 0$, we have
$$\dot{y} = ay.$$
Therefore, 
\begin{itemize}
	\item $a<0$, $x=0$ is locally stable.
	\item $a>0$, $x = 0$ is unstable.
	\item $a=0$, cannot tell from linear analysis.
\end{itemize}
\end{example}




\section{Notes on bibliography}

For an introduction, see \cite{luenberger1979introduction}.
For an excellent mathematical treatment on linear differential equation, see\cite{hirsch1974differential}


For dynamical system(with mathematical theory), see\cite{hirsch2012differential}\cite{perko2013differential}.

For high order linear differential equations, see \cite{Earl1989introduction}.
For advanced level treatment in ordinary differential equations, see \cite{coddington1955theory}.

For a concise yet deep treatment, see \cite{sanchez1968ordinary}.

For deep treatment in linear ordinary differential equations, see \cite{coddington1955theory}.

For financial applications, see \cite{zhu2004derivative}.


For excellent treatment on linear difference equation, see \cite{jerri2013linear}.
\printbibliography


\end{refsection}
\begin{refsection}

\chapter{Optimal Estimation in Dynamical systems}\label{ch:estimation-in-dynamical-systems}
\minitoc
\section{Least square estimation of constant vectors}\index{least square}
\subsection{linear static estimation from single measurement with no prior information}
\begin{definition}[linear static estimation with no prior information]
Considering a random variable $X$ taking values in $\R^n$, the problem of estimating $x$ given measurement of $X$ taking values in $\R^k$ using proposed linear model
$$z = Hx + v$$
where $H$ is known, $v$ is the measurement error, is known as linear static estimation problem.
\end{definition}

\begin{theorem}\cite[302]{stengel2012optimal}
Given a linear static estimation problem with no prior information, by minimizing
$$\min_{\hat{x}} J = \frac{1}{2}(z - H\hat{x})^T(z - H\hat{x}^T)$$
we can obtain the least square estimate of
$$\hat{x} = (H^TH)^{-1}H^Tz$$
under the sufficient condition of $\nabla^2 J = H^TH$ being positive definite.	
\end{theorem}
\begin{proof}
It is easy to show that first order necessary condition of the optimization problem requires
$$\hat{x} = (H^TH)^{-1}H^Tz$$ and $\nabla^2 J = H^TH$.
\end{proof}

\begin{lemma}[condition for $H^TH$ to be positive definite]
$H$ is a $k\times n$ matrix. For the definiteness of $H^TH$, we have following situations:
\begin{itemize}
	\item $k \geq n$ and $rank(H) = n$($H$ has full column rank)
	\item If $k < n$,  then $H^TH$ cannot be positive definite.
\end{itemize}
\end{lemma}
\begin{proof}
(1) If $k \geq n$ and $rank(H) = n$, then from \autoref{ch:linearalgebra:ranklemmatwo}, we have $rank(H^TH) = rank(H)$. Therefore $H^TH$ is full rank, $H^TH$ must be positive definite.
(2) since $rank(H^TH) = rank(H) \leq k < n$, $H^TH$ must be singular. 
\end{proof}

\begin{remark}[interpretation]\hfill
\begin{itemize}
	\item The situation $k < n$ means that the measurement process will cause the lost of information, and therefore the true values cannot be recovered.
\end{itemize}
\end{remark}



\subsection{linear static estimation from single measurement with prior information}
\begin{definition}[linear static estimation withs prior information]
	Considering a random variable $X$ taking values in $\R^n$, the problem of estimating $x$ given one measurement $z$ of $X$ taking values in $\R^k$ with prior information about $X$ given as
	$$E[X] = x_0, E[(X-EX)(X-EX)^T] = P_0$$
	and prior information about measure error
	$$E[v]= 0, E[vv^T] = R$$
		 using proposed linear model
	$$z = Hx + v$$
	where $H$ is known, $v$ is the measurement error, is known as linear static estimation problem.
\end{definition}


\begin{theorem}\cite[308]{stengel2012optimal}
	Given a linear static estimation problem with prior information, by minimizing
	
	
	$$\min_{\hat{x}} J = \frac{1}{2}(x - x_0)^TP_0^{-1}(x - x_0) + \frac{1}{2}(z-Hx)^TR^{-1}(z - Hx)$$
	we can obtain the least square estimate of
	$$\hat{x} = (H^TR^{-1}H + P_0^{-1})^{-1}(H^TR^{-1}z + P_0^{-1}x_0)$$
	under the sufficient condition of $\nabla^2 J = H^TR^{-1}H + P_0^{-1}$ being positive definite.
\end{theorem}
\begin{proof}
	It is easy to show that first order necessary condition of the optimization problem gives
	$$P^{-1}_0(\hat{x} - x_0) + H^TR^{-1}(z - H\hat{x}) = 0$$
	then we can solve
		$$\hat{x} = (H^TR^{-1}H + P_0^{-1})^{-1}(H^TR^{-1}z + P_0^{-1}x_0)$$
	It can also be showed that $\nabla^2 J = H^TR^{-1}H + P_0^{-1}$.
\end{proof}

\begin{remark}[least square solution as Maximum a posteriori estimation in Bayesian statistics]
Let $$p(x) \propto \exp(-\frac{1}{2}(x-x_0)^TP_0^{-1}(x-x_0))$$ be the prior model and data generation model is given as $$p(z|x)\propto \exp(-\frac{1}{2}(z-Hx)^TR^{-1}(z-Hx)) $$
then 
$$\hat{x} = \max_x p(x|z) = \max_x p(x)p(z|x)$$
(using log function)\\

Moreover, if prior information is not available, which is equivalent to set $p(x)$ as uniform distribution, then least square solution with no prior information gives
$$\hat{x} = \max_x p(x|z) = \max_x p(x)p(z|x) =  \max_x p(z|x)$$

\end{remark}


\subsection{Batch least square with multiple measurements}
\begin{mdframed}
	\begin{itemize}
		\item When we are given $m(m\geq 1)$ measurements $z_i\in\R^k$ of one single $x\in\R^n$, one can concatenate $z=[z_1^T, z_2^T, ..., z_m^T ]^T$ and measurement model matrix $H=[H_1;H_2;...;H_m]$, and then estimate $x$ using $H$ and $z$.
		\item When we are given $m(m\geq 1)$ measurements $z_i\in\R^k$ of $m$ different $x_i\in\R^n$, there are two special cases in the estimation problem:
		\begin{itemize}
			\item Each $x_i$ is independent of the other. In this case, we can individually estimate $x_i$ using single measurement method.
			\item $x_i$ are generated from a first-order dynamical model, then we can estimate $x_i$ using Kalman filter.
		\end{itemize}
	\end{itemize}
\end{mdframed}



\subsection{Recursive least square algorithms}\index{recursive least square}\label{ch:estimation-in-dynamical-systems:th:sec:recursiveleastsquare}
 When we are given $m(m\geq 1)$ measurements $z_i\in \R^k$ of one single $x\in \R^n$, one can estimate $x$ using following recursive algorithm:\cite[313]{stengel2012optimal}
 
\begin{algorithm}[H]
	\SetAlgoLined
	\KwIn{Given prior on $X$ with $EX = x_0, Cov(X) = P_0$}
	For each new measurement $z_i = H_ix + v_i$, where $V_i\sim N(0,R_i)$ is known
	
	$$\hat{x}_i = \hat{x}_{i-1} + K_i(z_i - H_i\hat{x}_{i-1})$$
	$$P_i = (P_{i-1}^{-1} + H_i^TR_i^{-1}H_i)^{-1}$$
	$$K_i = P_iH_i^TR_i^{-1}$$
	(note that update of $P_i$ can be made efficient by using matrix inversion lemma.)
	
		\caption{Recursive linear least square}
	\KwOut{The final estimate $x_m$}
\end{algorithm}

\begin{lemma}[Fundamental lemma of recursive least squares]\index{Fundamental lemma of recursive least squares}\cite[160]{bertsekas2016nonlinear}
Let $y_1,y_2$ be given vectors, and $A_1,A_2$ be given matrices such that $A_1^TA_1$ is positive definite. Then the vectors
$$z_1 = \arg\min_{x\in \R^n} \norm{y_1 - A_1x}^2$$
and 
$$z_2 = \arg\min_{x\in \R^n} (\norm{y_1 - A_1x}^2 + \norm{y_2-A_2x})$$
are also given by 
$$z_1 = z_0 + (A_1^TA_1)^{-1}A_1^T(y_1 - A_1z_0)$$
and
$$z_2 = z_1 + (A_1^TA_1 + A_2^TA_2)^{-1}A_2^T(y_1 - A_1z_1)$$
\end{lemma}
\begin{proof}
(1) use the result of normal equation $z_1 = (A_1^TA_1)^TA_1^T $ (\autoref{ch:functional-analysis:th:normalequation}) can verify the alternative form of $z_1$ is equivalent.	
(2) Again use normal equation and splitting technique, we have
$$z_2 = (A_1^TA_1 + A_2^TA_2)^{-1}(A_1^Ty_1 + A_2^Ty_2)$$
Use $Ay_1 = A_1^TA_1z_1$ and we can get the result.
\end{proof}




\begin{remark}[interpretation and significance]\hfill
\begin{itemize}
		\item For a multidimensional least square problem, we can always split into smaller least square problems. For example, we can split
		$$y = Ax$$ into
		$$\begin{bmatrix}
		y_1\\
		y_2
		\end{bmatrix} = \begin{bmatrix}
		A_1\\
		A_2
		\end{bmatrix} x$$
		\item \textbf{solutions to smaller least square problems are reusable} by correcting the existing results.(see how $z_2$ is obtained from $z_1$.)
\end{itemize}

\end{remark}



\begin{lemma}[Equivalence between batch  and recursive least square]\cite[160]{bertsekas2016nonlinear}
	Given $m(m\geq 1)$ measurements $z_i\R^k$ of one single $x\in \R^n$ and the measurement model $z_i = H_ix_i + v_i$, one can estimate $x$ using batch least square and recursive least square will give the same result.(Therefore the recursive algorithm is correct.)
\end{lemma}
\begin{proof}
The recursive method is equivalent to solve the following series of least square problems:
\begin{align*}
x_1 = \arg \min_x & \norm{z_1 - Hx}^2 + (x - x_0)^TP_0^{-1}(x - x_0) \\
x_2 = \arg \min_x & \norm{z_1 - Hx}^2 + \norm{z_2 - Hx}^2+ (x - x_0)^TP_0^{-1}(x - x_0) \\
\dots & \dots \\
\end{align*}
Then $x_n$ is equivalent to the minimizer of the batch least square problem.
\end{proof}

\begin{remark}[advantages of recursive method]\hfill
	\begin{itemize}
		\item Recursive method has the advantage of avoiding inversion of large matrix $H^TH$. 
		\item Recursive method can have fast convergence rate when the measurement data $z_i$ are 'similar'. (That is, extra measurements does not provide new information.)
	\end{itemize}
\end{remark}




\subsection{Nonlinear least square estimation}


\begin{algorithm}[H]
	\SetAlgoLined
	\KwIn{Given prior on $X$ with $EX = x_0, Cov(X) = P_0$}
	For each new measurement $z_i = h_i(x) + v_i$, where $V_i\sim N(0,R_i)$ is known
	
	$$\hat{x}_i = \hat{x}_{i-1} + K_i(z_i - H_i\hat{x}_{i-1})$$
	$$P_i = (P_{i-1}^{-1} + H_i^TR_i^{-1}H_i)^{-1}$$
	$$K_i = P_iH_i^TR_i^{-1}$$
	where $H_i = \frac{\Pa h_i}{\Pa x}|_{x=\hat{x}_{i-1}}$(note that update of $P_i$ can be made efficient by using matrix inversion lemma.)\\
	\KwOut{The estimation $\hat{x}_i,i=1,2,...,N$}
	\caption{Recursive nonlinear least square}
\end{algorithm}

\begin{remark}[multiple cycles can improve estimation results]\hfill
\begin{itemize}
	\item For nonlinear system, multiple cycles are needed. For example, the second cycle will use the filtered results from first cycle. Every new cycle can improve the result until cannot be improved. Moreover, multiple cycles cannot guarantee global optimal result can be achieved.
	\item For linear system, one cycle will guarantee optimal results to be achieved. 
\end{itemize}
\end{remark}

\section{Error Propagation in linear systems}\index{error propagation}
\subsection{Discrete-time system}
\begin{theorem}[Error propagation theorem]\label{ch:estimation-in-dynamical-systems:th:errorpropogation}\cite[318]{stengel2012optimal}
Given a discrete-time linear system,  
$$x_k = \Phi_{k-1}x_{k-1} + \Gamma_{k-1}u_{k-1} + \Lambda_{k-1}w_{k-1}$$
where $E[w_k] = 0, E[w_kw_k^T] = Q_k$, and the noise vector is independent. If $x_{k-1} \sim MN(\hat{x}_{k-1},P_{k-1})$, 
 Then
the distribution of $x_k$ is still multivariate normal with mean and covariance given as
$$\hat{x}_k = \Phi_{k-1}\hat{x}_{k-1} + \Gamma_{k-1}u_{k-1}$$
$$P_k = \Phi_{k-1}P_{k-1}\Phi_{k-1}^T + \Lambda_{k-1}Q_{k-1}\Lambda_{k-1}^T$$
\end{theorem}
\begin{proof}
Directly from \autoref{ch:theory-of-statistics:th:affinetransformmultivariatenormal} and \autoref{ch:theory-of-statistics:th:sumofmultivariatenormalwithjointnormality}, the new covariance matrix is the affine transformation of the origin covariance matrix plus the error convariance matrix.
\end{proof}

\subsection{Continuous-time system}
\begin{lemma}[discrete-time approximation and error propagation]\cite[326,336]{stengel2012optimal}
Given a continuous-time linear system
$$\dot{x}(t) = F(t)x(t) + G(t)u(t) + L(t)w(t)$$
its discrete-time approximation is given as
$$x(t_k) = \Phi(t_{k-1},t_k)x(t_{k-1}) + \int_{t_{k-1}}^{t_{k} }\Phi(t_{k-1},\tau)(\Gamma(\tau)u(\tau) + L(\tau)w(\tau)) d\tau,$$
where $\Phi(t_{k-1},t_k)$ is the state-transition matrix(\autoref{ch:dynamical-systems:def:statetransitionmatrix}).  

Note that 
$$\hat{x}_k = E[x_{k}] = \Phi(t_{k-1},t_k)\hat{x}_{k-1} + \int_{t_{k-1}}^{t_{k} }\Phi(t_{k-1},\tau)\Gamma(\tau)u(\tau) d\tau.$$
\end{lemma}
%$$\Phi_{k-1} = \Phi(t_{k-1},t_{k-1}+\Delta t)$$


%$$\int_{t_{k-1}}^{t_{k-1}+\Delta t}$$ 

\section{Kalman filer}
\subsection{Kalman filter for linear system}\index{Kalman filter}
Consider the observation $z_i \in \R^k$ is generalized from the following model:
$$x_{k} = \Phi_{k-1} x_{k-1} + \Gamma_{k-1}u_{k-1} + \Lambda_{k-1}w_{k-1}$$
$$z_k = H_kx_k + n_k$$
where $x_i\in \R^n$, $w_k\sim MN(0,Q_k),n_k\sim MN(0,R_k)$. We can use following Kalman filtering algorithm to infer $x_i$.\cite[342]{stengel2012optimal}

\begin{algorithm}[H]
	\SetAlgoLined
	\KwIn{Given prior on $X$ with $EX = x_0, Cov(X) = P_0$}
	For each new measurement $z_i$\\
	Prediction:
	$$\hat{x}_{k|k-1} = \Phi_{k-1}\hat{x}_{k-1} + \Gamma_{k-1}u_{k-1}$$
	$$P_{k|k-1} = \Phi_{k-1}P_{k-1}\Phi_{k-1}^T + \Lambda_{k-1}Q_{k-1}\Lambda_{k-1}^T$$
	
	Correction:
	$$\hat{x}_k = \hat{x}_{k|k-1} + K_k(z_i - H_k \hat{x}_{k|k-1})$$
	$$P_k = (P_{k|k-1}^{-1} + H_k^TR_k^{-1}H_k)^{-1}$$
	$$K_k = P_kH_k^TR_k^{-1}$$


	note that update of $P_i$ can be made efficient by using matrix inversion lemma.
	
	\KwOut{The estimation $\hat{x}_i,i=1,2,...,N$}
	\caption{Kalman filtering}
\end{algorithm}


\begin{remark}[Bayesian interpretation of Kalman filtering]
	The goal is to obtain 
	$$p(x_k|z_k,x_{k-1}) \propto p(z_k|x_k) p(x_k|x_{k-1})$$
	Since $p(z_k|x_k)$ and $p(x_k|x_{k-1})$ are all multivariate normal distribution, the prediction step is perform estimation of $x_k$ by maximizing $p(x_k|x_{k-1})$ using error propagation theorem(\autoref{ch:estimation-in-dynamical-systems:th:errorpropogation}).\\ 
	The correction step is maximizing $p(x_k|z_k,x_{k-1})$ with prior information. 
\end{remark}

\begin{remark}[optimality of Kalman filter] Kalman filter is the best linear filter in terms of norm-2.
\end{remark}



\begin{remark}[Kalman smoother]\index{Kalman smoother}
	The measurement obtained later can also be used to smooth previous $x$ estimations. This is known as Kalman smoother.\cite{sarkka2013bayesian}
\end{remark}

\subsection{Extended Kalman filter for nonlinear system}\index{extended Kalman filter}
Consider the observation $z_i \in \R^k$ is generalized from the following model:
$$x_{k} = \Phi_{k-1} x_{k-1} + \Gamma_{k-1}u_{k-1} + \Lambda_{k-1}w_{k-1}$$
$$z_k = h_k(x_k) + n_k$$
where $$x_i\in \R^n, w_k\sim MN(0,Q_k),n_k\sim MN(0,R_k).$$ We can use following Kalman filtering algorithm to infer $x_i$:\cite[342]{stengel2012optimal}
\begin{algorithm}[H]
	\SetAlgoLined
	\KwIn{Given prior on $X$ with $EX = x_0, Cov(X) = P_0$}
		For each new measurement $z_i$\\
		Prediction:
		$$\hat{x}_{k|k-1} = \Phi_{k-1}\hat{x}_{k-1} + \Gamma_{k-1}u_{k-1}$$
		$$P_{k|k-1} = \Phi_{k-1}P_{k-1}\Phi_{k-1}^T + \Lambda_{k-1}Q_{k-1}\Lambda_{k-1}^T$$
		
		Correction:
		$$\hat{x}_k = \hat{x}_{k|k-1} + K_k(z_i - H_k \hat{x}_{k|k-1})$$
		$$P_k = (P_{k|k-1}^{-1} + H_k^TR_k^{-1}H_k)^{-1}$$
		$$K_k = P_kH_k^TR_k^{-1}$$
	where $H_i = \frac{\Pa h_i}{\Pa x}|_{x=\hat{x}_{i-1}}$	
	(note that update of $P_i$ can be made efficient by using matrix inversion lemma.)\\

	\KwOut{The estimation $\hat{x}_i,i=1,2,...,N$}
	\caption{Extended Kalman filter}
\end{algorithm}

\begin{remark}[multiple cycles can improve estimation results]\hfill
	\begin{itemize}
		\item For nonlinear system, multiple cycles are needed. For example, the second cycle will use the filtered results from first cycle. Every new cycle can improve the result until cannot be improved. Moreover, multiple cycles cannot guarantee global optimal result can be achieved.
		\item For linear system, one cycle will guarantee optimal results to be achieved. 
	\end{itemize}
\end{remark}


\section{Particle filer}\index{paritcle filter}
\begin{mdframed}
\textbf{general remarks}
\begin{itemize}
	\item Particle filter is a sequential Monte Carlo algorithm for online Bayesian inference task.
	\item The dynamic model will not be limited to linear model and Gaussian distribution.
	\item The computation is expensive.
\end{itemize}
\end{mdframed}



\section{Notes on bibliography}

For optimization point of view on least square, see chapter 1 of \cite{bertsekas2016nonlinear}.

For systematic treatment, see \cite{kailath2000linear}\cite{crassidis2011optimal}.

For sequential Monte Carlo and particle filtering, see \cite{bruno2013sequential}.

For treatment from Bayesian point of view, see \cite{sarkka2013bayesian}

\printbibliography

\end{refsection}

