\startcontents[chapters]
\begin{refsection}
\chapter{Sets}\label{ch:sets}
%\minitoc

\printcontents[chapters]{}{1}{}
\section{Properties of sets}
\subsection{DeMorgan's Law}\index{DeMorgan's Law}
\begin{lemma}[Demorgan's law]\label{ch:sets:th:DemorganLaw}
\cite{johnsonbaugh2010foundations}Let $S$ and $T$ be sets, then
\begin{enumerate}
    \item $(S\cup T)^c = S^c \cap T^c$
    \item $(S\cap T)^c = S^c \cup T^c$
\end{enumerate}
Moreover, given a collection of sets indexed by $I$, we have
$$(\cup_{i\in I} A_i)^C = (\cap_{i\in I} A_i^C),$$ and $$(\cap_{i\in I} A_i)^C = (\cup_{i\in I} A_i^C).$$ 
\end{lemma}


\begin{lemma}[principle of inclusion exclusion]\index{principle of inclusion exclusion}
Let $A_1,A_2,...,A_n$ be sets, then
$$\abs{\cup_{i=1}^n A_i} = \sum_{i=1}^n (-1)^{i+1}S_i,$$
where
\begin{align*}
S_1 &= \sum_{i=1}^n \abs{A_i}\\
S_2 &= \sum_{1\leq i < j \leq n} \abs{A_i\cap A_j}\\
\cdots & = \cdots \\
S_m &= \sum_{1\leq i_i < i_2 < ... <i_m \leq n} \abs{A_{i_1}\cap A_{i_2}\cap ... \cap A_{i_m}}.
\end{align*}
More specifically, 
$$\abs{A\cup B} = \abs{A} + \abs{B} - \abs{A\cap B},$$
and
$$\abs{A\cup B\cup C} = \abs{A} + \abs{B} + \abs{C}- \abs{A\cap B}-\abs{C\cap B}-\abs{A\cap C}+\abs{A\cap B\cap C}.$$
\end{lemma}


\subsection{Algebra properties of sets}
\begin{lemma}
Sets have the following Algebraic properties:
\begin{itemize}
    \item Commutative: $A\cup B = B\cup A$, $A\cap B = B\cap A$
    \item Associative: $A \cup (B \cup C) = (A \cup B) \cup C$, $A \cap (B \cap C) = (A \cap B) \cap C$
    \item Distributive: $A\cup (B\cap C) = (A\cap B) \cup (A \cap C)$,$A\cap (B\cup C) = (A\cup B) \cap (A \cup C)$
\end{itemize}
\end{lemma}


\section{Functions}
\subsection{Basic concepts}
\begin{definition}[function]\index{function}
\cite{johnsonbaugh2010foundations}If $X$ and $Y$ are sets. A \emph{function} from $X$ to $Y$ is a subset $f$ of $X \times Y$ satisfying:
\begin{enumerate}
    \item Uniqueness of mapping: If $(x,y)$ and $(x,y')$ belong to $f$, then $y=y'$.
    \item Completenss: If $x\in X$, then $(x,y)\in f$ for some $y \in Y$. Every element $x$ in $X$ must have a $y \in Y$. 
\end{enumerate}
\end{definition}


\begin{definition}
\cite{johnsonbaugh2010foundations}.
For a function $f:X\to Y$, we have
\begin{itemize}
    \item $X$ is called the domain, $Y$ is called the codomain. $f(X)=\{f(x)|x\in X\}$ is called the range. 
    \item $f$ is \textbf{onto} $Y$ if $f(X) = Y$. Or equivalently, for any $y\in Y$, there exists $x \in X$(not necessarily unique) such that $f(x) = Y$.
    \item $f$ is \textbf{one-to-one} if $f(x) = f(x') \Rightarrow x = x'$
    \item If $f$ is one-to-one function, we can define $f^{-1}$ as a function from $f(X)$ to $X$. Note that it is not from $Y$, but from $f(X)$.  \emph{Onto} is not required for the existence of $f^{-1}$.
    \item If $f$ is one-to-one and onto, it is \textbf{bijective}.
    \item The \emph{inverse image} of $B \subseteq Y$ under $f$ is the set
    $$f^{-1}(B)=\{x|f(x)\in B\}$$
\end{itemize}
\end{definition}

\begin{definition}[inverse function]\index{inverse function}
Denote $f$ as a function $f:X\to Y$. 
\begin{itemize}
	\item If $f$ is one-to-one function, we can define $f^{-1}$ as a function from $f(X)$ to $X$. Note that it is $f^{-1}$ is not mapped from $Y$,
	\item If $f$ is one-to-one and onto function, we can define $f^{-1}$ as a function from $Y$ to $X$.
\end{itemize}
\end{definition}


\subsection{Inverse image vs. inverse function}\index{inverse image}\index{inverse function}
\begin{note}
Note that \emph{inverse image} and \emph{inverse function} are fundamentally different. Inverse image always exist whereas inverse function requires 1-1 to exist. 
\end{note}




\begin{example}
Take $X=Y=\R$, let $f(x) = cos(x)$. Then
\begin{itemize}
    \item inverse function $f^{-1}$ does not exist
    \item $f^{-1}(1)$ technically make no sense since inverse image will only take subset as input
    \item $f^{-1}(\{1\})=\{\text{all integer multiples of }2\pi \}$
    \item $f^{-1}(\{1\})=\emptyset$
    \item $f^{-1}([-1, 1])=\R$
    
\end{itemize}
\end{example}

\subsection{Set operations in function mapping}
\begin{lemma}[Preserving set operators in function mapping]\cite[7]{johnsonbaugh2010foundations}
Let $f$ be a function from $X$ into $Y$. Let $\mathcal{A}$ be a collection of subsets of $X$, and let $\mathcal{G}$ be a collection of subsets of $Y$. Let $C \subset Y$.
\begin{enumerate}
    \item $f(\cup \mathcal{A}) = \cup \{f(A)|A\in\mathcal{A}\}$
    \item $f^{-1}(\cup \mathcal{B}) = \{f^{-1}(C)|C \in \mathcal{G} \}$
    \item $f^{-1}(\cup \mathcal{B}) = \{f^{-1}(C)|C \in \mathcal{G} \}$
    \item $f^{-1}(C^C) = (f^{-1}(C))^C$
\end{enumerate}
\end{lemma}

\begin{remark}
It is in general not true that $f(A\cap B) = f(A)\cap f(B)$, because maybe $A\cap B = \emptyset$. However, if $f$ is a one-to-one function if and only if 
$$f(A\cap B) = f(A)\cap f(B)$$
because if $(A\cap B) = \emptyset$, then $f(A)\cap f(B) = \emptyset$.
\end{remark}

\subsection{Parameter change of function}
A function $f: A \rightarrow B$ is a rule to associate an element $a \in A$ to an element $b \in B$. The exact expression of $f$ depends on how we parameterize the set $A$. For example, consider $A=[0,1]$, and we want to map every element $x\in A$ to $5x$, then we have $f(x)=5x$. However, if we want to express/reparameterize $A$ as $x=5t,t\in [0,0.2]$, then we introduce a new local coordinate system on $A$ as $\phi(x)=x/5$. The functon on the new local coordinate system is given as $f\circ \phi^{-1}(t) = 25t$ 



\section{Set equivalence and partition}
\begin{definition}[relation]\index{relation}
A relation on a set $A$ is any statement which is either true or false for each ordered pair $(x,y)$ of elements in $A$. Examples are $x=y$,$x<y$
\end{definition}


\begin{definition}[equivalence relation]\cite{johnsonbaugh2010foundations}\index{equivalent relation}
Let $A$ and $B$ be two sets and let $f$ be a mapping of $A$ into $B$. If there exist a 1-1 mapping of $A$ \emph{onto} $B$, we say that $A$ and $B$ can be put in 1-1 correspondence, or that $A$ and $B$ have the same \emph{cardinal number}, or briefly, that $A$ and $B$ are \textbf{equivalent}, and we write $A\sim B$. The relation has the following properties:
\begin{enumerate}
\item It is reflective: $A \sim A$.
\item It is symmetric: if $A \sim B$, then $B \sim A$.
\item it is transitive: if $A \sim B$ and $B \sim C$, then $A \sim C$.
\end{enumerate}
\end{definition}

\begin{definition}[partitions of a set]\index{partition}
Let $S$ be a set. A collection of (finitely or infinitely many) nonempty subsets $A_1,A_2,...\subseteq S$ is called a partition of $S$ if:
\begin{itemize}
    \item These sets $A_i$ are pairwise disjoint.
    \item The union of all subsets $A_1\cup A_2 ... = S$. 
\end{itemize}
\end{definition}


\begin{theorem}[partition a set by equivalence]
Elements in a set $X$ equivalent to each other form an equivalent class. All the equivalent classes of a set partition the set.  
\end{theorem}
\begin{proof}
We can show that any element cannot belong to two distinct equivalent classes using transitivity.	
\end{proof}
 

\section{Countability}
\begin{definition}\cite{johnsonbaugh2010foundations}
For any positive integer $n$, let $P_n$ be the set whose elements are the integers 1,2,...,n; let $P$ be the set consisting of all positive integers. For any set $A$, we say:
\begin{enumerate}
\item $A$ is \emph{finite} if $A \sim P_n$ for some $n$.
\item $A$ is \emph{infinite} if $A$ is not finite.
\item $A$ is \emph{countable} if $A \sim P$ (countable infinite)or $A$ is finite.
\item $A$ is \emph{uncountable} if $A$ is not countable
\end{enumerate}
\end{definition}


\begin{example}\hfill
\begin{itemize}
    \item The integers $\mathbf{Z}$ form a countable set. The 1-1 mapping is given as $f(k)+2k $ if $k >= 0 $ and $f(k) = 2(-k) + 1$ if $k < 0$.
    \item The real number is uncountable set. 
\end{itemize}
\end{example}



\begin{lemma}
\cite{johnsonbaugh2010foundations}Properties of countable sets:
\begin{itemize}
    \item Any subset of a countable set is countable
    \item If $A, B$ are countable sets, then $A\cup B$ is a countable set.
    \item The Cartesian product of two countable sets is countable.
\end{itemize}
\end{lemma}
\begin{proof}
(3)	Let $S,T$ be the two countable sets. If they are finite, then $S\times T$ will be finite. Consider $S,T$ have infinite elements, we can list $S\times T$ as a table and count them diagonally from a corner. This counting can count all the element in $S\times T$.
\end{proof}



\begin{corollary}
Let k>1.
Then the cartesian product of k countable sets is countable.
\end{corollary}


\section{Real numbers}

\subsection{rational numbers}
\begin{definition}[rational number, irrational number]\index{rational number}\index{irrational number}\cite[21]{johnsonbaugh2010foundations} The set of \textbf{rational number}, denoted $\Q$, is the set 
	$$\{\frac{p}{q}|p,q\in \Z, ~and~ q\neq 0 \}.$$

A real number which is not rational is said to be \textbf{irrational}.
\end{definition}


\begin{lemma}[rational number and irrational number ] If $r$ is a rational number, which can be represented by $p/q$ and $x$ is an irrational number, then
\begin{itemize}
	\item $r+x$ is irrational.
	\item $rx$ is irrational, provided that $r\neq 0$.
\end{itemize}	
\end{lemma}
\begin{proof}
(1) Suppose $r+x$ is rational, then it can be represented by $m/n$. Then $x = r+x - r = m/n - p/q$ will still be rational, which contradicts that $x$ is a rational number.
(2)Suppose $rx$ is rational, then it can be represented by $m/n$. Then $x = rx/r= (m/n) /(p/q)$ will still be rational, which contradicts that $x$ is a rational number.
\end{proof}

\subsection{Dense subset}\index{dense subset}\cite[15]{fitzpatrick2006advanced}
\begin{definition}[dense subset in $\R$]\label{ch:sets：def:denseSubsetInRealNumbers}
Let $S$ be a subset of $\R$.
\begin{itemize}
	\item We say $S$ is a \textbf{dense subset} in $\R$ provided that every interval $I=(a,b), a < b$, contains a member of $S$.
	\item (alternative) We say $S$ is a \textbf{dense subset} in $\R$ provided that for every number $r\in \R$ and any $\epsilon > 0,$ there exists a member  $s\in S$ such that $\abs{s-r}<\epsilon$.
\end{itemize}	
\end{definition}

\begin{remark}[equivalence of the two definitions]\hfill
(1) implies (2): for $r \in \R$, there exists a number $s\in S, s\in (r-\epsilon,r+\epsilon)$ such that $\abs{r-s} < \epsilon$.
(2) implies (1): for any interval $(a,b)$, we let $\epsilon = d = \frac{b-a}{2}$, then there exists $s\in S$ such that $$\abs{s - \frac{a+b}{2}} < \frac{b-a}{2} \implies s \in (a,b).$$ 	
\end{remark}

\begin{theorem}[rational and irrational numbers are dense]\label{ch:sets:th:rationalIrrationalNumbersDense}\cite[23]{johnsonbaugh2010foundations}
If $a$ and $b$ are real numbers with $a < b$, then there exists both a rational number and an irrational number between $a$ and $b$.	
\end{theorem}
\begin{proof}
Let $k$ be an integer smaller than $a$, and let $n$ be an integer such that $n > \sqrt{2}/(b-a).$ Then
$$0<1/n< \frac{2}{n} < b-a.$$
So the sequence $k+1/n, k+2/n + ...$ will have at least one term falling into $(a,b)$.
Similarly, the sequence $k+\sqrt{2}/n, k+2\sqrt{2}/n + ...$ will have at least one term falling into $(a,b)$.
\end{proof}

\subsection{Axiom of completeness} \index{completeness axiom}
\begin{definition}[bounded above, bounded below]\hfill
	\begin{itemize}
		\item A nonempty subset $X$ of $\R$ is said to be \textbf{bounded above} if there exists a $a\in \R$ such that $a\geq x,\forall x\in X$.
		\item  A nonempty subset $X$ of $\R$ is said to be \textbf{bounded below} if there exists a $b\in \R$ such that $b\leq x,\forall x\in X$.
	\end{itemize}	
\end{definition}

\begin{definition}[Least upper bound, supremum]\cite[5]{johnsonbaugh2010foundations}
Let $S$ be a nonempty subset of $\mathbb{R}$. If $S$ is bounded above, then a number $u$ is said to be the \textbf{supremum} of $S$ if:
\begin{itemize}
\item u is an upper bound of $S$.
\item if $v$ is also an upper bound of $S$, then  $v \geq u$.
\end{itemize}
\end{definition}

\begin{definition}[greatest lower bound, infimum]\cite[5]{johnsonbaugh2010foundations}
	Let $S$ be a nonempty subset of $\mathbb{R}$. If $S$ is bounded above, then a number $u$ is said to be the \textbf{infimum} of $S$ if:
	\begin{itemize}
		\item $u$ is an lower bound of $S$.
		\item if $v$ is also an lower bound of $S$, then if $v \leq u$.
	\end{itemize}
\end{definition}

\begin{definition}[maximum of a set]
A real number $a_0$ is a maximum of the set $A$ if $a_0\in A$ and $a\leq a_0,\forall a\in A$, but it has a supremum of 1.
\end{definition}

\begin{remark}
The open interval $(0,1)$ does not have a maximum.
\end{remark}




\begin{theorem}[axiom of completeness, existence of least upper bound]\label{ch:sets:th:ExistenceOfLeastUpperBoundForBoundedRealSet}
	\textbf{Axiom of completeness:} Every nonempty subset of $\R$ that is bounded above has a least upper bound.
\end{theorem}

\begin{theorem}[existence of greatest lower bound]\cite[15]{johnsonbaugh2010foundations}
Every nonempty subset of $\R$ that is bounded below has a greatest lower bound.
\end{theorem}
\begin{proof}
Let $X$ be a nonempty subset of $\R$ that is bounded below. Let $Y$ be the set of lower bounds. Then $Y$ is bounded above, and therefore exists a least upper bound, denoted by $b$. We want to show that $b$ is the greatest lower bound of $X$. First $b\leq x,\forall x\in X$(if there exists $c\in X$, $c < b$, then $c\in Y$ contradicts that $b \geq y,\forall y\in Y$.) Second, for any other lower bound $d$, we have $d\leq b$, since $d\in Y$.
\end{proof}




\begin{lemma}[uniqueness of least upper bound]\cite[15]{johnsonbaugh2010foundations}
Let $X$ be a subset of $\R$. If $a$ and $b$ are least upper bounds of $X$, then $a = b$.	
\end{lemma}
\begin{proof}
Let $a$ be the least upper bound. Since $b$ is also a upper bound, then from the definition of least upper bound, we have $a\leq b$; Similarly, we have $a\geq b$. 
Therefore, $a = b$.
\end{proof}

\begin{lemma}[least upper bound is tight(has no hole)] \label{ch:sets:th:leastupperboundisTight}\cite[17]{johnsonbaugh2010foundations}\cite[17]{abbott2001understanding}\hfill
\begin{itemize}
	\item Let $X$ be a set of real numbers with least upper bound $a$. Then for any positive $\epsilon > 0$, there exists $x\in X$ such that $a-\epsilon<x\leq a.$
	\item Assume $s\in \R$ is an upper bound for a set $A\subseteq \R$. Then $s=\sup A$ if for any choice of $\epsilon > 0$, there exist an element $a\in A$ such that $s-a < \epsilon$.
\end{itemize}	
\end{lemma}
\begin{proof}
(1)	
$x\leq a$ is directly from the definition of least upper bound. Suppose there does not exist $x\in X$, such that $x > a-\epsilon$, then we conclude all $x\in X, x \leq a-\epsilon$. Let $b= a - \epsilon$ be another upper bound, then we have $b < a$, contradicting the fact that $a$ is the least upper bound.	
(2) Suppose $\sup A$ is the least upper bound, then we have $s\geq \sup A$. For contradiction purpose, assume $s > \sup A$. Let $\epsilon = 0.5*(s-\sup A)$, then $s-\epsilon > \sup A$, and there does not exist an element  $a\in A$ such that $s-\epsilon < a$, which is a contradiction.  Therefore, we must have $s = \sup A$.

\end{proof}

\begin{remark}
This lemma illustrates the idea of no-holes between an nonempty set and its least upper bound.
\end{remark}



\begin{lemma}[least upper bound of rational numbers]
Given a real number $a$, define $S = \{x|x\in \Q, x < a\}$. It follows that
$$\sup S = a.$$	
\end{lemma}
\begin{proof}
First any number greater than $a$ cannot be the least supper bound However, $a$ is smaller upper bound. Now suppose $\sup S < a$; however, there exists a rational number exists inside $(\sup S,a)$ since $\Q$ is dense subset(\autoref{ch:sets:th:rationalIrrationalNumbersDense}). therefore $\sup S = a$.
\end{proof}



\begin{theorem}[nested interval property]\index{nested interval property}
\cite[20]{abbott2001understanding}\label{ch:sets:NestedIntervalProperty}
For each $n\in \N$, assume we are given a closed interval $I_n=[a_n,b_n] = \{x\in \R: a_n \leq x \leq b_n\}$. And we also have $a_1 \leq a_2 \leq ....$ and $b_1 \geq b_2 \geq ...$, then,
$$I_1 \supseteq I_2 ...$$
has a nonempty intersection; that is $\cap^{\infty} I_n \neq \emptyset$.
\end{theorem}
\begin{proof}
 Let $A=\{a_1,a_2,...\}$, and let $x = \sup A$. Given any set $I_n=[a_n,b_n]$, we have $x\geq a_n$, because $x$ is the least upper bound. Also $x\leq b_n$, because $b_n$ is another upper bound for $A$. Since $n$ is arbitrary, we have $x\in \cap^{\infty} I_n$.	
\end{proof}

\section{Notes on bibliography}
The chapter mainly reference intermediate level real analysis textbooks\cite{johnsonbaugh2010foundations}\cite{abbott2001understanding}\cite{thomson2001elementary}.

\printbibliography

\end{refsection}
\begin{refsection}
	
\startcontents[chapters]
\chapter{Sequences \& Series}\label{ch:sequences-series}
%\minitoc

\printcontents[chapters]{}{1}{}
\index{sequence}\index{series}
\section{Sequence in $\R$}
\subsection{Basics}
\begin{definition}[sequence]
A \emph{sequence} in $\R$ a function $f$ maps from the set $P$ of all positive integers to $\R$. If $f(n)=x_n$, for $n\in P$, it is customary to denote the sequence $f$ by the symbol $\{x_n\}$. \cite{johnsonbaugh2010foundations}
\end{definition}


\begin{definition}[convergence of a sequence]
A sequence $\{p_n\}$ in $\R$ is said to \emph{converge} if there is a point $p \in \R$ with the following property. For every $\epsilon >0$ there is an integer $N$ such that $n > N$,$\abs{p_n - p} < \epsilon$.
\end{definition}


\begin{theorem}[uniqueness of limits]\cite{johnsonbaugh2010foundations}
A sequence in $\mathbb{R}$ can have at most one limit. 
\end{theorem}
\begin{proof}
	If a sequence has two different limits, then when $n$ is large enough, $a_n$ has to be increasingly closer to to both limit, which is contradiction. 
\end{proof}
 

\begin{theorem}[Boundedness of a convergent sequence]
Every convergent sequence in $\R$ is bounded.
\end{theorem}
\begin{proof}
	Because the sequence is convergent to a number $a$, then when given $\epsilon = 1$, there is an $N$, such that $n\geq N, \abs{x_n} \leq \abs{x_n-a} + \abs{a} < 1 + abs{a}$. Then $\abs{x_n} \leq \max(\abs{x_1}...\abs{x_N}, 1 + abs{a} )$.
\end{proof}




\begin{lemma}[algebra of limits]\cite[40]{johnsonbaugh2010foundations}\label{ch:sequences-series:th:algebraOfLimits}
Let $\{a_n\}$ and $\{b_n\}$ be two real-valued sequences and $\lim_{n\rightarrow \infty} a_n = M$, $\lim_{n\rightarrow \infty} b_n = L$, we have: \cite{johnsonbaugh2010foundations}
\begin{itemize}
    \item linearity: $\lim_{n \rightarrow \infty} \alpha a_n + \beta b_n = \alpha M + \beta L$
    \item product rule: $\lim_{n \rightarrow \infty} a_n b_n = ML$ 
    \item quotient rule: if $L\neq 0$,$ \lim_{n \rightarrow \infty} 1/a_n = 1/L $
    \item If $\{c_n\}$ is a bounded sequence, and $\lim_{n\to \infty} b_n = 0$, then
    $$\lim_{n\to \infty} c_n b_n = 0.$$
    \item (absolute value rule) $\lim_{n\to\infty} \abs{a_n} = \abs{M}.$
\end{itemize}
\end{lemma}
\begin{proof}
(1)linearity from triangle inequality;(2) $\abs{a_nb_n-LM} = \abs{a_nb_n +a_nM - a_nM-LM}$, then use triangle inequality and boundedness; (3) use boundedness. (4) Since $\{c_n\}$ is bounded. then there exists a number $S$ such that $\abs{c_n}\leq S, \forall n$. For any given $\epsilon > 0$, there exists a $N$ such that for all $n>N$, $\abs{b_n}\leq \epsilon/N$. Therefore, 
$$\abs{c_nb_n} \leq \abs{c_n}\abs{b_n} \leq S\epsilon/S = \epsilon, \forall n > N.$$
(5) Note that for any given $\epsilon > 0$, there exists a $N$ such that for all $n>N$, 
$$\abs{\abs{a_n}-\abs{M}} \leq \abs{a_n - M}\leq  S\epsilon/S = \epsilon.$$
\end{proof}

\begin{note}[the reverse of absolute value rule is not true]
Note that if $\{\abs{a_n}\}$ converges, $\{a_n\}$ not necessarily converges. For example $\{(-1)^n\}$.	
\end{note}


\begin{lemma}[sequence limit inequality]\cite[47]{johnsonbaugh2010foundations}\label{ch:sequences-series:th:SequenceLimitInequality} Let $\{a_n\}$ be a convergent sequence with limit $L$. It follows that
\begin{itemize}
	\item If $a_n \geq M$ for all $n\geq 0$, then $L\geq M$.
	\item If $a_n \geq b_n$ for all $n\geq 0$, and $\lim_{n\to \infty} b_n = K$then $L\geq K$.
\end{itemize}	
\end{lemma}
\begin{proof}
(1)	For contradiction purpose, we assume $L < M$. Denote $d = M - L, d > 0$. Since $a_n$ converges to $L$, then for $\epsilon = d/2$, there exist a $N$ such that
$$\abs{a_N - L} < \epsilon = d/2,$$
which implies $$a_N < d/2 + L = \frac{M-L}{2} + L = \frac{M+L}{2} < M.$$
This contradict the condition that $a_n \geq M$.
(2) Note that $(a_n - b_n) \geq 0$. Using (1) we have
$$\lim_{n \to \infty} (a_n - b_n) = L - K \geq 0,$$
where we have used the algebraic properties of limits(\autoref{ch:sequences-series:th:algebraOfLimits}).
\end{proof}



\begin{lemma}[squeeze theorem]\cite[47]{johnsonbaugh2010foundations}\label{ch:sequences-series:th:squeezeTheorem}
Let $\{a_n\},\{b_n\}$ and $\{c_n\}$ be sequences such that 
$$a_n\leq b_n\leq c_n, \forall n\in\cN.$$
	
If
$$\lim_{n\to\infty } a_n = L = \lim_{n\to \infty} c_n,$$
then
$$\lim_{n\to \infty} b_n = L.$$	
\end{lemma}
\begin{proof}
Note that for any $\epsilon > 0$ there exists $N_1$ such that for all $n > N_1$ such that
$$L-\epsilon < a_n < L + \epsilon.$$
Similarly, there exists $N_2$ such that for all $n > N_1$ such that
$$L-\epsilon < c_n < L + \epsilon.$$
	
Then for $n > \max(N_1,N_2)$, we have
	$$L-\epsilon < a_n \leq b_n \leq c_n < L + \epsilon.$$

That is, $b_n \to L$ as $n\to \infty.$	
\end{proof}



\begin{corollary}[applications of squeeze theorem]\hfill
\begin{itemize}
	\item If $\lim_{n\to \infty} \abs{a_n} = 0$, then $\lim_{n\to \infty} a_n = 0.$ 
	\item For any number $c$, 
	$$\lim_{n\to \infty} \frac{c^n}{n!} = 0.$$	
	\item $$\lim_{n\to\infty} \frac{n!}{n^n} = 0.$$
\end{itemize}
\end{corollary}
\begin{proof}
(1) Note that 
$$-\abs{a_n} \leq a_n \leq \abs{a_n},$$
and
$$\lim_{n\to \infty}-\abs{a_n} =\lim_{n\to \infty} \abs{a_n},$$
then use squeeze theorem to prove.	
(2)Choose $k$ to be an integer such that $k\geq 2\abs{c}$. Then if $n\geq K$
$$0\leq \abs{\frac{c^n}{n!}} \leq \abs{c}^k (\frac{1}{2})^{n-k} \to 0, as n\to \infty.$$
(3)
$$\frac{n!}{n^n}= \frac{1}{n}\frac{2}{n}\cdots \frac{n-1}{n}\frac{n}{n} \leq \frac{1}{n}\cdot 1 \cdot 1 \cdots 1 = 1/n \to 0, as~n\to \infty. $$
\end{proof}

\subsection{Cauchy criterion}
\begin{definition}[Cauchy sequence in $\R$]\index{Cauchy sequence}\cite[59]{johnsonbaugh2010foundations}
A sequence $\{a_n\}$ in $\R$ is called a Cauchy sequence, if for every $\epsilon > 0$, there exist an $N$ such that for all $n,m\geq N$, we have
$$\abs{x_n-x_m} < \epsilon.$$
\end{definition}

\begin{remark}[interpretation]
	Informally, a sequence $\{x_k\}$ satisfies the Cauchy criterion if, by choosing $k$ large enough, the distance between any two element $x_m$ and $x_l$ in the 'tail' of the sequence can be made as small as desired. A sequence satisfying Cauchy criterion is called a \textbf{Cauchy sequence}. 	
\end{remark}

\begin{lemma}[boundedness of Cauchy sequence]
	Every Cauchy sequence in $\R$ is bounded.
\end{lemma}
\begin{proof}
	Let $\epsilon = 1$, then there exists an $N$, such that for all $n,m \geq N$, we have
	$$\abs{x_n} \leq \abs{x_n - X_N} + \abs{X_N} < 1 + \abs{X_N} $$
Then the Cauchy sequence is bounded by the maximum of $1 + \abs{x_N}, \abs{x_1},...,\abs{x_{N-1}}$.
\end{proof}


\begin{theorem}[Cauchy sequence is a convergent sequence, vice versa]\index{Cauchy sequence}\label{ch:sequences-series:th:CauchySequenceIsConvergent}
	\cite[66]{abbott2001understanding}
	A sequence $\{x_k\}$ in $\mathbb{R}$ is Cauchy sequence if and only if it is a convergent sequence. 
\end{theorem}
\begin{proof}
	(1) the converse part that a convergent sequence is a Cauchy sequence can be proved triangle inequality; (2) the forward part: 
	Because Cauchy sequence is bounded, then there will be a subsequence $x_{n_i}$ converge to a limit $a$(\autoref{ch:sequences-series:th:Bolzano-Weierstrasstheorem}). 
	Let $\epsilon >0$, then there exist an $N$, such that for all $n,m,n_K > N$,$\abs{x_{n_K}-a} < \epsilon$.
	We have
	
	$$\abs{x_n - a} \leq \abs{x_n-x_{n_K}} + \abs{x_{n_K}-a} < 2\epsilon$$	
\end{proof}


\begin{remark}[Cauchy sequence might not be convergent in more general space]
\textbf{In incomplete normed space, a Cauchy sequence will not converge.} Since $\R$ is complete, therefore the Cauchy sequence will converge.
\end{remark}



\subsection{Sequence characterization of dense subset}\index{dense set}

\begin{theorem}\cite[36]{fitzpatrick2006advanced}
Let $S$ be a subset in $\R$. It follows that 
\begin{itemize}
	\item If for every number $x \in \R$ there exists a sequence $\{s_n\}, s_n\in S$ converging to $x$, then $S$ is a dense subset. 
	\item If $S$ is a dense subset, then  for every number $x \in \R$ there exists a sequence $\{s_n\}, s_n\in S$ converging to $x$.
\end{itemize}	
\end{theorem}
\begin{proof}
(1) If for every $x$ there exists a sequence converging to it, then that means for every interval $(a,b)$, there exists at least a number getting arbitrary closer to $(a+b)/2$, that is inside the interval $(a,b)$.
(2)  Based on the definition of dense subset (\autoref{ch:sets：def:denseSubsetInRealNumbers}), we can construct a sequence $s_n$ such that $s_n$ lying inside the interval $(x-1/2n,x+1/2n)$. Such sequence $\{s_n\}$ will converge to $x$.
\end{proof}

\begin{corollary}[sequential density of rations]\label{ch:sequences-series:th:SequentialDensityOfRationals}
For every $x\in \R$	there exists a sequence $\{s_n\}, s_n\in \Q$ converging to $x$.	
\end{corollary}
\begin{proof}
Use the fact that $\Q$ is a dense subset in $\R$(\autoref{ch:sets：def:denseSubsetInRealNumbers}).
\end{proof}

\begin{example}
Consider the irrational number $\sqrt{2}$, the above theorem ensures that there exists a sequence of rational numbers converging to $\sqrt{2}$, even though $\sqrt{2}$ is irrational.	
\end{example}



\section{Monotone sequence}
\subsection{Fundamentals}
\begin{definition}[monotone sequence] \index{monotone sequence}
A sequence if monotone is it is either increasing(i.e., $a_{n+1}\geq a_n$) or decreasing.
\end{definition}


\begin{theorem}[convergence of monotone sequence]\label{ch:sequences-series:th:monotoneSequenceConvergence}\cite[48]{johnsonbaugh2010foundations}
A monotone increasing sequence $\{a_n\}$ is convergent if and only if $\{a_n\}$ is bounded. 

If it is convergent, then its limit is $\sup_n a_n.$
\end{theorem}
\begin{proof}
(1)Suppose $\{a_n\}$ is bounded, then there exists a least upper bound(\autoref{ch:sets:th:ExistenceOfLeastUpperBoundForBoundedRealSet}). Let $A = \sup \{a_n\}$. Then given $\epsilon > 0$, there exists $a_k$ such that $A-\epsilon < a_k < A$(\autoref{ch:sets:th:leastupperboundisTight}); take $N=k$, then we prove $A$ is the limit.(2) The converse is the direct result of boundedness of any convergent sequence. 	
\end{proof}

\begin{note}[equivalence of $\sup$ and sequential limit for monotone sequence]\label{ch:sequences-series:Remark：EquivalenceSupAndSequentialLimitMonotoneSequence}
For a convergent monotone increasing sequence $\{a_n\}$, we have
$$\sup_n a_n = \lim_{n\to \infty} a_n.$$

Usually directly taking $\sup$ can be difficult and we can seek how to find a monotone increasing sequence.
\end{note}

\begin{remark}[application in $\limsup$]
	Let $\{a_n\}$ be a bounded real sequence(not necessarily convergent) and define
	$$\limsup_{n\to\infty} a_n = \lim_{n\to\infty}(\sup_{k\geq n} a_k).$$
	Then the sequence $\{\sup_{k\geq n} a_k\}_n^\infty$ is a monotone increasing sequence therefore $\limsup_{n\to\infty} a_n$ will have a limit.	
	
\end{remark}
 
\subsection{Applications}

\begin{lemma}[limit of exponent]\label{ch:sequences-series:th:limitOfExponent}\hfill
\begin{itemize}
	\item If $a > 1$, then $\lim_{n\to \infty} a^{1/n} = 1$.
	\item If $0<a<1$, then $\lim_{n\to \infty} a^{1/n} = 1$.
\end{itemize}	
That is, for all $a>0$, we have $\lim_{n\to \infty} a^{1/n} = 1$.
	
\end{lemma}
\begin{proof}
(1) When $a > 1$, $a^{1/n}$ is a decreasing sequence and bounded below by 1. Therefore $\{a^{1/n}\}$ is a convergent sequence. Let the limit be $L$. Then
$$\lim_{n\to \infty} a^{1/n} = L, \lim_{n\to \infty} a^{1/n}a^{1/n} = \lim_{n\to \infty} (a^2)^{1/n}= L^2.$$
Also note that $a^2>1$, then $\lim_{n\to \infty} (a^2)^{1/n}= L.$

So we have $L^2 = L \implies L=1.$

(2) When $0<a<1$, let $b = 1/a$, then 
$$\lim_{n\to \infty} a^{1/n} = \lim_{n\to \infty} (\frac{1}{b})^{1/n} = \lim_{n\to \infty} (\frac{1}{b^{1/n}})= L.$$
where we use the quotient rule(\autoref{ch:sequences-series:th:algebraOfLimits}). 
\end{proof}


\begin{lemma}[the limit to e]\label{ch:sequences-series:th:thelimitToE}\cite[53]{johnsonbaugh2010foundations}\hfill


\begin{itemize}
	\item The sequence $$\{a_n = (1 +\frac{1}{n})^{1/n}\}$$
	is increasing and convergent.\href{https://math.stackexchange.com/questions/167843/i-have-to-show-1-frac1nn-is-monotonically-increasing-sequence/704506#704506}{link}. And we denote 
	$$e\triangleq \lim_{n\to \infty } (1 +\frac{1}{n})^{1/n}.$$
	\item 
	$$\lim_{n\to \infty} (1 - \frac{1}{n})^n = \frac{1}{e}$$
	\item 
	$$\lim_{n\to \infty} (1 + \frac{1}{n^2})^{n^2} = e,\lim_{n\to \infty} (1 + \frac{1}{n^2})^n = 1, \lim_{n\to \infty} (1 + \frac{1}{n})^{n^2} = \infty $$
	
	\item Let $c\in \R$, then
	$$\lim_{n\to \infty} (1 + \frac{c}{n})^n = e^c$$
	\item Let $c\in \R$ and $\lim_{n\to\infty} q_n = 0$, then
	$$\lim_{n\to \infty} (1 + \frac{c}{n} + \frac{q_n}{n})^n = e^c$$
	
\end{itemize}


\end{lemma}
\begin{proof}
(1)  
Let $x_1 = 1, x_2=x_3=\cdots = x_{n+1} = 1 + \frac{1}{n}.$

Then
$$(x_1x_2\cdots x_{n+1})^{1/n+1} < \frac{x_1+x_2+...+x_{n+1}}{n+1},$$
where we have used the geometric average inequality(can be showed using convexity).
That is
$$(1+\frac{1}{n})^{n} < (\frac{1 + n(1+\frac{1}{n})}{n+1})^{n+1} = (1 + \frac{1}{n+1})^{n+1}.$$

To show it is bounded. We let $x_1 = 1, x_2=x_3=\cdots = x_{n+1} = 1 - \frac{1}{n}$ and use the geometric average inequality to show
$$b_n \geq b_{n+1},$$
where $b_n = (1 - \frac{1}{n})^{-n}$. 

Further, we can show 
$$b_n = a_{n-1}(1 + \frac{1}{n-1}) \geq a_{n-1},$$
where $a_n = (1 + \frac{1}{n})^n.$

Therefore, we have
$$a_1 \leq a_2  \leq a_3 \leq \cdots \leq b_3 \leq b_2 \leq b_1.$$


(2)	$$ (1 - \frac{1}{n})^n = \frac{1}{(\frac{n}{n-1})^n} = \frac{1}{(1 + \frac{1}{n-1})^n},$$
and note that $\lim_{n\to \infty}(1 + \frac{1}{n-1})^n = e$

(3) (a) this is a subsequence of (1). (b) note that 
$$\lim_{n\to \infty} (1 + \frac{1}{n^2})^n=\lim_{n\to \infty} \sqrt{1/n}{(1 + \frac{1}{n^2})^{n^2}} = 1$$
where we use the factor for $a > 0, \lim_{n\to \infty} a^{1/n} = 1$ in \autoref{ch:sequences-series:th:limitOfExponent},since $(1 + \frac{1}{n^2})^{n^2}$ will be bounded. 
	
(4)
Let $f(x) = x^a$. Then $f$ is a continuous function. Therefore
$$\lim_{n\to \infty} f(x_n) = f(\lim_{n\to \infty} x_n), $$
where $x_n = (1 + \frac{c}{n})^{n/c}.$
\end{proof}

\begin{lemma}
The sequence $\{n^{1/n}\},n\geq 3$ is decreasing and $\lim_{n\to \infty} n^{1/n} = 1.$
\end{lemma}
\begin{proof}
Consider $f(x) = x^{1/x}$ and $g(x) = \ln f(x) = \frac{\ln x}{x}$. $f(x)$ and $g(x)$ are decreasing for $x\geq e$. Also, $\lim_{x\to \infty} g(x) = 0 \implies  \lim_{x\to \infty} f(x) = 1$.
\end{proof}


\section{Subsequence and limits}



\subsection{Subsequence} \index{subsequence}
\begin{definition}[subsequence]\cite[39]{johnsonbaugh2010foundations}
Let $\{a_n\}$ be a sequence. Let $f$ be a strictly increasing function for the positive integer set $\PP$ to $\PP$. The sequence $\{a_{f(n)}\}$ is called a \emph{subsequence} of $\{a_n\}$. 
\end{definition}

\begin{remark}\hfill
\begin{itemize}
    \item Note that subsequence also have infinite number of terms.
    \item Usually, we denote $n_k = f(k)$ as the $k$th term in the subsequence $\{a_{n_k}\}$, where $n_1 < n_2 < n_3...$, and $n_k \geq k$.
\end{itemize}
\end{remark}
 

\begin{theorem}[subsequences of a convergent sequence have the same limit]\cite[39]{johnsonbaugh2010foundations}
If $\{a_n\}$ has a limit $L$, then every subsequence of $\{a_n\}$ has a limit $L$; If every subsequence of $\{a_n\}$ has a limit $L$ and $\{a_n\}$ has a limit $L$. 
\end{theorem}
\begin{proof}
(1) For any $\epsilon >0$, there exists a $N$ such that $\abs{a_k - L} < \epsilon,\forall k > N$.  We have the terms in subsequence with $n_k \geq k > N$ will have $\abs{a_{n_k} - L} < \epsilon$; (2) The converse is easy since the sequence itself is a subsequence.	
\end{proof}

\begin{example}\cite[39]{johnsonbaugh2010foundations}The sequence $\{1/2^n\}$ and $\{1/n!\}$ are subsequences of $\{1/n\}$ and $\lim_{n\to \infty }1/n = 0$. Therefore, 
	$$\lim_{n\to\infty} \frac{1}{2^n} = 0 =\lim_{n\to\infty} \frac{1}{n!}.$$
\end{example}



\subsection{Bolzano-Weierstrass theorem}
\begin{theorem}[Bolzano-Weierstrass theorem]\label{ch:sequences-series:th:Bolzano-Weierstrasstheorem}
\cite[62]{abbott2001understanding}
Every bounded sequence has at least one convergent subsequence.
\end{theorem}
\begin{proof}
 We use nested interval property(\autoref{ch:sets:NestedIntervalProperty}) to prove. Let $I_1$ be the interval bounding the sequence. Let $I_2$ be one-half of $I_1$ that bounding infinite terms(if both halves have infinite terms, pick either one; there must one half containing infinite terms since a sequence has infinite terms). As we continue to partition, $I_n$ becomes smaller. Note that the nested interval properties guarantee the existence a common element $a \in \cap^{\infty} I_n$ and the decreasing size of $I_n$ guarantee the subsequence are sufficiently close to $a$.	
\end{proof}


\subsection{Subsequence limits}

\begin{definition}[limit point]\cite[79]{strichartz2000way} \index{limit point}
	If $\{x_j\}$ is a sequence of real numbers and $x$ a real number, we say $x$ is a \textbf{limit point of the sequence} if for every $\epsilon > 0$, there are \textbf{infinite number of terms $x_j$} satisfying $\abs{x_j - x} < \epsilon$. 
\end{definition}

\begin{remark}[$+\infty$ as a limit point]
By convention, we say $+\infty$ is a limit point for $\{x_n\}$, if for any given $M > 0$, there exists infinitely many terms greater than $M$. If a sequence is unbounded above, then $+\infty$ is one limit point of the sequence.
	
\end{remark}

\begin{remark}[limit vs. limit point]\hfill
	\begin{itemize}
		\item For the definition of limit, we see a \textbf{stronger} statement "all terms beyond $N$"(therefore infinite many terms) should satisfy $\abs{x_i - x} < \epsilon$.
		\item For the definition of limit, we see a \textbf{weaker} statement "infinitely many terms" should satisfy $\abs{x_i - x} < \epsilon$.
	\end{itemize}
\end{remark}

\begin{lemma}[relation between subsequence and limit point]\cite[80]{strichartz2000way}
Let $\{x_n\}$ be a real sequence. A real number(even including $-\infty$ and $\infty$) is a limit point of $\{x_n\}$ if and only if there exists a subsequence $\{x_{n_k}\}$ such that $$\lim_{n_k\to \infty} x_{n_k} = x.$$
\end{lemma}
\begin{proof}
(1)(forward) If there exists a sequence, then for any given $\epsilon > 0$, there exists a $K > 0$, such that for all $n_k > K$(therefore infinitely many terms), $\abs{x_{n_k} - x} < \epsilon$. Therefore  $x$ is a limit point. 
(2)	(backward) Because $x$ is a limit point, then given any given $\epsilon > 0$, there exists infinitely many terms, denoted by index set $\cK$, in $\{x_n\}$, such that $\abs{x_m - x} < \epsilon,\forall m\in \cK$. Order the index in $\cK$, then $\{x_n\}_K$ is a subsequence converging to $x$.
\end{proof}

\begin{definition}[$\liminf$ and $\limsup$ for bounded sequences]\cite{johnsonbaugh2010foundations}
Let $\{a_n\}$ be a bounded real sequence and let $\mathcal{L}_a$ denote the set of all different limits $L_a$ of all the convergent subsequences, i.e. 
$$\lim_{k\rightarrow \infty} a_{n_k} = L_a$$
then we define
$$\limsup_{n\rightarrow \infty} a_n = \sup \mathcal{L}_a$$
$$\liminf_{n\rightarrow \infty}  a_n = \inf \mathcal{L}_a$$
\end{definition}



\begin{definition}[$\liminf$ and $\limsup$ for bounded and unbounded sequences,alternative]\cite[619]{hoggintroduction}\cite[69]{johnsonbaugh2010foundations}
Let $\{a_n\}$ be a bounded real sequence and let
$$b_n = \sup \{a_n,a_{n+1},a_{n+2},...\}$$
and
$$c_n = \inf \{a_n,a_{n+1},a_{n+2},...\}$$
then we define
$$\limsup_{n\rightarrow \infty} a_n = \liminf_{n\to\infty}b_n,$$
$$\lim_{n\rightarrow \infty} a_n = \lim_{n\to\infty}c_n.$$

If $\{a_n\}$ is not bounded above, then we define $\limsup_{n\to \infty} a_n = \infty$;
If $\{a_n\}$ is not bounded above, then we define $\liminf_{n\to \infty} a_n = -\infty$.
\end{definition}


\begin{remark}[interpretation]\hfill
\begin{itemize}
    \item $\{b_n\}$ and $\{c_n\}$ are nonincreasing and nondecreasing sequences. And since $\{a_n\}$ is bounded, both are monotone and bounded and therefore have limits.
    \item $c_n\leq a_n \leq b_n$.
    \item $\liminf$ and $\limsup$ for a sequence (no matter bounded or not) always exist. 
\end{itemize}
\end{remark}

\begin{example}\hfill
\begin{itemize}
	\item Let $\{a_n = n\}$. Then $$\limsup_{n\to \infty} = \infty,\liminf_{n\to \infty} = \infty. $$
	
	\item Let $\{a_n\}$ be defined by
	$$a_n = \begin{cases}
	-n, n ~is~odd \\
	0, n ~is~even
	\end{cases}.$$
	Then, 
	$$\limsup_{n\to \infty} = 0,\liminf_{n\to \infty} = -\infty. $$
\end{itemize}
\end{example}



\begin{theorem}\cite{johnsonbaugh2010foundations}
Let $\{a_n\}$ be a bounded sequence, and let $L = \lim_{n\rightarrow \infty} \sup a_n$ and $M = \lim_{n\rightarrow \infty} \inf a_n$ 
\begin{enumerate}
    \item If $\epsilon > 0$, there exist infinitely many positive integers $n$ such that $L-\epsilon  < a_n$ and there exist a positive interger $N_1$ such that if $n > N_1$, then $a_n < L + \epsilon$.
    \item If $\epsilon > 0$, there exist infinitely many positive integers $n$ such that $M+\epsilon  > a_n$ and there exist a positive interger $N_1$ such that if $n > N_1$, then $a_n > M - \epsilon$.
\end{enumerate}
\end{theorem}
\begin{proof}
We only prove the first part. Suppose there exist only finite $n$ such that $a_n > L -\epsilon$, then there will not exist a convergent subsequence approaching $L$; Suppose there are infinitely many $n$ such that $a_n > L + \epsilon$, because of the Bolzano-Weierstrass theorem, there exist a convergent subsequence converge to a limit greater than $L+\epsilon$.	
\end{proof}


\begin{theorem}[limit from $\liminf$ and $\limsup$]
Let $\{a_n\}$ be a bounded sequence, then $\lim_{n\to \infty} a_n = L$ if and only if 
$$\limsup_{n\rightarrow \infty}  a_n = L = \liminf_{n\rightarrow \infty} a_n.$$
Or equivalently, if and only $\{a_n\}$ has only one limit point.
\end{theorem}
\begin{proof}
(1) let
$$b_n = \sup \{a_n,a_{n+1},a_{n+2},...\}$$
and
$$c_n = \inf \{a_n,a_{n+1},a_{n+2},...\}$$
and use $c_n\leq a_n \leq b_n$ to squeeze. (2) If $\{a_n\}$ converges and every subsequence will converge to the same limit.	
\end{proof}



\section{Infinite series}
\subsection{Fundamental results}
\begin{definition}[convergence of series]\cite{johnsonbaugh2010foundations}
We denote $\sum_{n=1}^\infty a_n$ as infinite series, $s_n=\sum_{i=1}^n a_i$ as the partial sum. We say the infinite series $\sum_{n=1}^\infty a_n$ converges to $L$ if $\sum_{n=1}^\infty a_n$ has sum $L$.
\end{definition}
\begin{remark}
Note that the convergence properties/limit properties of infinite series usually can be deduced from $s_n$.
\end{remark}

\begin{definition}[absolute convergence]
	Let $\sum_{n=1}^\infty a_n$ be an infinite series. If $\sum_{n=1}^\infty \abs{a_n}$, we say $\sum_{n=1}^\infty a_n$ converge absolutely. If $\sum_{n=1}^\infty a_n$ converges and $\sum_{n=1}^\infty \abs{a_n}$ diverges, we say $\sum_{n=1}^\infty a_n$ converge conditionally.  
\end{definition}

\begin{example}
	$\sum_{n=1}^\infty (-1)^n/n$ converge conditionally.	
\end{example}

\begin{theorem}[important results of convergence]\cite[76]{johnsonbaugh2010foundations}\label{ch:sequences-series:th:importantseriesconvergenceresult}
\begin{itemize}
	\item If the $\sum_{n=1}^\infty a_n$ converges, then $\lim_{n\rightarrow \infty}a_n = 0$.
	\item \textbf{(absolute convergence implies convergence)}If $\sum_{n=1}^\infty a_n$ converges absolutely, then $\sum_{n=1}^\infty a_n$ converge.
	\item If $\sum_{n=1}^\infty a_n$ converges absolutely, then $\sum_{n=1}^\infty a_n^2$ converge absolutely.
		\item If $\sum_{n=1}^\infty a_n$ converges absolutely, then $\sum_{n=1}^\infty a_na_{n+k}$ converge absolutely, where $k$ is a fixed integer.
	\item (linearity)If $\sum_{n=1}^\infty a_n$ and $\sum_{n=1}^\infty b_n$ are convergent with limits $A$ and $B$, then
	$$\sum_{n=1}^\infty k a_n + m b_n = kA+mB $$
\end{itemize}

\textbf{Note:} If $\sum_{n=1}^\infty a_n$ only converge conditionally, then $\sum_{n=1}^\infty a_n^2$ might diverge. For example $\sum_{n=1}^\infty (-1)^n/\sqrt{n}$ converge conditionally and $\sum_{n=1}^\infty 1/n$ diverges.
\end{theorem}
\begin{proof}
(1)$s_{n+1}-s_n =a_n$, using the algebraic properties of limits of $s_n$ to prove.	(2)Construct $\{p_n = max(a_n,0)\}$ and $\{q_n = max(-a_n,0)\}$. Then the partial sum $\{p_n\}$ and $\{q_n\}$ is bounded monotone sequence, thus converges; since $\{a_n = p_n - q_n\}$, then using algebraic properties of limits to prove. (3)We know that $\lim_{n\to \infty} a_n =0$ from (1), then there exist an $N$ such that  for all $n \geq N$, $\abs{a_n} < 1$, therefore $a_n^2 < \abs{a_n}$. From comparison test, we know that $\sum_{n=N}^\infty \abs{a_n}$ will converge, therefore $\sum_{n=1}^\infty a_n^2$ converges absolutely(\autoref{ch:sequences-series:th:ConvergenceComparisonTestForSeries}).(4) use the fact that 
$$\abs{a_na_{n+k}} \leq \frac{1}{2}(a_n^2 + a_{n-k}^2).$$
Then the series $\sum \abs{a_na_{n+k}} $ is bounded above since $\sum a_n^2$ and $\sum a_{n+k}^2$ are finite due to (3).
 (5)Directly from algebraic property of sequences.
\end{proof}


\begin{remark}
Some theorems (e.g. in Fourier series, time series analysis)simply state the assumptions of absolute convergence. Then from this theorem, we know that squared convergence is implied.
\end{remark}

\begin{remark}[product of two series]\cite[76]{johnsonbaugh2010foundations}\hfill
\begin{itemize}
	\item It is possible that the series $\sum_{n=1}^\infty a_n$ and $\sum_{n=1}^\infty b_n$ both converge but the series $\sum_{n=1}^\infty a_nb_n$ diverges. For example, $\sum_{n=1}^\infty (-1)^n/\sqrt{n}$ converge conditionally and $\sum_{n=1}^\infty 1/n$ diverges.
	\item Even if $\sum_{n=1}^\infty a_n$  converges to $M$ and $\sum_{n=1}^\infty b_n$  converges to $N$, and $\sum_{n=1}^\infty a_nb_n$ converges, the sum limit in general will not be $MN$.
\end{itemize}	
\end{remark}

\begin{theorem}[Cauchy criterion analog]
The series $\sum_{n=1}^\infty a_n$  is convergent if and only for any $\epsilon > 0$, an $N$ can be found such that 
$$\abs{a_{n+1} + a_{n+2} + ... + a_m} < \epsilon, \forall m > n >N$$
\end{theorem}

\subsection{Tests for convergence}
For a comprehensive treatment, see \cite{kaplan1973advanced}. 

\begin{theorem}[comparison test for convergence]\label{ch:sequences-series:th:ConvergenceComparisonTestForSeries}
If $\abs{a_n} \leq b_n, \forall n=1,2,...$, and $\sum_{n=1}^\infty b_n$ converges, then $\sum_{n=1}^\infty a_n$ is absolutely converge.  
\end{theorem}
\begin{proof}
The partial sum sequence for $\sum_{n=1}^\infty \abs{a_n}$ is bounded (by $\sum_{n=1}^\infty b_n$) and monotone sequence. Then we use monotone convergence theorem(\autoref{ch:sequences-series:th:monotoneSequenceConvergence}). 	
\end{proof}


\begin{example}
$\sum_{n=1}^\infty 1/(n2^n)$ is convergent because $\sum_{n=1}^\infty 1/2^n$ is convergent. 
\end{example}

\begin{theorem}[geometric series]
The geometric series
$\sum_{i=0}^\infty ar^i$ converges for $-1<r<1$, and diverges for $\abs{r}\geq 1$.
\end{theorem}



\begin{theorem}[ratio test]\cite[86]{johnsonbaugh2010foundations}\label{ch:sequences-series:th:ratioTestForSeriesCnvergence}
For a series $\sum_{n=1}^\infty a_n$, if $a_n \neq 0, \forall n$, and $$\lim_{n\to \infty} \abs{\frac{a_{n+1}}{a_n}} = L$$
then we have: if $L<1$, absolutely converges; if $L=1$, the test not applicable; if $L>1$, divergent.
\end{theorem}
\begin{proof}
 To prove absolute convergence, we prove the $\sum_{n=1}^\infty \abs{a_n}$ is bounded first and then use monotone convergence property. To prove boundedness, let $L<r<1, b_n = \abs{a_{n+1}/a_n}$, then there exists $N$ such that $0<b_n < r, n>N$, then rewrite:
 $$\abs{a_{N+1}}+\abs{a_{N+2}} + ... = \abs{a_{N+1}}(1 + b_{N+1} + ...) < \abs{a_{N+1}}\sum_{i=0} r^i.$$
 The right hand side is a convergent series. Based on comparison test(\autoref{ch:sequences-series:th:ConvergenceComparisonTestForSeries}), $\sum_{n=1}^\infty a_n$ will converge absolutely.
\end{proof}

\begin{example}
The series $\sum_{n=1}^\infty 1/n!$ converges since the ratio
$$\frac{1/(n+1)!}{1/n!} = \frac{1}{n+1}$$
converges to 0 as $n\to \infty$.	
\end{example}


\begin{theorem}[root test]\cite[85]{johnsonbaugh2010foundations}
For a series $\sum_{n=1}^\infty a_n$, let 
$$\lim_{n\to\infty} \sqrt[n]{\abs{a_n}} = R$$
Then we have: if $R < 1$, absolutely converges; if $R>1$, diverges; if $R=1$, the test not applicable.
\end{theorem}
\begin{proof}
Suppose $L<1$. Choose a number $L<M<1$. Then there exists a $N$ such that for $n \geq N$ then
$$\abs{a_n}^{1/n} < M \Leftrightarrow \abs{a_n} < M^n.$$

Since the geometric series $\sum_{n=1}^\infty M^n$ converges, the series $\sum_{n=1}^\infty a_n$ converges absolutely based on comparison test(\autoref{ch:sequences-series:th:ConvergenceComparisonTestForSeries}).	
\end{proof}

\begin{lemma}[common applications of convergence test]\label{ch:sequences-series:th:commonSeriesConvergence}\hfill
\begin{itemize}
	\item The series $\sum_{n=1}^\infty n!/n^n$ converges.
	\item The series $\sum_{n=1}^\infty M^n/n!$ converges for all $M > 0$.
\end{itemize}	
\end{lemma}
\begin{proof}
(1) note that the ratio
$$\frac{(n+1)!/(n+1)^{n+1}}{n!/n^n} = \frac{n+1}{(n+1)^{n+1}}n^n = \frac{1}{(1 + 1/n)^n}$$
converges to $1/e < 1$. Then we use ratio test(\autoref{ch:sequences-series:th:ratioTestForSeriesCnvergence}).
(2) note that the ratio
$$\frac{M^{n+1}/(n+1)!}{M^n/n!} = \frac{M}{n+1}$$ converges to zero.
Then we use ratio test(\autoref{ch:sequences-series:th:ratioTestForSeriesCnvergence}).
\end{proof}


\subsection{Inequalities and $l_2$ series}
\subsubsection{Holder's and Minkowski's inequality}
\begin{lemma}[Holder's inequality and Minkowski's inequality for finite real-valued terms]
Consider any real-valued sequence $\{x_n\}$ and $\{y_n\}$. Then
\begin{itemize}
	\item (Holder's inequality) For any number $p,q\in (1,\infty)$ with $1/p + 1/q = 1$, we have
	$$\sum_{k=1}^n\abs{x_ky_k} \leq (\sum_{k=1}^n\abs{x_k}^p)^{1/p}(\sum_{k=1}^n\abs{y_k}^q)^{1/1}  $$
	\item (Minkowski's inequality) For any number $p\in [1,\infty)$
	$$(\sum_{k=1}^n\abs{x_k + y_k}^p)^{1/p} \leq (\sum_{k=1}^n\abs{x_k}^p)^{1/p}+(\sum_{k=1}^n\abs{y_k}^p)^{1/p}$$
\end{itemize}	
\end{lemma}


\begin{theorem}[Holder's inequality and Minkowski's inequality for series]
	Consider any real-valued sequence $\{x_n\}$ and $\{y_n\}$. Then
	\begin{itemize}
		\item (Holder's inequality) For any number $p,q\in (1,\infty)$ with $1/p + 1/q = 1$, we have
		$$\sum_{k=1}^\infty\abs{x_ky_k} \leq (\sum_{k=1}^\infty\abs{x_k}^p)^{1/p}(\sum_{k=1}^\infty\abs{y_k}^q)^{1/1}  $$
		\item (Minkowski's inequality) For any number $p\in [1,\infty)$
		$$(\sum_{k=1}^\infty\abs{x_k + y_k}^p)^{1/p} \leq (\sum_{k=1}^\infty\abs{x_k}^p)^{1/p}+(\sum_{k=1}^\infty\abs{y_k}^p)^{1/p}$$
	\end{itemize}	
\end{theorem}

\begin{remark}[application of Minkowski's inequality]
The Minkowski's inequality is mainly used to prove the triangle equality for the norm, that is, 
$$\norm{x + y} \leq \norm{x} + \norm{y},$$
where we define $\norm{x} = (\sum_{k=1}^\infty\abs{y_k}^p)^{1/p}.$
	
	
\end{remark}


\subsubsection{Cauchy-Schwarz inequality}

\begin{lemma}[Cauchy-Schwarz inequality for finite real-valued terms, recap]\cite[120]{johnsonbaugh2010foundations} Let $a_1,a_2,...,a_n \in \R$ and $b_1,b_2,...,b_n \in \R$. Then 
	$$\abs{\sum_{k=1}^n a_kb_k} \leq \sqrt{(\sum_{k=1}^n a_k^2)\sum_{k=1}^n b_k^2)}$$
\end{lemma}
\begin{proof}
See \autoref{ch:functional-analysis:th:Cauchy-SchwarzInequalityFiniteRealTerms} and simply use Holder's inequality with $p = q = 2$.
\end{proof}


\begin{lemma}[absolute convergence of product series]\cite[123]{johnsonbaugh2010foundations}
Let $l^2$ denote the set of all real sequence $\{c_k\}$ such that $\sum_{k=0}^\infty c_k^2$ converges. If $\{a_k\}$, $\{b_k\} \in l^2$, then $$\sum_{k=1}^\infty a_kb_k$$
converges absolutely. 	
\end{lemma}
\begin{proof}
For every positive integer $n$, therefore
\begin{align*}
\sum_{k=1}^n \abs{a_kb_k} &\leq \sqrt{(\sum_{k=1}^n a_k^2)(\sum_{k=1}^n b_k^2)} \\
&\leq \sqrt{(\sum_{k=1}^\infty a_k^2)(\sum_{k=1}^\infty b_k^2)}.
\end{align*}
Note that the right side is bounded because $\sum_{k=1}^\infty a_k^2,\sum_{k=1}^\infty b_k^2$ converge. Finally, the monotone convergence theorem(\autoref{ch:sequences-series:th:monotoneSequenceConvergence}) ensures that $\sum_{k=1}^\infty \abs{a_kb_k}$. Or we simply use comparison test(\autoref{ch:sequences-series:th:ConvergenceComparisonTestForSeries}).
\end{proof}



\begin{theorem}[Cauchy-Schwarz inequality for $l_2$ series]\cite[123]{johnsonbaugh2010foundations}
Let $l^2$ denote the set of all real sequence $\{c_k\}$ such that $\sum_{k=0}^\infty c_k^2$ converges. If $\{a_k\}$, $\{b_k\} \in l^2$, then $$\sum_{k=1}^\infty a_kb_k$$
converges absolutely and 
	$$\abs{\sum_{k=1}^\infty a_kb_k} \leq \sqrt{(\sum_{k=1}^\infty a_k^2)\sum_{k=1}^\infty b_k^2)} = M$$
\end{theorem}
\begin{proof}
Let $B_n =\sum_{k=1}^n a_kb_k$. We note that $B_n$ converges due to $\sum_{k=1}^\infty a_kb_k$ converges absolutely(\autoref{ch:sequences-series:th:importantseriesconvergenceresult}). Using the absolute-value rule(\autoref{ch:sequences-series:th:algebraOfLimits}), we have that $\abs{B_n}$ converges. Let $$\lim_{n \rightarrow \infty} \abs{B_n} = L.$$
Because $\abs{B_n} \leq M$ for every $n$(Cauchy inequality \autoref{ch:functional-analysis:th:Cauchy-SchwarzInequalityFiniteRealTerms}), the limit-inequality rule(\autoref{ch:sequences-series:th:SequenceLimitInequality}) gives that $L\leq M$. 
\end{proof}


\section{Alternating series}\index{alternating series}

\begin{theorem}[alternating series test]\cite[25]{johnsonbaugh2010foundations} Let $\{a_n\}$ be a \textbf{decreasing} sequence such that $\lim_{n\to \infty } a_n = 0$.Then 
\begin{itemize}
	\item $$\sum_{n=1}^{\infty}(-1)^{n+1} a_n$$
	converges.
	\item Let $L$ be the limit and $s_n$ be the partial sum, then
	$$\abs{s_n - L} \leq a_{n+1}$$
\end{itemize}	
\end{theorem}
\begin{proof}
(1)Note that 
$$s_{2n} = (a_1-a_2) + (a_3-a_4) + ... + (a_{2n-1}-a_{2n})$$
is a monotone sequence in terms of $n$. Moreover, 
$$s_{2n} = a_1 - (a_2-a_3) - ... - (a_{2n-2}-a_{2n-1}) - a_{2n} \leq a_1.$$
Therefore, $s_{2n}$ is monotone and bounded; therefore converges(\autoref{ch:sequences-series:th:monotoneSequenceConvergence}) .
Similarly, $s_{2n-1}$ will converge since $s_{2n-1} = s_{2n}+a_{2n}$ and algebraic property of limits(\autoref{ch:sequences-series:th:algebraOfLimits}).
Since $s_{2n}$ and $s_{2n-1}$ converge, $s_n$ must converge.

(2) 
\begin{align*}
s_n - L &= s_n - \sum_{i=1}^\infty (-1)^{i+1} a_i \\
&=(-1)^{n+2}a_{n+1} + (-1)^{n+3}a_{n+2} + ...\\
&\begin{cases}
\leq a_{n+1}, ~if~n~is~even\\
\geq -a_{n+1}, ~if~n~is~odd
\end{cases}
\end{align*}
where we use analog result from(1) that $\lim_{n\to \infty} s_{2n} \leq a_1,\lim_{n\to \infty} s_{2n-1} = \lim_{n\to \infty} s_{2n} \leq a_1$ and $\lim_{n\to \infty} s_{n} \leq a_1$.
\end{proof}



\begin{remark}[the oscillation mode]
Note that $s_n$ will oscillate as a function of $n$. 
\end{remark}

\begin{remark}[application in approximation]
Given an alternating series, we can approximate the limit using partial sum $s_n$, which contains finite terms and the error is bounded by $a_{n+1}$. 
\end{remark}


\begin{corollary}\cite[25]{johnsonbaugh2010foundations}
If $s>0$, then 
$$\sum_{n=1}^{\infty}(-1)^{n+1} \frac{1}{n^s}$$
converges.
\end{corollary}

\section{Notes on bibliography}
The chapter mainly refers to intermediate level real analysis textbooks\cite{johnsonbaugh2010foundations}\cite{abbott2001understanding}\cite{thomson2001elementary}\cite{strichartz2000way}.

\printbibliography



\end{refsection}
\begin{refsection}

\startcontents[chapters]
\chapter{Metric Space \& Topological Space}\label{ch:metric-space}
%\minitoc

\printcontents[chapters]{}{1}{}

\section{Metric space}

\subsection{Definitions}


\begin{definition}[metric, distance function] \cite[117]{johnsonbaugh2010foundations}
Let $M$ be a set $X$. A \textbf{metric} on $M$ is a function $d$ from $M\times M$ into $[0,\infty)$ which satisfies 
\begin{enumerate}
\item $d(p,q) > 0 ~\text{if}~ p \neq q;$
\item  $d(p,q)=0$ if and only if $p=q$;
\item $d(p,q) = d(q,p);$
\item (triangle inequality)$d(p,q) \leq d(p,r) + d(r,q) ~\text{for any} p,r,q \in M$
\end{enumerate}
\end{definition}

\begin{definition}[metric space]\cite[117]{johnsonbaugh2010foundations}
A \textbf{metric space} is an ordered pair $(M,d)$, where $M$ is a set and $d$ is a metric on $M$.	
\end{definition}

\begin{definition}[metric space subspace]\label{metric space subspace}
Let $(M,d)$ be a metric space. Let $A$ be a subset of $M$. Let $d_{X\times X}$ be the metric $d$ restricted to the domain $X\times X$. Then $(A,d_{X\times X})$ is called the \textbf{subspace} of metric space $(M,d)$.
\end{definition}



\begin{lemma}[the metric space $\R^n$ with Euclidean metric]\label{ch:metric-space:th:RnMetricSpaceWithEnclideanMetric}
The set $\R^n$ with the metric $d(x,y)$ defined by
$$d(x,y) = \sqrt{\sum_{k=1}^n(x_k - y_k)^2},$$
where $x=(x_1,...,x_n), y = (y_1,...,y_n)$.
\end{lemma}
\begin{proof}
We can verify $d(x,y)$ satisfies
\begin{enumerate}
	\item $d(p,q) > 0 ~\text{if}~ p \neq q;$
	\item  $d(p,q)=0$ if and only if $p=q$;
	\item $d(p,q) = d(q,p);$
	\item (triangle inequality) we have
	\begin{align*}
d(x,z) = \sqrt{\sum_{i=1}^n (x_i - z_i)^2} &= \sqrt{\sum_{i=1}^n (x_i - y_i + y_i- z_i)^2} \\	
&= \sqrt{\sum_{i=1}^n (x_i - y_i)^2 +2(x_i-y_i)(y_i-z_i) + (y_i- z_i)^2} \\
&= \sqrt{\sum_{i=1}^n (x_i - y_i)^2 +\sum_{i=1}^n2(x_i-y_i)(y_i-z_i) + \sum_{i=1}^n(y_i- z_i)^2} \\
&\leq \sqrt{\sum_{i=1}^n (x_i - y_i)^2 +2\sqrt{\sum_{i=1}^n (x_i-y_i)^2\sum_{i=1}^n(y_i-z_i)^2} + \sum_{i=1}^n(y_i- z_i)^2} \\
& = \sqrt{(\sqrt{\sum_{i=1}^n (x_i - y_i)^2}+\sqrt{\sum_{i=1}^n (y_i - z_i)^2})^2} \\
& = (\sqrt{\sum_{i=1}^n (x_i - y_i)^2}+\sqrt{\sum_{i=1}^n (y_i - z_i)^2}) \\
& = d(x,y) + d(y,z)
\end{align*}
\end{enumerate}
where we Cauchy inequality for finite terms(\autoref{ch:functional-analysis:th:Cauchy-SchwarzInequalityFiniteRealTerms}) such that
$$\abs{\sum_{i=1}^n(x_i-y_i)(y_i-z_i)}\leq \sqrt{\sum_{i=1}^n (x_i-y_i)^2\sum_{i=1}^n(y_i-z_i)^2}$$.	
\end{proof}


\begin{lemma}[the metric space $l^2(\N)$ with Euclidean metric]
	Let $l^2(\N)$ denote the set of all real sequence $\{c_k\}$ such that $\sum_{k=0}^\infty c_k^2$ converges.
	The set $l^2(\N)$ with the metric $d(x,y)$ defined by
	$$d(x,y) = \sqrt{\sum_{k=1}^\infty(x_k - y_k)^2},$$
	where $x=(x_1,...,x_n,...), y = (y_1,...,y_n,...)$.
\end{lemma}
\begin{proof}	
	First, we want to show that $d(x,y)$ actually exists(i.e. converges):
	$$\sum_{k=1}^\infty(x_k - y_k)^2 = \sum_{k=1}^\infty x_k^2 + \sum_{k=1}^\infty y_k^2 -2 \sum_{k=1}^\infty x_ky_k$$
	where the right side three series converge; therefore the left side converges.
	
	We can verify $d(x,y)$ satisfies
	\begin{enumerate}
		\item $d(p,q) > 0 ~\text{if}~ p \neq q;$
		\item  $d(p,q)=0$ if and only if $p=q$;
		\item $d(p,q) = d(q,p);$
		\item (triangle inequality)  we have, similar to $\R^n$ case, for finite terms
		\begin{align*}
		\sqrt{\sum_{i=1}^n (x_i - z_i)^2} &= \sqrt{\sum_{i=1}^n (x_i - y_i + y_i- z_i)^2} \\	
		& \leq (\sqrt{\sum_{i=1}^n (x_i - y_i)^2}+\sqrt{\sum_{i=1}^n (y_i - z_i)^2}) \\
		& = d(x,y) + d(y,z)
		\end{align*}
	\end{enumerate}
	
	Then take limit on both side and use the sequence-limit-inequity rule(\autoref{ch:sequences-series:th:SequenceLimitInequality}), we have
	$$d(x,y)\leq d(x,z) + d(z,y).$$
\end{proof}

\subsection{Metric space vs. normed (vector) space vs. Banach space}
\begin{note}[metric space, normed space and Banach space]\hfill
\begin{itemize}
	\item All normed spaces are metric spaces by defining $d(x,y)=\norm{x-y}$. However, metric space is not necessarily normed space because the two properties:
	\begin{enumerate}
		\item Translation invariance: $d(u+w,v+w) = d(u,v)$
		\item Scaling properties: $d(tu,tv)=\abs{t}d(u,v)$
	\end{enumerate}
	not necessarily satisfied by the metric. 
	\item Also, normed space requires the underlying space is vector space, but metric space does not require so.\cite{kreyszig1989introductory}
	\item A Banach space is a normed vector space which is also a complete metric space.(Complete with respect to the metric defined by the norm)
\end{itemize}	
\end{note}




\subsection{Isometry}
\begin{definition}[isometry]
Let $(X,d_X),(Y,d_Y)$ be two metric spaces. An isometry is a distance-preserving bijection $\phi: X\rightarrow Y$ such that:
$$d_Y(\phi(x_1),\phi(x_2)) = d_X(x_1,x_2)$$	
\end{definition}


\section{Sequences in metric space}
\begin{definition}\cite[125]{johnsonbaugh2010foundations}
Let $\{a_n\}$ be a sequence in a metric space $(M,d)$. We say that $\{a_n\}$ converges to $L$, where $L \in M$, and write $$\lim_{n\rightarrow \infty} a_n = L$$
if for every $\epsilon > 0$, there exists $N>0$ such that if $n > N$, then $d(a_n,L) < \epsilon$.
\end{definition}
\begin{remark}
Note that when $M$ is equipped with different metric $d$, a same sequence might have different convergent properties. For example, consider $M=\R$,and $d_1$ is the Euclidean norm, $d_2(x,y)=1-\delta(x,y)$. Then a sequence convergent using $d_1$ will not be convergent using $d_2$.
\end{remark}


\begin{theorem}[componentwise convergence condition for sequence in $\R^n$]\label{ch:metric-space:th:componentWiseConvergenceConditionForConvergenceInRn}\cite[126]{johnsonbaugh2010foundations}
Let $\{a^{(k)}\}_{k=1}^\infty,a^{(k)}\in \R^n$ be a sequence in $\R^n$ such that for each $k$, $a^{(k)}=(a^{(k)}_1,a^{(k)}_2,...,a^{(k)}_n)$. Then $a^{(k)}$ converges to $a\in \R^n$ if and only if 
$$\lim_{k\to\infty} a_j^{(k)} = a_j, for~ j=1,2,...,n.$$ 	
\end{theorem}
\begin{proof}
(1) First suppose $a^{(k)}$ converges to $a$, which means for every $\epsilon > 0$, there exists a N such that $\forall k > N$, 
$$\norm{a^{(k)} - a}_2 < \epsilon.$$
Note that for each component $a^{(k)}_i$, we have
$$\abs{a^{(k)}_i - a_i}\leq \norm{a^{(k)} - a}_2 < \epsilon;$$
that is $\{a^{(k)}_i\},i=1,2,...,n$ converges to $a_i$.
(2) Second suppose $\{a^{(k)}_i\},i=1,2,...,n$ converges to $a_i$, which means for every $\epsilon > 0$, there exists a $N_i$ such that $\forall k > N_i$, 
$$\abs{a^{(k)} - a}_2 < \epsilon/\sqrt{n}.$$
Let $N = \max (N_1,N_2,...,N_n)$, we have
\begin{align*}
\norm{a^{(k)} - a}_2 &= \sqrt{\sum_{i=1}^n (a^{(k)}_j - a_j)^2} \\
&< \sqrt{\sum_{i=1}^n \epsilon^2/n} \\
& = \epsilon;
\end{align*}
that is $\{a^{(k)}\}$ converges to $a$.
\end{proof}




\section{Closed sets \& open sets in metric space}
\subsection{Closed set}
\begin{definition}[closure point, closure, closed subset]\index{closure}\index{closure point}
\cite[25]{luenberger1969optimization}
\begin{itemize}
	\item A point $x\in X$ is said to be \textbf{a closure point} of a set $P$ if given $\epsilon >0$, there is a point $p \in P$ satisfying $d(x,p)<\epsilon$. The collection of all closure points of $P$ called the \textbf{closure} of $P$, denoted as $\bar{P}$. It is clear that $P\subseteq \bar{P}$ 
	\item A set $P$ is \textbf{closed} if $P = \bar{P}$.
\end{itemize}
\end{definition}

\begin{definition}[limit point, closed set, alternative]
	\cite[128-129]{johnsonbaugh2010foundations}\hfill
\begin{itemize}
	\item Let $(M,d)$ be a metric space, and let $X$ be a subset of $M$. We say that a point $x \in M$ is a limit point of $X$ if there is a sequence $\{x_n\}$ that $x_n\in X,\forall n$ and $\lim_{n \rightarrow \infty} x_n = x$.
	\item Let $M$ be a metric space, and let $X$ be a subset of $M$. If \textbf{every} convergent sequence in $X$ and its limit point belongs to $X$, we say that $X$ is closed in $M$.
\end{itemize}	
\end{definition}
	
\begin{remark}[equivalence of the two definitions]
	These definitions about closed sets consistent; all says that points in a closed set can be getting to arbitrarily close. 
\end{remark}



\begin{lemma}[closure is the smallest containing closed set]
The closure of a set $X$ is the smallest closed set containing $X$. 
\end{lemma}
\begin{proof}
Let $L$ be the set of all limits of every convergent sequences in $X$. Then $\bar{X} = X\cup L$ is a closed set containing $X$. To show it is the smallest closed set, we need to show that for any other closed set $Y$ containing $X$, $\bar{X}\subset Y$. For any convergent sequence in $\bar{X}$, which is also in $Y$, the limit point is also in both $\bar{X}$ and $Y$, and therefore $\bar{X}\subset Y$.
\end{proof}



\subsection{Open sets}
\begin{definition}[open ball]\index{open ball}\cite[132]{johnsonbaugh2010foundations}
Let $(M,d)$ be a metric space. Let $\epsilon > 0$ and let $x\in M$. We let
$$B(\epsilon,x)=\{y\in M| d(x,y) < \epsilon\}$$
\end{definition}

\begin{definition}[interior point,interior, open subset]\cite[24]{luenberger1969optimization}\index{interior}\index{interior point}
\begin{itemize}
	\item Let $P$ be a subset of a metric space $(X,d)$. The point $p\in P$ is said to be \textbf{an interior point} of $P$ if there is an $\delta > 0$ such that $B(x,\delta)$ is a subset of $P$. The collections of all interior points is called \textbf{interior} of $X$, denoted as $int(P)$.  It is clear that $int(P) \subseteq P$.
	\item A set $P$ is said to be open if $int(P)=P$.
\end{itemize}	
\end{definition}


\begin{lemma}[interior as the largest open set contained inside, characterization]
The interior of $X$ is the largest open set contained inside $X$.	
\end{lemma}
\begin{proof}

\end{proof}


\subsection{Further characterization and properties}



\begin{lemma}[empty set, entire set, and singleton set are closed sets]\label{ch:metric-space:th:EmptysetEntireSetSingletonSetAreClosedSet}\cite[130]{johnsonbaugh2010foundations}
	The empty set $\emptyset$ and the entire space are closed sets; A singleton subset is a closed set. 
\end{lemma}
\begin{proof}
	The closure of an empty set is still empty. From $P\subseteq \bar{P}$ we have the entire space must be closed. For a singleton subset, any element outside this subset, we can find a $\epsilon$ such that there is no point in the singleton set getting close.	
\end{proof}

\begin{theorem}[empty set and entire space are open]The empty set and the entire space $X$ are open sets.
\end{theorem}
\begin{proof}
	The interior points of an empty set is empty set. The entire set $X$: for every point in $X$ that is near $x_0 \in X$ will be in $X$, therefore every point in $X$ is interior point.	
\end{proof}

\begin{theorem}
	Let $M$ be a metric space and let $x\in M$. Then $M,\emptyset$ and $\{x\}$ (a singleton set) are closed subsets of $M$.
\end{theorem}
\begin{remark}\hfill
	\begin{itemize}
		\item The reason that a singleton set is closed because there exist a constant sequence converge to $x$.
		\item There maybe confusion on why $M$ is closed. Note that a limit point is defined for points in $M$. If $M=(0,1)$, then limits points does not include 0 and 1, therefore $M$ is closed. However, if $M=\R$,$(0,1)$ is not closed in $M$.
		\item Based on the theorem of complement of open set is closed. We know that $M$ is both open and closed. $\R$ is both open and closed. $\emptyset$ is both open and closed. 
	\end{itemize}
\end{remark}

\begin{theorem}[the complementary characterization of open and closed sets]\cite[285]{fitzpatrick2006advanced}\label{ch:metric-space:th:complementary characterizationOpenClosedSetsInMetricSpace}
	Let $A$ be a subset in metric space $(M,d)$. 
	\begin{itemize}
		\item If $A$ is a closed subset, then its complement is an open subset.
		\item If $A$ is an open subset, then its complement is closed set.
	\end{itemize}	 
\end{theorem}
\begin{proof}
	(1)	
	Let $A$ be a closed set.
	(2)
	Let $A$ be an open set. 	
\end{proof}


\begin{remark}[intuition on closed set and open set]
	Closed set means every point in it can be getting arbitrarily close to, which is useful in taking limits. Open set means every point in it can be surrounded by an open ball belong to the set.
\end{remark}


\begin{theorem}
	\cite[134]{johnsonbaugh2010foundations}Let $M$ be a metric space and let $X\subset M$. Then $X$ is open if and only if $X^C$ is closed. 
\end{theorem}
\begin{proof}
	see the closed set subsection.
\end{proof}

\begin{theorem}\cite[130]{johnsonbaugh2010foundations}
	Any finite union of closed sets is closed; Any arbitrary intersection of closed set is closed.
\end{theorem}
\begin{proof}
	(1) use induction from above lemma; (2) let $\cG$ be the collection of closed sets, then let $M = \cap \cG$. If $M=\emptyset$, then $M$ is a closed set. If $M\neq \emptyset$, then let $x\in M$, and $x$ is a limit point(since every element of a closed set is a limit point), then there exist a sequence in $M$ that converges to $x$
\end{proof}

\begin{remark}[Why finite union]
	consider the collection of sets like $[0,1-1/n]$, where the union is $[0,1)$
\end{remark}



\begin{theorem}\cite[134]{johnsonbaugh2010foundations}
	Let $M$ be a metric space, then any \textbf{finite} intersection of open subsets are open in $M$; And \textbf{arbitrary} union of open subsets is open in $M$.
\end{theorem}
\begin{proof}
	(1) Let $U_1,...U_n$ be open sets. Let $M = \cup_i U_i$. Consider $x\in M$, then there exist open balls in $U_1,...,U_n$ centered at $x$. Choose the smallest open ball $B$ and $B$ will be in $M$ and therefore $M$ is open. (2) Let  $M = \cap_i U_i^C = (\cup U_i)^C$, then $M$ will be closed since arbitrary intersection of closed set is closed, and then $\cup U_i$ will be open since its complement is open.
\end{proof}

\begin{remark}[Why finite intersection?]
	Consider the countable intersections of sets $(-1/n,1/n)$, which will produce the set $\{0 \}$, which is closed set.
\end{remark}


\begin{remark}
	Let $\mathcal{J}$ be the collection of subsets of $M$, such that
	\begin{enumerate}
		\item $M \in \mathcal{J}$ and $\emptyset \in \mathcal{M}$.
		\item if $O_1,O_2,...,O_n \in \mathcal{J}$, then $O_1\cap O_2 \cap ...O_n \in \mathcal{J}$(finite intersection).
		\item if for each $\alpha \in \mathcal{I}$, $O_a \in \mathcal{J}$, then $\cup_{\alpha \in \mathcal{I}} O_{\alpha} \in \mathcal{J}$(arbitrary union).
	\end{enumerate}
	$\mathcal{J}$ is called a topology on $M$.
\end{remark}


\subsection{Open and closed sets in $\R^n$}

\begin{note}[the metric space $\R^n$]
In this section, we discuss the metric space $(\R^n, d)$, where the validity of the metric is discussed in \autoref{ch:metric-space:th:RnMetricSpaceWithEnclideanMetric}.	
\end{note}




\begin{definition}[open ball in $\R^n$]\cite[282]{fitzpatrick2006advanced}
Let  $u\in \R^n$ be a point in metric space $(\R^n, d)$. Let $r$ be a positive number, we call the set
$$B_r(u) \triangleq \{v\in \R^n|d(u,v)<r\}$$
the \textbf{open ball} with radius $r$ about $u$.
\end{definition}

\begin{definition}[interior and open sets in $\R^n$]\cite[282]{fitzpatrick2006advanced}
Let $A$ be a subset of $\R^n$. 
\begin{itemize}
	\item The \textbf{interior} of $A$, denoted by $int~A$ is the set of points in $\R^n$ such that each element in $int A$, there is an open ball about it contained in $A$.
	\item $A$ is called an open subset if $int~A = A$.
\end{itemize}	
\end{definition}



\begin{definition}[closed sets in $\R^n$]\cite[284]{fitzpatrick2006advanced}
A subset $S \subset \mathbb{R}^n$ is closed provided that whenever $\{x_n\},x_n\in \R^n$ is a sequence of points in $S$ that converges to a point $x$ in $\R^n$, then $x$ belongs to $\R^n$. 
\end{definition}


\begin{lemma}[closed interval is closed subset in $\R$， open interval is not closed subset in $\R$]\cite[37]{fitzpatrick2006advanced}
\begin{itemize}
	\item Let $[c,d], c < d$ be a closed interval. Then it is closed.
	\item Let $(c,d),c < d$ be an open interval. Then it is not closed. 
\end{itemize}	
\end{lemma}
\begin{proof}
(1)	
Let $\{a_n\}$ be a convergent sequence such that every term lying in $[c,d]$. Based on the sequence-limit-inequality(\autoref{ch:sequences-series:th:SequenceLimitInequality}),  the limit must be in $[c,d]$. Therefore, $[c,d]$ is a closed set.
(2) We can construct a convergent sequence $\{a_n\}$ that $x_n \in (c,d)$ for all $k$, but it converges to $d$. Then we can see that its limit point $d$ does not belong to the set $(c,d)$.  
\end{proof}

\begin{note}[caution! closed set is a relative concept, specifying the metric space is critical]\hfill
\begin{itemize}
	\item In the space $\R$, the interval $(0,1)$ is open.
	\item However, if the space is $(c,d)$, then $(c,d)$ is closed set in $(c,d)$. This is because the entire set is always closed(\autoref{ch:metric-space:th:EmptysetEntireSetSingletonSetAreClosedSet}). 
\end{itemize}	
\end{note}

\begin{lemma}[rational number is not closed in $\R$ ]\cite[37]{fitzpatrick2006advanced}
The $\Q$ of rational number is not closed in $\R$.(However, it is not necessarily open)	
\end{lemma}
\begin{proof}
Because of the sequential density property of the rationals (\autoref{ch:sequences-series:th:SequentialDensityOfRationals}), there exists a rational number sequence converging to $\sqrt{2}$, which is not a rational number. Therefore, $\Q$ is not closed.	
\end{proof}

\begin{theorem}[the complementary characterization of open and closed sets]\cite[285]{fitzpatrick2006advanced}
A subset of $\R^n$ is open in $\R^n$ if and only if its complement in $\R^n$ is closed in $\R^n$.	
\end{theorem}
\begin{proof}
\autoref{ch:metric-space:th:complementary characterizationOpenClosedSetsInMetricSpace}.
\end{proof}

\begin{example}
For example, the interval $(0,1)$ is open is because we can pick 
Some examples are:
\begin{enumerate}
    \item some sets are neither open or closed, say $[0,1)$
    \item Half-interval $[1,+\infty)$ is closed.
    \item $\mathbb{R}^n$ and $\emptyset$ are both open and closed. 
\end{enumerate}
\end{example}






\section{Compact sets}
\subsection{Basic concepts}
\begin{definition}[open cover]\index{open cover}
\cite{spivak}\cite[98]{abbott2001understanding}\cite[144]{johnsonbaugh2010foundations}A collection $\mathcal{O}$ of opens sets is an \textbf{open cover} of $A$ if every point $x \in A$ is in some open set in the collection $\mathcal{O}$. A \textbf{finite subcover} of $A$ is a finite subcollection of open sets from the original open cover whose union still manages to completely contain $A$.
\end{definition}

\begin{definition}[compact via open set]\index{compact}
A set $A$ is called \textbf{compact} if \textbf{every open cover} $\mathbb{O}$ of $A$ contains a finite subcollection of open sets which also covers $\mathbb{A}$.
\end{definition}

\begin{remark}
The definition requires \textbf{every} open cover contains a finite subcover.  
\end{remark}

\begin{example}
\cite[98]{abbott2001understanding}The open interval $(0,1)$ has a open cover $\cup_{n=1}^\infty (1/n,1)$ that does not have a finite subcover: we can always select a $\epsilon >0$ which is small enough that cannot be covered.
\end{example}


\begin{definition}[compact set via sequence]
Metric space $M$ is compact if every sequence in $M$ has a convergent subsequence.
\end{definition}

\begin{remark}
The equivalence between the two definitions are discussed in \cite[150]{johnsonbaugh2010foundations}
\end{remark}


\begin{theorem}[necessary condition for compact set]\index{compact set}
Compact sets are closed and bounded.
\end{theorem}
\begin{proof}
(1) Suppose it is not bounded, then we can have a sequence $\{x_n\}$ with $x_n \geq n$ that does not have convergent sequence. (2) It is closed can be proved directly from definition of closed set that every convergent sequence has a limit in it. 	
\end{proof}
 


\begin{remark}\textbf{Not every closed and bounded set in metric space is compact.} But a closed and bounded subset in $\R^n$ is compact.
\end{remark}

\subsubsection{closed set vs. compact set}
\begin{itemize}\index{closed set}
    \item the definition of the closed set says "every convergent sequence in $E$ has a limit in $E$"
    \item the definition of compact set says "every sequence has a convergent subsequence that has a limit in $E$"
    \item compact set has more restrictive requirement and therefore compact sets are always closed. 
\end{itemize}




\subsection{Compact sets in $\R^N$}

\begin{definition}[compact sets in $\R$]
\cite[96]{abbott2001understanding} A set $K\subseteq \R$ is compact if \textbf{every} sequence in $K$ that has a subsequence that converges to a limit in $K$.
\end{definition}




\begin{example}
The basic example is a closed interval. For every convergent sequence(hence bounded), there is a convergent subsequence (due to Bolzano-Weierstrass theorem \autoref{ch:sequences-series:th:Bolzano-Weierstrasstheorem}). Because it is closed, the convergent subsequence will have a limit in it. Therefore, the closed interval is compact.
\end{example}

\begin{definition}[bounded sets in $\R$]
A set $A\subseteq \R$ is bounded if there exists $M>0$ such that $\abs{a} < M,\forall a\in A$.
\end{definition}

\begin{theorem}[compact set on $\R^n$]
\cite[96]{abbott2001understanding}A set $S \subset \mathbb{R}$ is compact if and only if it is closed and bounded. 
\end{theorem}
\begin{proof}
(1) Suppose $K$ is compact, and assume $K$ is not bounded, that is given a positive $M>0$, we can always find $x\in K, \abs{x} > M$. Construct a sequence which consists of $\{x_n,x_n > n\}$, every subsequence of this sequence will be convergent and therefore bounded because of the assumption that $K$ is compact. However, we can find a subsequence(the sequence itself) is not bounded, therefore we have contradiction,and we prove a compact set must be bounded. 
To show a compact set is closed(every Cauchy sequence has a limit contained in it), because of the compactness, every Cauchy(therefore convergent) sequence will have subsequence converge to a limit in $K$. The limit of the subsequence must be the same limit of its master sequence. 
(2) The converse is straightforward, every sequence has a convergent subsequence; and because of the close set, the convergent subsequence must converge to a limit in $K$.	
\end{proof}




\begin{lemma}[compactness of $\R$]
\cite[145]{johnsonbaugh2010foundations} The space $\R$ is not compact.
\end{lemma}
\begin{proof}
The open cover $\cup_{n=1}^{\infty} (-n,n)$ does not have a finite subcover.	
\end{proof}



\begin{lemma}
\cite[99]{abbott2001understanding}Let $K$ be a subset of $\R$. If $K$ is non-empty and compact, then $\sup K$ and $\inf K$ both exist and are elements of $K$ 
\end{lemma}
\begin{proof}
Because $K$ is closed and bounded, then $\sup K$ and $\inf K$ both exist due to the completeness of $\R$. Suppose $\sup K$ in $K^C$, which is an open set, then there exist an open neighborhood around $\sup K$ in which some points are upper bound but smaller then $\sup K$, which is a contradiction.	
\end{proof}


\subsection{The Hine-Borel Theorem and boundedness of continuous function}
\begin{theorem}\cite[113]{johnsonbaugh2010foundations}
Let $\mathcal{F}$ be a collection of open intervals such that $$[a,b] \subset \cup \cF $$
Then there exist a finite subset $\{I_1,I_2,...,I_n\}$ of $\cF$ such that 
$$[a,b] \subset \cup_{i=1}^n I_i$$
\end{theorem}



\section{Completeness of metric space}
\subsection{Sequence and completeness}
\begin{definition}[Cauchy sequence]
\cite[159]{johnsonbaugh2010foundations} Let $(M,d)$ be a metric space. A sequence $\{x_n\}$ in $M$ is Cauchy sequence if for every $\epsilon >0$, there exists a $N>0$ such that 
if $m,n > N$, then $d(x_m,d_n) < \epsilon$.
\end{definition}

\begin{theorem}[every convergent sequence in metric space is Cauchy sequences]
	Every convergent sequence in a metric space is Cauchy sequence.
\end{theorem}
\begin{proof}
Using triangle inequality, similar to  \autoref{ch:sequences-series:th:CauchySequenceIsConvergent}.
\end{proof}





\begin{definition}[completeness of metric space]\label{ch:metric-space:def:completeMetricSpace}
\cite[159]{johnsonbaugh2010foundations}Let $M$ be a metric space. If every Cauchy sequence in $M$ is convergent to a point in $M$, then $M$ is a \textbf{complete} metric space.
\end{definition}

\begin{example}
The metric space $(0,2)$ with the Euclidean metric is not complete metric space because we can construct a Cauchy sequence $\{2-\frac{1}{n}\}$, whose every term is in $(0,2)$, but converges to 2(outside of $(0,2)$).  	
\end{example}




\begin{remark}
Even if the Cauchy sequence converge in a complete metric space, the limit point might not lie in the metric space. If the space is \textbf{closed}, then the limit lie in the metric space.
\end{remark}





\begin{remark}[compared with Cauchy sequence in $\R$]
In real line, \textbf{every Cauchy sequence is convergent, since real line is complete.}\autoref{ch:sequences-series:th:CauchySequenceIsConvergent} For general metric space, this is not true. 
\end{remark}



\subsection{Completeness of $\R^n$}


\begin{theorem}[metric space $\R^n$ with Euclidean metric is complete]\cite[323]{fitzpatrick2006advanced}\label{ch:metric-space:th:CompletenessOfRnWithEuclideanMetric}\hfill
\begin{itemize}
	\item The metric space $\R^n$ with the Euclidean metric is a complete metric space.
	\item Every closed subset of $\R^n$ is a complete metric space.
\end{itemize}	
\end{theorem}
\begin{proof}
(1)	From the definition of complete metric space(\autoref{ch:metric-space:def:completeMetricSpace}), we want to show that every Cauchy sequence in $\R^n$ is convergent to a point in $\R^n$. Now consider a Cauchy sequence $\{a^{(k)}\},a^{(k)}\in \R^n$, because
$$\abs{a^{(m)}_i - a^{(n)}_i} \leq \norm{a^{(m)} - a^{(n)}},i=1,2,...,n,$$
therefore each component sequence $\{a^{(k)}_i\}$ is a Cauchy sequence and will converge to an element in $\R$. The componentwise convergence will further imply the convergence of $\{a^{(k)}\}$ in $\R^n$.(\autoref{ch:metric-space:th:componentWiseConvergenceConditionForConvergenceInRn}). 
(2) Note that based on the definition of closedness, every convergent sequence will have its limit point in the subset; therefore, any closed subset of $\R^n$ is also a complete metric space.
\end{proof}






\begin{remark}
	Closed subset of real line is complete metric space, however, open subset is not. For example, open interval $(0,1)$ is not complete, because the Cauchy sequence $\{1/n\}$ not converge to an element in $(0,1)$.
\end{remark}


\section{Topology space}
\subsection{Definitions}
\begin{definition}[topology space]\index{topology space}\cite[136]{johnsonbaugh2010foundations}
A topological space is a set $X$ together with a set $\mathcal{J}$ of subsets of $X$, such that
\begin{enumerate}
\item $X \in \mathcal{J}$
\item $\emptyset \in \mathcal{J}$
\item The intersection of any \textbf{finite} elements in $\mathcal{J}$ will be an element of $\mathcal{J}$; that is, if $O_1,O_2,...,O_n \in \mathcal{J}$, then $O_1\cap O_2 \cap ...O_n \in \mathcal{J}$
\item if for each $\alpha \in \mathcal{I}$, $O_a \in \mathcal{J}$, then $\cup_{\alpha \in \mathcal{I}} O_{\alpha} \in \mathcal{J}$.
\end{enumerate}
$\mathcal{J}$ is called a topology on $X$.
\end{definition}

\begin{remark}[Why finite intersection]
Note that \textbf{only finite intersection is allowed, countable intersection is not allowed}, since we can countably intersect a nested interval to create a singleton, which is not an open set.
\end{remark}


\begin{example}\hfill
\begin{itemize}
    \item The real line $\R$ with its topology consist of open sets given by $(a,b)=\{x\in \R,a<x<b\}$ for any real numbers $a$ and $b$
\end{itemize}
\end{example}


\begin{definition}[Open sets]\index{open set}
Let $(X,\mathcal{J})$ be any topological space. Then the member of $\mathcal{J}$ are called open sets. Then in a \textbf{topological space}, we have 
\begin{enumerate}
\item $X,\emptyset$ are open sets
\item the union of any(finite or infinite) number of open sets is an open set
\item the intersection of any finite number of open sets is an open set
\end{enumerate}
\end{definition}

\begin{remark}
Note that open set in topology is an abstract concept. Different specifications or definitions of open set will give different topology space. \cite{wiki:openset}The simplest example is in metric spaces, where open sets can be defined as those sets which contain an open ball around each of their points, for example  in $ \R,(a,b)$ is defined as open sets.
\end{remark}


\begin{definition}[Hausdorff, Hausdorff space]
For any distinct $x_1,x_2\in X$, there exist open sets $O_1$ and $O_2$ such that $x_1 \in O_1, x_2\in O_2$, and $O_1\cap O_2 = \emptyset$. Any topological space $X$ that satisfies this is call \emph{Hausdorff space}. Or we say it is \textbf{Hausdorff}.
\end{definition}

\begin{remark}
Note: The concept of Hausdorff implies that around any two distinct points on $X$, we can find two non-overlapping open sets. \cite{krim2015geometric} 
\end{remark}

\begin{definition}[Neighborhood]
Given a topological space $(X,\mathcal{J})$, a subset $N$ of $X$ is called a neighborhood of a point $a\in X$ if $N$ contains an open set that contains $a$.
\end{definition}

\begin{remark}
	Neighborhood aournd a point is not necessarily an open set, but it must contain an open set.
\end{remark}



\subsection{Continuous function, Homeomorphism in topological space}
\begin{definition}[continuous function]
A function $f: X\rightarrow Y$ between two topological space $X,Y$ is continuous if for every open set $V \in Y$, the pre-image $f^{-1}(V)$ is an open set in $X$.
\end{definition}

\begin{remark}
consider a function on real line with some 'jumps', this function is not continuous because at one end of the discontinuity, it is closed. 
\end{remark}
 
 \begin{definition}[homemorphism]
 Suppose $f: X\rightarrow Y$ is a bijective function between topological spaces $X,Y$. If both $f,f^{-1}$ are continuous, then $f$ is called a \emph{homeomorphism}. Two topological spaces $X,Y$ are said to be \emph{homeomorphic} if there exist a homeomorphism between them.
 \end{definition}



\subsection{Subspaces of topological space}\index{subspace}
\begin{definition}
\cite{Mendelson}Let $(X,\mathcal{J}), (Y,\mathcal{J}')$ be topological spaces. The topological space $Y$ is called a \emph{subspace} of $X$ if $Y\subset X$ and if all the open subsets of $Y$ are precisely the subsets $O'$ of the form
$$O' = O \cap Y$$ for some open subset $O$ of $X$. If $Y$ is the subspace $X$, we may say that each open subset $O'$ of $Y$ is the restriction to $Y$ of an open subset $O$ of $X$.Note that a subset $O'$ that is open in $Y$ is often called \emph{relatively open}. Relatively open subsets of $Y$ are in general not open in $X$, because it does not contain its neighborhood in $Y$.
\end{definition}

\begin{example}\hfill
\begin{itemize}
    \item Consider $\R$ as a subspace in $\R^2$, the subsets $\R$ is just relatively open in $\R^2$. It is not open in $\R^2$, because it does not contain its neighborhood in $\R$.
\end{itemize}
\end{example}


\section{Notes on bibliography}
The chapter mainly reference intermediate level real analysis textbooks\cite{johnsonbaugh2010foundations}\cite{abbott2001understanding}\cite{thomson2001elementary}\cite{strichartz2000way}.
\cite{duren2012invitation}
\printbibliography

\end{refsection}
\begin{refsection}
\startcontents[chapters]	
\chapter{Calculus}\label{ch:calculus}
%\minitoc

\printcontents[chapters]{}{1}{}
\section{Continuous functions}
\subsection{Continuous function on $\R$}\index{continuous function}
\subsubsection{Basics}
\begin{definition}[continuity]
\cite[122]{abbott2001understanding}\cite[53]{fitzpatrick2006advanced}

 A function $f:D\to \R$ is continuous at $c\in D$ if for all $\epsilon > 0$, there exists a $\delta > 0$, such that for all $\abs{x-c} < \delta, \abs{f(x)-f(c)} < \epsilon$, or equivalently, 
$$\lim_{x\to c}f(x) = f(c).$$

$f$ is said to be \textbf{continuous} on $D$ if $f$ is continuous on every point in $D$.
\end{definition}

\begin{example}
The function $f(x)=1/x$ is continuous on real line except at $x=0$. 
\end{example}

\begin{definition}[continuity, alternative definition]
\cite[123]{abbott2001understanding} Let $\{x_n\}$ be a real-valued sequence with limit $x$. Let $f$ be a function defined on $\R$, then $f$ is continous at $x$ if
$$\lim_{n\to \infty} f(x_n) = f(\lim_{n\to\infty} x_n) = f(x)$$
\end{definition}

\begin{corollary}[criterion for discontinuity]
\cite[123]{abbott2001understanding}Let $f:A\to \R$, and let $c\in A$ be a limit point of $A$. If there exists a sequence $\{x_n\} \subseteq A$, and $x_n \to c$ but $f(x_n)$ not converges to $f(c)$
\end{corollary}

\begin{remark}[exchange of limit and function evaluation]
From the perspective of exchange operations, continuity ensures that we can exchange the operation of taking limit and evaluation.	
\end{remark}

\begin{lemma}[algebraic properties of continuity]\cite[55]{fitzpatrick2006advanced}
Suppose that the function $f:D\to \R$ and $g:D\to \R$ are continuous at the point $x_0\in D$. Then
\begin{itemize}
	\item the sum $f+g:D\to \R$ is continuous at $x_0$.
	\item the product $fg:D \to \R$ is continuous at $x_0$.
	\item If $g(x)\neq 0$ for all $x\in D$, then
	$$f/g:D\to\R$$
	is continuous at $x_0$.
\end{itemize}
\end{lemma}
\begin{proof}
(1) Let $\{x_n\}$ be a sequence in $D$ convergent to $x_0$. By the definition of continuity, we have
$$\lim_{n\to \infty} f(x_n) = f(x_0), \lim_{n\to \infty} g(x_n) = g(x_0).$$
The algebraic property of limits \autoref{ch:sequences-series:th:algebraOfLimits} gives
$$\lim_{n\to \infty} f(x_n) + g(x_n) = f(x_0) + g(x_0).$$
(2)	 Let $\{x_n\}$ be a sequence in $D$ convergent to $x_0$. By the definition of continuity, we have
$$\lim_{n\to \infty} f(x_n) = f(x_0), \lim_{n\to \infty} g(x_n) = g(x_0).$$
The algebraic property of limits \autoref{ch:sequences-series:th:algebraOfLimits} gives
$$\lim_{n\to \infty} f(x_n)g(x_n) = f(x_0)g(x_0).$$
(3) similar to (1)(2).
\end{proof}



\begin{lemma}[continuity of composition]
For functions $f:D\to \R$ and $g:D\to \R$ such that $f(D)\subset U$, assume $f$ continuous at the point $x_0\in D$ and $g:U\to \R$ is continuous at the point $f(x_0)$. Then the composition 
$$g\circ f: D\to \R$$
is continuous at $x_0$.
\end{lemma}
\begin{proof}
Let $\{x_n\}$ be a sequence in $D$ convergent to $x_0$. By the definition of continuity of $f$, we have
$$\lim_{n\to \infty} f(x_n) = f(x_0)$$.
Then $\{f(x_n)\}$ is a sequence convergent to $f(x_0)$. By the definition of continuity of $g$, we have
$$\lim_{n\to \infty} g(f(x_n)) = g(f(x_0))$$.
Therefore $g\circ f$ is continuous at $x_0$.
\end{proof}




\begin{theorem}[continuous via open sets]
\cite[12]{spivak} If $A\subseteq \R^n$, a function $f:A\to \R^m$ is continuous if and only if for every open set $U\in \R^m$ there is some open set $V\subseteq \R^n$ such that $f^{-1}(U) = V\cap A$
\end{theorem}
\begin{proof}
(1) Suppose $f$ is continuous. If $a\in f^{-1}(U)$, then $f(a) \in U$. Since $U$ is open, then there exist an open ball $B$ in $U$ that $f(a)\in B$. Since $f$ is continuous at $a$, then there exist an open ball $C$ in $A$ such that $f(C) \subseteq B$. Do this for each $a\in A$, and let the union of all these $C$ be $V$. Then $V$ will be an open set, since union of open sets are open. And clearly $f^{-1}(U) = V\cap A$. (2) The converse is directly form the limit based definition of continuity. Given $\epsilon > 0$, let $B$ be an open set around $f(a)$ such that $\norm{f(x)-f(a)} < \epsilon$. From the assumption, there exist an open set $V\cap A = f^{-1}(B)$, such that every element $x\in V\cap A$ will have $\norm{f(x)-f(a)} < \epsilon$. Then there exist an open ball $C$ with radius $\delta$, such that $C\subseteq V\cap A$	
\end{proof}



\begin{remark}
The nature of continuity is that when select an open ball $B$ in the range, no matter how small the open ball is, we can always an open ball $C$ in the domain such that $f(C) \subseteq B$. Consider a discontinuous function having an isolated point $f(a)$, choose an small open ball $B$ containing $f(a)$, then $f^{-1}(B)$ is finite sized set, which is closed(see metric space chapter for details.).    
\end{remark}

\begin{theorem}[Intermediate value theorem]\index{Intermediate value theorem}\label{ch:calculus:th:intermediatevaluetheoremonrealline}\cite[63]{fitzpatrick2006advanced}
	If the real-valued function $f$ is continuous on the closed interval $[a, b]$ and $f(a) < f(b)$. Then for any $y$ in between, $f(a) < y < f(b)$, there exists $x\in (a,b)$ with $f(x) = y$.
\end{theorem}
\begin{proof}
	We use divide and conquer plus nested interval method to prove. We subset the interval $[a_n,b_n]$ half each time, such that the subset $[a_{n+1},b_{n+1}]$ has the property that $y$ is lying between $f(a_{n+1})$ and $f(b_{n+1})$. Continue this process, we know the intersection is not empty and the element within this interval can be made arbitrarily small to $y$.
\end{proof}


\begin{remark}[interpretation]
	The intermediate value theorem is an existence theorem, based on the real number property of completeness
\end{remark}

\begin{lemma}[map an interval to an interval]\label{ch:calculus:th:continuousFunctionMapIntervalToInterval}\cite[65]{fitzpatrick2006advanced}
Let $I$ be an interval and suppose that the function $f:I\to \R$ is continuous. Then its image $f(I)$ also is an interval.	
\end{lemma}
\begin{proof}
To show $f(I)$ is an interval, we want to show $f(I)$ is a convex set. Take $y_1,y_2\in f(I), y_1 < y_2$. From intermediate value theorem we have for any $c\in [y_1,y_2]$, there exists an $x_0$ between $x_1$ and $x_2$, where $f(x_1)=y_1,f(x_2)=y_2$. Therefore, $c \in f(I)$ since $x_0\in I$.
\end{proof}



\begin{lemma}[preservation on compactness ]
	\cite[129]{abbott2001understanding}\cite[12]{spivak} If $f:A\to \R^m$ is continuous, where $A\subseteq \R^n$, and $A$ is compact, then $f(A)\subseteq \R^m$ is compact.
\end{lemma}
\begin{proof}
	Let $\mathcal{O}$ be an open cover of $f(A)$. Because every element in $B\in \mathcal{O}$ has an open set $C \subseteq A$ and $C = f^{-1}(B)\cap A$. The pre-image of every element in $\mathcal{O}$ must be an open cover $\mathcal{O}_A$ for $A$. Because $A$ is compact, then there exist an finite subcover, the image of the subcover must be the finite subcover of $\mathcal{O}$, therefore, every open cover of $f(A)$ has a finite subcover, therefore $f(A)$ is compact.  	
\end{proof}


\subsubsection{Continuity and inverse}

\begin{lemma}[continuity and image topology] \cite[76]{fitzpatrick2006advanced}\label{ch:calculus:th:continuityAndImagetopogoly}
Let $f:I\to \R, I=(a,b)$ be monotone increasing and with range $f(I)$ being an interval. Then $f$ is continuous on $I$.

conversely, if $f$ is continuous on $I$, then its range $f(I)$ is also an interval.	
\end{lemma}
\begin{proof}
Suppose $f$ has a jump at $x_0$. Let $y_0^-$ and $y_0^+$ be the two side limit values at the jump. 
Then at least $(y_0^-, f(x_0))$ or $(f(x_0),y_0^+)$ must be nonempty(otherwise there cannot be jumps). Pick the nonempty one and denote it $J$. Note that
$$J\subset (y_0^-,y_0^+)\subset (f(a),f(b)),$$
so the range $(f(a),f(b))$ contains an interval.	

Therefore, $f$ cannot contain jumps and must be continuous.


The second part directly from the fact that continuous function maps an interval to an interval\autoref{ch:calculus:th:continuousFunctionMapIntervalToInterval}.
\end{proof}

\begin{remark}[geometric intuition]
Suppose a monotone increasing function has a jump. Then if standing on the $x=+\infty$ and look into the direction of $-x$, then we will see an jump projecting on the $y$ axis and make the range not an interval.	
\end{remark}


\begin{lemma}[continuity of inverse]\label{ch:calculus:th:continuityOfInversefunctions}
	If $f$ is continuous and strictly monotone on an interval, then $f^{-1}$ is also continuous.	
\end{lemma}
\begin{proof}
(1)first method.	Consider strictly increasing function. Let $x_0$ be any point in the interval and $f(x_0) = y$. For any given $\epsilon > 0$, we take
	$$\delta = \min(f(x_0) - f(x_0-\epsilon),f(x_0+\epsilon)-f(x_0)),$$
	then when $\abs{y - y_0} < \delta$, we have
	$$y_0 - \delta < y < y + \delta,$$
	Because $f^{-1}$ is also strictly increasing, we have
	$$f^{-1}(y_0 - \delta) < f^{-1}(y) < f^{-1}(y + \delta).$$
	Further use the fact that
	$$x_0 - \epsilon < f^{-1}(y_0 - \delta),$$
	we have
	$$x_0 - \epsilon < x < x_0 + \epsilon, $$
	
	That is
	$$\abs{x-x_0} = \abs{f^{-1}(y) - f^{-1}(y_0)} < \epsilon.$$
(2) second method. Use the fact that $f^{-1}$ is also strictly monotone and the image of $f^{-1}$ is an interval. Then use \autoref{ch:calculus:th:continuityAndImagetopogoly}. 	
\end{proof}

\subsection{Continuous function in metric space}
\begin{definition}[continuous in metric space]
	\cite[136]{johnsonbaugh2010foundations}
	Let $(M_1,d_1)$ and $(M_2,d_2)$ be metric spaces, let $f$ be a function from $M_1$ to $M_2$. We say function $f$ is continuous at some point $c$ if for every $\epsilon >0$, there exist a $\delta >0$, such that for all points with $d_1(x,c)<\delta$, we have $d_2(f(x),f(c)) < \epsilon$. \cite{johnsonbaugh2010foundations}
\end{definition}

\begin{definition}[continuous in metric space, alternative]
	\cite[136]{johnsonbaugh2010foundations}
	Let $f$ be a function from a metric space $M_1$ into a metric space $M_2$. Let $a\in M_1$. Then $f$ is continuous at $a$ if and only if whenever $\{x_n\}$ is a sequence in $M_1$ such that $\lim_{n\rightarrow \infty} x_n = a$, then $\lim_{n\rightarrow \infty} f(x_n) = f(a)$
\end{definition}

\begin{theorem}[Preserving topological property via continuity]\label{ch:calculus:th:continuitypreserveTopology}
	\cite[139]{johnsonbaugh2010foundations} Let $f$ be a function from a metric space $M_1$ into a metric space $M_2$. The following are equivalent:
	\begin{itemize}
		\item $f$ is continuous on $M_1$
		\item $f^{-1}(C)$ is closed whenever $C$ is a closed subset of $M_2$
		\item $f^{-1}(C)$ is open whenever $C$ is a open subset of $M_2$
	\end{itemize}
\end{theorem}
\begin{proof}
	We first prove (1)(3) are equivalent: (a) Suppose $f$ is continuous. If $a\in f^{-1}(U)$, then $f(a) \in U$. Since $U$ is open, then there exist an open ball $B$ in $U$ that $f(a)\in B$. Since $f$ is continuous at $a$, then there exist an open ball $C$ in $A$ such that $f(C) \subseteq B$. Do this for each $a\in A$, and let the union of all these $C$ be $V$. Then $V$ will be an open set, since union of open sets are open. And clearly $f^{-1}(U) = V\cap A$. (b) The converse is directly form the limit based definition of continuity. Given $\epsilon > 0$, let $B$ be an open set around $f(a)$ such that $\norm{f(x)-f(a)} < \epsilon$. From the assumption, there exist an open set $V\cap A = f^{-1}(B)$, such that every element $x\in V\cap A$ will have $\norm{f(x)-f(a)} < \epsilon$. Then there exist an open ball $C$ with radius $\delta$, such that $C\subseteq V\cap A$	
\end{proof}






\begin{lemma}[preservation on compactness]
	\cite[152]{johnsonbaugh2010foundations} If $f$ is a continuous function from a compact metric space to $M_1$ to a metric space $M_2$, then $f(M_1)$ is compact.
\end{lemma}
\begin{proof}
	For any sequence in $f(M_1)$, the inverse image of the sequence in $M_1$ will have a convergent subsequence $\{x_i\}$. Then then image of this subsequence will be a convergent subsequence in $f(M_1)$ since $f$ is continuous. Therefore, every sequence in $f(M_1)$ has a convergent subsequence and thus $f(M_1)$ is compact.
\end{proof}

\begin{corollary}
	If $f$ is a continuous function from a compact metric space to $M_1$ to a metric space $M_2$, then $f(M_1)$ is closed and bounded.
\end{corollary}
\begin{proof}
	Use the fact that compact set is closed and bounded.
\end{proof}



\subsection{Boundedness and extreme value theorems}
\begin{lemma}[Boundedness of continuous function]
	If $f$ is continuous at $c$ in real line, there exists $\delta > 0$ such that $f$ is bounded on $(c-\delta,c+\delta)$
\end{lemma}
\begin{proof}
	Because $f$ is continuous at $c$, then given $\epsilon = 1$, there exist a $\delta > 0$, such that $\abs{f(x)-f(c)} < 1$ on whenever $x\in (c-\delta,c+\delta)$.	
\end{proof}


\begin{theorem}[Boundedness of continuous function on closed interval]\label{ch:calculus:th:boundednesscontinuousfunctiononclosedinterval}
	\cite[114]{johnsonbaugh2010foundations}If $f$ is continuous at $[a,b]$, then $f$ is bounded on $[a,b]$.  
\end{theorem}
\begin{proof}
	Note that there exists an open interval $I_c$ around $c$ where $f$ is bounded. The collections of $I_c,c\in [a,b]$ is an open cover for $[a,b]$, then there exist a finite subcover. Since $f$ is bounded on each of the element in the subcover, $f$ is bounded on $[a,b]$.
\end{proof}


\begin{theorem}[Extreme value theorem in $\R$]
	\cite[114]{johnsonbaugh2010foundations}\cite[130]{abbott2001understanding}\label{ch:calculus:th:extremevaluetheoremfunctiononclosedinterval}
	If a function $f$ is defined on a closed interval $A=[a,b]$ (or any closed and bounded set) and is continuous there, then the function attains its maximum; The same is true of the minimum of $f$.
\end{theorem}
\begin{proof}
	Because $f(A)$ is compact, then it has a maximum and minimum contained in $f(A)$.	
\end{proof}


\begin{lemma}[boundedness at continuous point]
	\cite[146]{johnsonbaugh2010foundations} Let $f$ be a real-valued function on a metric space $M$. If $f$ is continuous at $a\in M$, then there exist an open set $U\subset M$ containing $a$ such that $f$ is bounded on $U$.
\end{lemma}
\begin{proof}
	Since $f$ is continuous at $a$, then for $\epsilon = 1$, there exists an open ball $B(a,\delta)(\delta > 0)$ such that 
	$$\abs{f(x) - f(a)} < 1,\forall x \in B(a,\delta)$$
	which implies 
	$$\abs{f(x)} < \abs{f(a)} + 1,\forall x \in B(a,\delta)$$
\end{proof}


\begin{theorem}[boundedness at compact set]
	\cite[146]{johnsonbaugh2010foundations} If $f$ is continuous on a compact metric space $M$ then $f$ is bounded on $M$.
\end{theorem}
\begin{proof}
	Since $f$ is continuous at every point $a\in M$, then the associated collection of open balls will cover $M$. Because $M$ is compact, then there is a finite subcover covers $M$. Since $f$ is bounded on each subset in the subcover, $f$ will be bounded on the subcover.
\end{proof}


\begin{corollary}[Weierstrass theorem]\index{Weierstrass theorem}
	\cite[146]{johnsonbaugh2010foundations}
	A continuous real-valued function on a compact set $M$ achieves a maximum and a minimum.
\end{corollary}
\begin{proof}
	Use the fact the $f$ is bounded on $M$ and the real line is complete.
\end{proof}


\begin{corollary}[non-empty compact lower set implies existence of global minimizer]
Let $f:\R^n\to \R$ be continuous on all of $\R^n$. If there exists a scalar $a\in \R$ such that the level set
$$S=\{x\in X: f(x) <a\}$$
is nonempty and compact, then there exists at least one global minimum.
\end{corollary}
\begin{proof}
 There exists at least one global solution $f(x^*)$ on $S$. And since $S\subset \R^n$ and $f(x) > \alpha,\forall x\in \R^n-S$, therefore $f(x^*)$ will be the global minimizer. 
\end{proof}

\subsection{More on extreme values}

\begin{definition}[coercive function]\index{coercive function}
A function $\R^D\to \R$ is said to be coercive if for every sequence $\{x_n\} \in \R^D$, $\norm{x_n}\to \infty$ implies $f(x_n)\to \infty$ as $n\to \infty$.
\end{definition}

\begin{remark}\hfill
\begin{itemize}
	\item Any monotonic function will not be coercive.
	\item Coercive functions have the general bowl shape that goes to positive infinite as $\norm{x}$ goes to infinite.
\end{itemize}
\end{remark}

\begin{lemma}[coercity and compactness]
Let $f:\R^n\to \R$ be continuous on all of $\R^n$. The function $f$ is coerice if and only if for every $\alpha \in\R$ the set $\{x|f(x)\leq \alpha\}$ is compact.
\end{lemma}
\begin{proof}
(1) First the interval $(-\infty,\alpha]$ is a closed set, therefore the preimage $f^{-1}((-\infty,\alpha])$ is a closed set due to continuity of $f$(\autoref{ch:calculus:th:continuitypreserveTopology}). The set $f^{-1}((-\infty,\alpha])$ must be bounded, otherwise as $\norm{x}\to \infty$, $f$ can not be bounded by $\alpha$ by the definition of coercity of $f$. Therefore, for every $\alpha \in\R$ the set $\{x|f(x)\leq \alpha\}$ is closed and bounded, and hence compact.
(2) Consider a sequence $\{x_n\}$ where $\norm{x_n}\to \infty$ as $n\to \infty$. Suppose there exists a subsequence $\{x_{n_k}\}$ such that $\{f(x_{n_k})\}$ is bounded, then there will a compact set $S=\{x|f(x)\leq \alpha\}$ such that $\{x_{n_k}\}\subset S$. Since $S$ is compact, then the subsequence $\{x_{n_k}\}$ must be bounded, which contradicts $\norm{x_n}\to \infty$ as $n\to \infty$. Therefore, for any sequence $\norm{x_n}\to \infty$ as $n\to \infty$, we have $f(x_n)\to \infty$.
\end{proof}


\begin{theorem}[existence of global minimizer for coercive function]\label{ch:calculus:th:WeierstrassCoerciveFunc}
Let $f:\R^n\to \R$ be continuous on all of $\R^n$. If $f$ is coercive, then $f$ has at least one global minimizer.
\end{theorem}
\begin{proof}
Choose $\alpha \in \R$ such that the set $S= \{x|f(x)\leq \alpha \}$ is non-empty. By coercivity, this set is compact, and therefore there exists at least one global solution $f(x^*)$. And since $S\subset \R^n$ and $f(x) > \alpha,\forall x\in \R^n-S$, therefore $f(x^*)$ will be the global minimizer. 
\end{proof}



\section{Uniform continuity}\index{uniform continuity}

\subsection{Uniform continuity on real line}
\subsubsection{Concepts}
\begin{definition}[uniformly continuous]\cite[67]{fitzpatrick2006advanced}
A function $f:D\to \R$ is said to be \textbf{uniformly continuous} provided that whenever $\{u_n\}$ and $\{v_n\}$ are sequences in $D$ such that
$$\lim_{n\to \infty} u_n - v_n = 0,$$
we have
$$\lim_{n\to \infty} f(u_n) - f(v_n) = 0.$$
\end{definition}

\begin{definition}[uniformly continuous, alternative]\cite[73]{fitzpatrick2006advanced}
A function $f:D\to \R$ is said to be \textbf{uniformly continuous} if given  any $\epsilon > 0$, there exists a positive number $\delta$ such that for \textbf{all} $u,v$ in $D$, 
$$\abs{f(u)-f(v)} < \epsilon, ~whenever~ \abs{u - v} < \delta.$$
\end{definition}




\begin{note}[uniform continuity vs. continuity]
Note that uniform continuity is a stronger continuity condition. If a function is uniformly continuous then it is continuous. It can be showed by replaced $v$ to be any fixed $x_0\in D$.	
\end{note}

\begin{example}
	Define $f(x) = x^2, x\in \R$. Then $f$ is continuous on $\R$ but not uniformly continuous on $\R$.	
	For example, set
	$$u_n = n, v_n = n + 1/n.$$
	Then $$\lim_{n\to \infty} u_n - v_n = \lim_{n\to \infty} 1/n =0,$$
	but
	$$\lim_{n\to \infty} f(u_n) - f(v_n) = \lim_{n\to \infty} 2 + 1/n^2 =2\neq 0.$$
\end{example}


\begin{example}
	Define $f(x) = 1/x, x\in (0,1)$. Then $f$ is continuous on $(0,1)$ but not uniformly continuous on $\R$.	
	For example, set
	$$u_n = 1/n, v_n = 1/2n.$$
	Then $$\lim_{n\to \infty} u_n - v_n = \lim_{n\to \infty} 1/2n =0,$$
	but
	$$\lim_{n\to \infty} f(u_n) - f(v_n) = \lim_{n\to \infty} -n \neq 0.$$	
\end{example}


\begin{lemma}[uniform continuity preserving Cauchy sequence]
	Suppose $D\subset R$ and $f:D\to R$ is uniformly continuous. If $\{x_n\}$ is a Cauchy sequence in $D$, then $\{f(x_n)\}$ is a Cauchy sequence in $f(D)$.
\end{lemma}	
\begin{proof}Given any $\epsilon_1 > 0$, there exists $N>0$, such that if $m,n>N$, then
	$$\abs{x_m-x_n} < \epsilon_1$$
	Given any $\epsilon_2 > 0$, there exist a $\delta$, such that
	if $\abs{x_m-x_n} < \delta$, we have
	$$\abs{f(x_m)-f(x_n)} < \epsilon_2$$
	Set $\epsilon_1 = \delta$, then we can find such $N>0$, such that if $m,n>N$, then
	$$\abs{f(x_m)-f(x_n)} < \epsilon_2$$
\end{proof}


\begin{remark}
	Notes:
	\begin{itemize}
		\item Continuity is insufficient to preserve Cauchy sequences, but uniform continuity will. 		
		\item The function $f(x)=x/(1-x)$ is continuous but not uniformly continuous. The sequence $x_n = 1-1/n$ is a Cauchy sequence, its image $f(x_n)=n-1$ is not. \cite{carter2001foundations}
	\end{itemize}
\end{remark}


\begin{lemma}[continuous function on a closed bounded interval is uniformly continuous]\cite[68]{fitzpatrick2006advanced}
Let $f:[a,b]\to \R$ be a continuous function on$[a,b]$, then $f$ is uniformly continuous.	
\end{lemma}
\begin{proof}
Let $\{u_n\}$ and $\{v_n\}$ be two sequences in $[a,b]$ such that $$\lim_{n\to \infty} u_n - v_n = 0.$$

For the purpose of contradiction, we assume there exist some $\epsilon > 0$ such that 
$$\abs{f(u_n) - f(v_n)} \geq \epsilon, \forall n.$$

From \autoref{ch:sequences-series:th:Bolzano-Weierstrasstheorem}, there exists subsequences $\{u_{n_k}\},\{v_{n_k}\}$ converge to the same limit $x_0$.



However, $f(x)$ is continuous, we have
$$\lim_{k\to\infty} f(u_{n_k}) = f(x_0),\lim_{k\to\infty} f(v_{n_k}) = f(x_0). f(x_0).$$
Therefore, $$\lim_{k\to\infty} f(u_{n_k}) - f(v_{n_k}) = 0,$$
contradicts the fact that 	there exist some $\epsilon > 0$ such that 
$$\abs{f(u_n) - f(v_n)} \geq \epsilon, \forall n.$$
\end{proof}


\begin{lemma}[uniform continuity on open interval implies boundedness]
Suppose that the function $f:(a,b)\to \R$ is uniformly continuous. Then $f$ is bounded on $A = (a,b)$.	
\end{lemma}
\begin{proof}
Assume that $f$ is unbounded, and $sup_{x\in A}f(x) = \infty.$ Then there exits a sequence $\{x_n\}\in A$ such that $\lim_{n\to \infty} f(x_A) = \infty.$
Pick a subsequence $\{y_n\}$ from $\{x_n\}$ such that $f(y_{n+1}) - f(y_n) > 1, \forall n\in \cN$. 

Since $f$ is uniformly continuous, we should have that there exists a $\delta$ such that
$$\abs{x-y}<\delta, x,y\in A \implies \abs{f(x)-f(y)} < 1.$$ 	


From \autoref{ch:sequences-series:th:Bolzano-Weierstrasstheorem}, the bounded sequence $\{y_n\}$ has a convergent subsequence $\{z_n\}$ such that for $\delta$, there exists $N$ such that for all  $m,n > N$
$$\abs{z_m - z_n} < \delta, \abs{f(z_m)-f(z_n)} > 1,$$
which is a contradiction.  	
\end{proof}


\begin{remark}[compare with continuous function]
Note that a continuous function on an open interval cannot ensure boundedness; for example $f(x)=1/x, x\in (0,1)$.	
\end{remark}

\subsubsection{Lipschitz continuity}
\begin{definition}[Lipschitz continuous]\index{Lipschitz continuous}\hfill
\begin{itemize}
	\item 	A function $f:X\rightarrow \R, X\subseteq \R$ is \textbf{locally Lipschitz continuous} at $x_0 \in X$ if there is a constant $\beta \geq 0$ and $\delta > 0$ such that for every $\abs{x-y} < \delta$
	$$\abs{f(x)-f(x_0)} \leq \beta \abs{x-x_0}.$$
	\item 
		A function $f:X\rightarrow \R, X\subseteq \R$ is \textbf{globally Lipschitz continuous} on $X$ if there is a constant $\beta \geq 0$ such that for every $x,x_0 \in X$
		$$\abs{f(x)-f(x_0)} \leq \beta \abs{x-x_0}$$
\end{itemize}	
\end{definition}

\begin{lemma}[Lipschitz continuity implies continuity]
Suppose a function $f:X\rightarrow \R$ is locally Lipschitz continuous at $x_0 \in X$ with a constant $\beta \geq 0$. Then $f$ is continuous at $x_0$.	
\end{lemma}
\begin{proof}
Let $\{x_n\}$ be a sequence with limit of $x_0$.	
From definition of Lipschitz continuity, we have that any given $\epsilon >0$, there exist a $N$ such that for all $n > N$, we have
$$\abs{x-x_n} < \epsilon/\beta,$$
and
$$ \abs{f(x_n) -f(x_0)}<\beta \abs{x-x_n} < \epsilon. $$
That is
$$\lim_{n\to \infty} f(x_n) = f(x_0).$$
\end{proof}


\begin{lemma}[globally Lipschitz continuity implies uniformly continuity]
	A globally Lipschitz function is uniformly continuous.
\end{lemma}
\begin{proof}
	Let $\beta$ be the Lipschitz constant. 
	Given any $\epsilon > 0$, we can find a $\delta = \epsilon/\beta$
	$$\abs{f(x)-f(y)} < \beta \norm{x-y} < \epsilon$$
	for $\norm{x-y}\leq \delta$
\end{proof}







\begin{lemma}[differentiability and local Lipschitz continuity]
	\cite[312]{johnsonbaugh2010foundations}\hfill
	\begin{itemize}
		\item If $f:D\to \R$ is differential at $x$, then it is locally Lipschitz continuous at $x$; 
		\item If $f$ is locally Lipschitz continuous at $x$ with constant $K$, then $f$ is differentiable at $x$ and with $\abs{f'(x)} \leq K$. 
		\item A function $f$ is differential and $\abs{f'(x)}\leq M, \forall x$, then $f$ is globally Lipschitz and uniformly continuous.
		\item A continuously differentiable function $f$ defined on a closed and bounded interval of $\R$ is globally Lipschitz continuous and uniformly continuously. 
	\end{itemize}
	
	
	
\end{lemma}
\begin{proof}
	(1)Because $f$ is differential at $x$, then given $\epsilon = 1$, there exist a $\delta > 0 ,\forall 0<\abs{y-x} < \delta$, we have
	$$\abs{\frac{f(y)-f(x)}{y-x} - f'(x)} < 1$$
	rearrange and we have
	$$\abs{f(y)-f(x)} \leq (1+f'(x))\abs{y-x}.$$(2) If it is locally Lipschitz, then there exists a neighborhood around $x$ such that $$\frac{f(y)-f(x)}{y-x}$$ is bounded. When take the limit $y\to x,y\neq x$, the limit is bounded.
	(3)
	from (1).
	(4) A continuously differentiable function on a closed and bounded interval has bounded derivatives. 
\end{proof}



\begin{example}\hfill
\begin{itemize}
	\item $f(x) = \abs{x}$ is \textbf{globally Lipschitz continuous and uniformly continuous but not differentiable at $x = 0$}.
	\item $f(x) = \sqrt{x}$ defined on $[0,1]$ is \textbf{continuous but not Lipschitz continuous at $x = 0$}.
	\item  A differentiable function $f$ on the real numbers need not be a continuously differentiable function, that is, $f'$ is not continuous.
	Consider the function:
	$$g(x) = 
	\begin{cases}
	x^{3/2}\sin(1/x),x\neq 0\\
	0,x=0
	\end{cases}$$
	Then
	$$g'(x) = 
	\begin{cases}
	\frac{3}{2}x^{1/2}\sin(1/x)-x^{-1/2}\cos(1/x),x\neq 0\\
	0,x=0
	\end{cases}.$$
	Note that $g$ has unbounded derivative when $x\to 0$, but $g'(0) = \lim_{x\to 0}\frac{	x^{3/2}\sin(1/x)}{x} = 0$. $g$ is not global Lipschitz continuous since $g'$ is unbounded near $x = 0$.
\end{itemize}	
\end{example}



\begin{remark}[continuity, Lipschitz continuity, uniform continuity, differentiability]
In general, for functions defined over a closed and bounded subset of real line, we have
$$continuously~differentiable\subseteq Lipschitz~continuous \subseteq uniformly~continuous~continuous$$	
\end{remark}










\subsection{Uniform continuity on metric space}
\begin{definition}
A function $f:X\rightarrow Y$ is \emph{uniformly continuous} if for every $\epsilon > 0$ there exist a $\delta > 0$ such that for every $x,x_0 \in X$,
$$d(x,x_0) \leq \delta \Rightarrow d(f(x),f(x_0)) < \epsilon $$
\end{definition}

\begin{remark}[criterion for absence of uniform convergence]
\cite[132]{abbott2001understanding}
A function $f:A\to B$ fails to be uniformly continuous on $A$ if there exists a particular $\epsilon > 0$ and two sequence $\{x_n\}$ and $\{y_n\}$ in $A$ satisfying
$$d(x_n,y_n)\to 0, n\to \infty$$
but
$$d(f(x_n)-f(y_n)) \geq \epsilon$$
\end{remark}


\begin{theorem}[continuity implies uniform continuity on compact set]
\cite[154]{johnsonbaugh2010foundations}If $f$ is continuous function from a compact metric space $M_1$ into a metric space $M_2$, then $f$ is uniformly continuous on $M_1$. 
\end{theorem}
\begin{proof}
Given a $\epsilon > 0$, for every $z\in M_1$, there exists a $delta(z) > 0$, such that for all $x \in B(z,\delta(z))$, $d(f(x),f(z)) < \epsilon$. These open balls will form an open cover on $M_1$, and since $M_1$ is compact, there exists a finite subcover consisting of a finite set $J$ of open balls. Let $\delta$ be the smallest radius of open balls in $J$, then for any point $x\in M$, there exists an open ball with center $z$ within $\delta$ distance to $x$. Then for any point $x,y$ satisfying $d(x,y)<\delta/2$, there will exist a $z$ such that $x,y\in B(z,\delta)$. Then we can use triangle inequality as
$$d(f(x),f(y)) \leq d(f(x),f(z)) + d(f(z),f(y)) =2\delta$$
\end{proof}

\begin{corollary}
If $f$ is a continuous real-valued function on a closed and bounded subset $X$ of $\R^n$, then $f$ is uniformly continuous on $X$.
\end{corollary}

\begin{remark}\hfill
\begin{itemize}
	\item uniform continuity implies continuity, but not converse. 
\end{itemize}
\end{remark}


\subsection{Locally and globally Lipschitz continuous}
\begin{definition}[locally Lipschitz continuous]\index{Lipschitz continuous}
A function $f:X\rightarrow Y$ is locally \emph{Lipschitz} continuous at $x_0 \in X$ if there is a constant $\beta \geq 0$ and $\delta > 0$ such that for every $\norm{x-y} < \delta$
$$\norm{f(x)-f(x_0)} \leq \beta \norm{x-x_0}$$
\end{definition}

\begin{lemma}[sufficient condition for local Lipschitz]
\cite[312]{johnsonbaugh2010foundations}
If $f$ is differential at $x$, then it is locally Lipschitz continuous at $x$.
\end{lemma}
\begin{proof}
Because $f$ is differential at $x$, then given $\epsilon = 1$, there exist a $\delta > 0 ,\forall 0<\abs{y-x} < \delta$, we have
$$\abs{\frac{f(y)-f(x)}{y-x} - f'(x)} < 1$$
rearrange and we have
$$\abs{f(y)-f(x)} \leq (1+f'(x))\norm{y-x}$$
\end{proof}

\begin{lemma}[necessary and sufficient condition for local Lipschitz]
A differentiable function $g: \R \to \R$ is locally Lipschitz continuous at $x$if and only if it has bounded first derivative at $x$
\end{lemma}
\begin{proof}
(1) The sufficient part is the same as above theorem. (2) If it is locally Lipschitz, then there exists a neighborhood around $x$ such that $$\frac{f(y)-f(x)}{y-x}$$ is bounded. When take the limit $y\to x,y\neq x$, the limit is bounded.
\end{proof}



\begin{example}
The function $f(x) = x^{1/3}$ has unbounded first derivative at $x=0$, and therefore it is not locally Lipschitz there.
\end{example}




\begin{definition}[globally Lipschitz continuous]
A function $f:X\rightarrow Y$ is globally \emph{Lipschitz} continuous if there is a constant $\beta \geq 0$ such that for every $x,x_0 \in X$
$$\abs{f(x)-f(x_0)} \leq \beta \norm{x-x_0}$$
\end{definition}

\begin{lemma}[globally Lipschitz continuous implies uniformly continuous]
A globally Lipschitz function is uniformly continuous.
\end{lemma}
\begin{proof}
	Let $\beta$ be the Lipschitz constant. 
Given any $\epsilon > 0$, we can find a $\delta = \epsilon/\beta$
$$\abs{f(x)-f(y)} < \beta \norm{x-y} < \epsilon$$
for $\norm{x-y}\leq \delta$
\end{proof}





\begin{lemma}[bounded differential implies global Lipschitz]
A function $f$ is differential and $\abs{f'(x)}\leq M, \forall x$, then $f$ is globally Lipschitz. 
\end{lemma}
\begin{proof}
from mean value theorem.	
\end{proof}





\section{Differentiation}
\subsection{Differential function concept}
\begin{definition}[differential function]\index{differential function}\hfill
\begin{itemize}
	\item (differentiability at a point)Let $f$ be a real-valued function define at $(a,b) \subset \R$. If the limit:
	$$f'(x)=\lim_{y\rightarrow x} \frac{f(y)-f(x)}{y-x}$$
	exists, then $f$ is differentiable at $x$.
	\item (differentiability for an open interval)Let $f$ be a real-valued function define at $(a,b) \subset \R$. We say $f$ is a differentiable function in this interval $(a,b)$ if $f$ is differentiable for $\forall x\in (a,b)$.
\end{itemize}	

\end{definition}

\begin{lemma}[differentiability implies continuity]\cite[91]{fitzpatrick2006advanced} If $f$ is differentiable at a point $c$, then $f$ is continuous at $c$.
\end{lemma}
\begin{proof}
$$\lim_{y\rightarrow c} f(y)-f(c) = f'(c)\lim_{y\rightarrow c}(y-c) = 0.$$	
\end{proof}
 

\begin{definition}[continuously differentiable function]\index{continuously differentiable function}
A function $f$ is continuously differentiable at $x$ if
$$\lim_{y\to x}\frac{f(y)-f(x)}{y-x} = f'(x) = \lim_{y\to x} f'(y)$$
that is, the derivative $f'$ is continuous at $x$.
\end{definition}


\begin{mdframed}
\textbf{Caution!}\\
A differentiable function $f$ on the real numbers need not be a continuously differentiable function, that is, $f'$ is not continuous.
Consider the function:
$$g(x) = 
\begin{cases}
x^2\sin(1/x),x\neq 0\\
0,x=0
\end{cases}$$
Then
$$g'(x) = 
\begin{cases}
2x\sin(1/x)-\cos(1/x),x\neq 0\\
0,x=0
\end{cases}$$
In particular, $g'(0) = 0$, but $\lim_{x\to 0}g'(x)$ does not exist. Note that the $g'(0)$ can be calculated using the definition of derivative.
\end{mdframed}




\subsection{Differential rules}

\begin{lemma}[algebraic rules of differentiation]\cite[91]{fitzpatrick2006advanced}
Let $I$ be a neighborhood of $x_0$ and suppose that the function $f:I\to \R$ and $g:I\to \R$ are differentiable at $x_0$. Then
\begin{itemize}
	\item the sum $f+g:I\to \R$ is differentiable at $x_0$ and
	$$(f+g)'(x_0) = f'(x_0) + g'(x_0).$$
	\item the product $fg:I\to \R$ is differentiable at $x_0$ and 
	$$(fg)'(x_0) = f(x_0)g'(x_0) + f'(x_0)g(x_0).$$
	\item If $g(x)\neq 0 \forall x\in I$, then the reciprocal $1/g:I\to \R$ is differentiable at $x_0$ and
	$$(\frac{1}{g})'(x_0) = \frac{-g'(x_0)}{(g(x_0))^2}.$$  
\end{itemize}	
\end{lemma}
\begin{proof}
(1)
$$\lim_{x\to x_0}\frac{(f+g)(x)-(f+g)(x_0)}{x-x_0}=\lim_{x\to x_0}\frac{f(x)-f(x_0)}{x-x_0} + \frac{g(x)-g(x_0)}{x-x_0} = f'(x_0) + g'(x_0).$$
(2)
\begin{align*}
& \frac{f(x)g(x) - f(x_0)g(x_0)}{x-x_0} \\
=& \frac{f(x)g(x) - f(x)g(x_0) + f(x)g(x_0) - f(x_0)g(x_0)}{x-x_0} \\
=& f(x)\frac{g(x) - g(x_0) }{x-x_0} + g(x_0)\frac{ f(x) - f(x_0)}{x-x_0} 
\end{align*}	
Take limits and use the algebraic properties of  limits\autoref{ch:sequences-series:th:algebraOfLimits}.
(3) Note that
\begin{align*}
& \frac{1/g(x) - 1/g(x_0)}{x-x_0} \\
=& -\frac{1}{g(x)g(x_0)}\frac{g(x) - g(x_0)}{x-x_0} 
\end{align*}
Take limits and use the algebraic properties of  limits \autoref{ch:sequences-series:th:algebraOfLimits}.
\end{proof}


\begin{lemma}[derivative of the composition, chain rule]\cite[99]{fitzpatrick2006advanced}
	Let $I$ be a neighborhood of $x_0$ and suppose that the function $f:I\to \R$ and $g:I\to \R$ are differentiable at $x_0$. Let $J$ be an open interval such that $f(I)\subseteq J$ and suppose that the function $g:J\to \R$ is differentiable at $f(x_0)$. Then
the composition $g\circ f:I\to \R$ is differentiable at $x_0$ and
		$$(g\circ f)'(x_0) = g'(f(x_0))f'(x_0).$$	
\end{lemma}
\begin{proof}(informal)
Define $y_0 = f(x_0), y = f(x)$. And we have

$$\frac{(g\circ f)(x) - (g\circ f)(x_0)}{x-x_0} = \frac{(g(y) - g(y_0)}{x-x_0} = \frac{(g(y) - g(y_0)}{y-y_0}\frac{f(x)-f(x_0)}{x-x_0},$$
where we use the fact that $$\frac{f(y)-f(y_0)}{x-x_0} = 1.$$
	
\end{proof}


\begin{lemma}[derivative of the inverse]\cite[97]{fitzpatrick2006advanced}
	Let $I$ be a neighborhood of $x_0$ and suppose that the function $f:I\to \R$ be strictly monotone and continuous. 
	Suppose that $f$ is differentiable at $x_0$ and that $f'(x_0)\neq 0$. 

	\begin{itemize}
		\item Define $J = f(I)$. The inverse $f^{-1}:J\to \R$ is differentiable at $y_0 = f(x_0)$. 
		\item
		$$(f^{-1})'(y_0) = \frac{1}{f'(x_0)}.$$
	\end{itemize}	
\end{lemma}
\begin{proof}
Take $y_0 = f(x_0)$. For $y\in J, y\neq y_0$, define $x = f^{-1}(y)$. 

Note that
$$\frac{f^{-1}(y) - f^{-1}(y_0)}{y-y_0} = \frac{x - x_0}{f(x)-f(x_0)} = \frac{1}{\frac{f(x)-f(x_0)}{x - x_0}}.$$

Use the fact that $f^{-1}$ is continuous(\autoref{ch:calculus:th:continuityOfInversefunctions}): when $y\to y_0$, $x\to x_0$. Then we can take the limits of the above and use the algebraic properties of limits(\autoref{ch:sequences-series:th:algebraOfLimits}).	
\end{proof}




\begin{lemma}[leibniz rule]
Let $u,v$ be n-times differentiable functions, then the product is also n-times differentiable and its nth derivative is given by
$$(uv)^{n} = \sum_{k=0}^n \binom{n}{k}u^{(k)}v^{(n-k)}$$
\end{lemma}
\begin{proof}
This can be proof directly using Pascal's triangle.
\end{proof}



\subsection{Mean value theorem}



\begin{lemma}[derivative characterization of extreme values]\cite[103]{fitzpatrick2006advanced}\label{ch:calculus:th:derivativeCharacterizationExtrameValues}
Let $I$ be a neighborhood of $x_0$ and suppose that the function $f:I\to \R$ is differentiable at $x_0$. If the point $x_0$ is either a maximizer or a minimizer of the function $f$, then $f'(x_0) = 0$.	
\end{lemma}
\begin{proof}
Let $x_0$ be the point $f(x_0)$ takes the maximum value。	
By the definitions of derivative and limits, we have
$$f'(x) = \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} = \lim_{x\to x_0, x<x_0} \frac{f(x)-f(x_0)}{x-x_0} = \lim_{x\to x_0,x>x_0} \frac{f(x)-f(x_0)}{x-x_0}.$$

From 
$$f(x_0) = \lim_{x\to x_0, x<x_0} \frac{f(x)-f(x_0)}{x-x_0} \geq 0,$$
and
$$f(x_0) = \lim_{x\to x_0, x>x_0} \frac{f(x)-f(x_0)}{x-x_0} \leq 0.$$

Therefore, $f(x_0)\leq 0, f(x_0)\geq 0 \implies f(x_0) = 0.$


\end{proof}

\begin{theorem}[mean-value theorem]\index{mean-value theorem}\cite{johnsonbaugh2010foundations}\cite[103]{fitzpatrick2006advanced}\label{ch:calculus:th:meanValueTheorem}\hfill
\begin{itemize}
	\item \textbf{(Rolle's Theorem)} Suppose that the function $f:[a,b]\to \R$ is continuous the that the restriction of $f$ to the open interval $(a,b)$ is differentiable. Assume that 
	$$f(a) = f(b),$$
	then there exists a point $x_0$ in the open interval $(a,b)$ at which $f'(x_0) = 0.$
	\item If $f$ and $g$ are continuous real functions on $[a,b]$, and differentiable on $(a,b)$, then there is a point $c\in(a,b)$ such that 
	$$[f(b)-f(a)]g'(c) = [g(b)-b(a)]f'(c);$$
	if $g'(x)\neq 0 \forall x\in (a,b)$, we can write as
	$$\frac{f(b)-f(a)}{g(b)-g(a)} = \frac{f'(c)}{g'(c)}.$$
	
	
	\item If $f$ is real-valued and continuous on $[a,b]$ and differentiable in $(a,b)$, then there exists a point $c\in (a,b)$ such that
	$$f(b)-f(a) = f(c)'(b-a)$$
\end{itemize}	
\end{theorem}
\begin{proof}
(1) If the maximum value and minimum value occur at the end point, then $f(x)$ is a constant and therefore $f'(x) = 0$. If maximum value or minimum value occur inside the interval, then $f'=0$ at which $f$ takes extreme values(\autoref{ch:calculus:th:derivativeCharacterizationExtrameValues}).	
(2)Construct $h(x) = [f(b)-f(a)]g(x) - [g(b)-b(a)]f(x)$ and $h(a)=h(b)$ implies the existence of extreme value at $(a,b)$ based on extreme value theorem, then $f' = 0$ at this extreme point.	
(3)using above theorem and set $g(x) = x$.	
\end{proof}


\begin{lemma}[The identity criterion for differentiable functions]\cite[104]{fitzpatrick2006advanced}\label{ch:calculus:th:IdentityCriterionForDifferentiableFunctions}\hfill
\begin{itemize}
	\item Let $I$ be an open interval and suppose that the function $f:I\to \R$ is differentiable. Then $f:I\to \R$ is constant if and only if
	$$f'(x) = 0, \forall x\in I.$$
	\item Let $I$ be an open interval and suppose that the function $g:I\to \R$ and $h:I\to \R$ are differentiable. 
	Then $g$ and $h$ differ by a constant if and only if 
	$$g'(x) = h'(x), \forall x\in I.$$
	In particular, $g= h, \forall x$ if and only if
	$$g'(x) = h'(x), \forall x\in I,$$
	and there exists some point $x_0\in I$ at which
	$$g(x_0) = h(x_0).$$
\end{itemize}	
\end{lemma}
\begin{proof}
(1)	
Let $c = f(x_0)$, where $x_0$ is an arbitrary number in $I$. Then any $x\neq x_0, x\in I$, we have(using mean value theorem \autoref{ch:calculus:th:meanValueTheorem})
$$f'(c) = \frac{f(x)-f(x_0)}{x-x_0} =0 \implies f(x)= f(x_0) =c,$$
where $c$ is a number between $x_0$ and $x$.
(2) Let $f = g - h$ and $f(x_0) = c.$ And then use (1).
\end{proof}


 
\subsection{L'Hospital's Rule}
\begin{theorem}[L'Hospital's Rule]\index{L'Hospital's Rule}
Assume $f$ and $g$ are differentiable on an open interval $I$ except possibly at a point $c\in I$. If 
$$\lim_{x\to c} f(x) = \lim_{x\to c} g(x) = 0 ~or~ \pm \infty$$
and
$$g'(x)\neq 0, \forall x\in I\\c$$
and
$$\lim_{x\to c} \frac{f'(x)}{g'(x)} ~exists,$$
then
$$\lim_{x\to c}\frac{f(x)}{g(x)} = \lim_{x\to c} \frac{f'(x)}{g'(x)}.$$		
\end{theorem}

\begin{example}
	$$\lim_{x\to \infty} \frac{x^n}{e^x} = \lim_{x\to \infty} \frac{nx^{n-1}}{e^x} = ... = \lim_{x\to \infty} \frac{n!}{e^x} = 0$$
\end{example}

\section{Implicit function theorem}
\begin{theorem}\index{implicit function theorem}
\cite[112]{kaplan1973advanced}\cite[514]{luenberger2015linear} For $i=1,..., m$, let the functions $F_i (y_1,...,y_m,x_1,...,x_n)$ all be defined in a neighborhood of the point $(y_1^0,...,y_m^0,x_1^0,...,x_n^0)$ and the $m\times m$ Jacobian matrix $J$ of the first $m$ variables, i.e. $J_{ij} = \Pa F_i / \Pa y_j$ is non-singular, then we have
\begin{itemize}
    \item There exist a neighborhood around $(x_1,...,x_n)$ such that there are differentiable functions $\phi_i(x_1,x_2,...,x_n),i=1,2...,m$ such that $y_i^0 = \phi_i(x_1^0,x_2^0,...,x_n^0),i=1,...,m$
    \item $$F_i(\phi_1,\phi_2,...,\phi_m,x_1,x_2,...,x_n) = 0$$ at $$(y_1^0,...,y_m^0,x_1^0,...,x_n^0)$$
\end{itemize}
\end{theorem}


\begin{remark}[interpretation]
The implicit function theorem gives the condition such that: given $m$ functions $h_i:\R^n\to \R$, we are able to 'solve' $m$ unknowns in terms of the rest of $n-m$ variables.
\end{remark}

\begin{example}
Consider $Ax=b,A\in\R^{m\times n},m>n,rank(A)=m$, suppose we can partition $A = [B,C]$, where $B$ is the $m \times m$ non-singular matrix, then we can solve $m$ variables via $x_{1:m} = B^{-1}(b-Cx_{m+1:n})$.
\end{example}


\section{Inverse function theorem}\index{inverse function theorem}
\begin{theorem}[inverse function theorem on $\R^n$]\cite{spivak}
Suppose that $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ is continuously differentiable in an open set containing $a$, and $\det f'(a) \neq 0$. Then there exist an open set $V$ containing $a$ and an open set $W$ containing $f(a)$ such that $f:V\rightarrow W$ has a continuous inverse $f^{-1}: W\rightarrow V$ which is differentiable and for all  $y \in W$ satisfies $$(f^{-1})'(y) = [f'(f^{-1}(y))]^{-1}$$
Note that $f'$ is the Jacobian. Here we can write more compactly
$$\frac{dy}{dx} \frac{dx}{dy} = I$$
where $y(x): \R^n\rightarrow\R^n$ and $x(y)$ is the inverse function.
\end{theorem}

\begin{remark}[relations between Jacobians]\cite[119]{kaplan1973advanced}\index{Jacobian}
	The Jacobian matrix of the inverse mapping is simply the inverse of the Jacobian matirx of the mapping. 
\end{remark}

\begin{corollary}[inverse function theorem on $\R$]\cite{spivak}
Suppose that $f:\mathbb{R} \rightarrow \mathbb{R}$ is continuously differentiable in an open set containing $a$ and $f'(a) \neq 0$. Then there is an open interval $V$ containing $a$ such that $f'(x) > or < 0$, i.e., $f$ is increasing or decreasing on $V$, and is therefore 1-1 with an inverse function $f^{-1}$ defined on some open interval $W$ containing $f(a)$. The derivative of the inverse function is\cite{spivak} $$(f^{-1})'(y)=\frac{1}{f'(x)}=\frac{1}{f'(f^{-1}(y))}$$
More more compactely
$$\frac{dx}{dy}|_y = (1/\frac{dy}{dx})|_{x = f^{-1}(y)} $$
\end{corollary}





\section{Riemann Integral}
\subsection{Construction of Riemann integral}\index{Riemann integral}
\begin{definition}[partition, upeer sums, and lower sums]\cite[218]{abbott2001understanding}\hfill
	\begin{itemize}
		\item Let $[a,b]$ be a closed interval. A \textbf{partition} $P$ of $[a,b]$ is a finite set $\{x_0,x_1,...,x_n\}$ such that 
		$$a=x_0 < x_1 ... < x_n =b.$$
		\item Let $f$ be a bounded function on $[a,b]$, let $\alpha$ be an \textbf{increasing} function on $[a,b]$, and let $P$ be the partition. The \textbf{upper sum} of $f$ with respect to $\alpha$ for the partition $P$ is
		$$U(f,P)=\sum_{i=1}^n M_i (x_i-x_{i-1}),M_i=\inf\{f(x)|x\in[x_{i-1},x_i]\}.$$
		\item  The \textbf{lower sum} is
		$$L(f,P)=\sum_{i=1}^n m_i (x_i-x_{i-1}),m_i=\sup\{f(x)|x\in[x_{i-1},x_i]\}.$$ 
	\end{itemize}	
\end{definition}

\begin{definition}
A partition $Q$ is a \textbf{refinement} of a partition $P$ if $Q$ contains all of the points $P$: that is $P\subseteq Q$.	
\end{definition}

\begin{definition}[upper integral, lower integral]\cite[220]{abbott2001understanding}
Let $\cP$ be the collection of all possible partitions of the interval $[a,b]$. The \textbf{upper integral} of $f$ is defined to be
$$U(f) = \inf\{U(f,\cP),P\in \cP\}.$$

Similarly, the \textbf{lower integral} of $f$ is defined to be
$$L(f) = \inf\{L(f,\cP),P\in \cP\}.$$


\end{definition}


\begin{lemma}[inequality of lower and upper sums]\cite[220]{abbott2001understanding}\hfill
\begin{itemize}
	\item 	If $P\subseteq Q$, then $L(f,P)\leq L(f,Q)$, and $U(f,P)\geq U(f,Q)$.
	\item For any bounded function $f$ on $[a,b]$, it is always the case that $U(f)\geq L(f)$.
\end{itemize}	
\end{lemma}
\begin{proof}

\end{proof}




\begin{definition}[Riemann integrability]
A bounded function $f$ defined on the interval $[a,b]$ is \textbf{Riemann integrable} if $U(f) = L(f)$ in this case we define $$\int_a^b f(x) = U(f) = L(f).$$	
\end{definition}


\begin{theorem}[integrability criterion]\cite[221]{abbott2001understanding}
A bounded function $f$ is integrable on $[a,b]$ if and only if for every $\epsilon > 0$, there exists a partition $P_\epsilon$ of $[a,b]$ such that
$$U(f,P_\epsilon) - L(f,P_\epsilon) < \epsilon.$$	
\end{theorem}



\subsection{Riemann integrability}
\subsubsection{Basics}

\begin{theorem}[sequential criterion for integrability]\cite[223]{abbott2001understanding}
A \textbf{bounded} function	$f$ is integrable on $[a,b]$ if and only if there exists a sequence of partitions $\{P_n\}_{n=1}^\infty$ satisfying
$$\lim_{n\to\infty} [U(f,P_n) - L(f,P_n)] = 0,$$
and in this case 
$$\int_a^b f dx = \lim_{n\to\infty} U(f,P_n)  = \lim_{n\to\infty}L(f,P_n).$$
\end{theorem}




\begin{remark}[how to use]\cite[223]{abbott2001understanding}
We can let $P_n$ be the partition of $[a,b]$ into $n$ equal subintervals.	
\end{remark}



\begin{lemma}[endpoints and integrability]\cite[224]{abbott2001understanding} If $f:[a,b]\to\R$ is \textbf{bounded} and $f$ is \textbf{integrable} on $[c,b]$ for all $c\in(a,b)$, then $f$ is integrable on $[a,b]$. An analogous result holds at the other endpoints.
\end{lemma}


\begin{lemma}[continuous function on closed interval implies integrability]\cite{johnsonbaugh2010foundations}\cite[156]{fitzpatrick2006advanced}\cite[222]{abbott2001understanding}
	If $f$ is continuous at interval $[a,b]$, then $f$ is Riemann integrable. 
\end{lemma}
\begin{proof}
	Because $f$ is continuous on a closed interval,then it is \textbf{uniformly continuous}, given a $\epsilon$, we can find fine enough partition such that the upper bound and lower upper in each partition component is small enough, such that $U(f,T)-L(f,T)<\epsilon$.	
\end{proof}


\begin{lemma}[Integrability implies boundedness, necessary condition]\cite{mathanalysischinese} If function $f$ is integrable at interval $[a,b]$, then $f$ is bounded.

In other words, \textbf{if a function is unbounded, then it is not integrable.}
\end{lemma}
\begin{proof}
	Note that this is not \textbf{Riemann-Stieltjes} integral. To prove: suppose it is not bounded, then not matter how finer the partition will be, then there exist a component in the partition,say $\Delta_k$, in which the function $f$ is unbounded. For any number $M$, there exist a $x^*\in \Delta_k$, such that $f(x^*)\Delta_k > M$, and this make the Riemann sum $\sum f(\eta_i)\Delta_i$ always greater than any given large number.	
\end{proof}


\subsubsection{Lebesgue characterization of integrability}

\begin{definition}[zero measure set]\cite{spivak}\index{zero measure set}
	A subset $A$ of $\mathbb{R}^n$ has \textbf{measure 0} if for every $\epsilon > 0 $ there is a cover $\{U_1,U_2,U_3,...\}$ of $A$ by closed rectangles such that $\sum_{i=0}^{\infty}v(U_i) < \epsilon$.	
\end{definition}

\begin{remark}\hfill
	\begin{itemize}
		\item A set with only finitely many points clearly has measure 0.
		\item  If $A$ has infinitely countable of points that arranged in sequence $a_1,a_2, a_3,...$, then $A$ also has measure 0, since we can use choose $U_i$ to be a closed rectangle containing $a_i$ with $v(U_i) < \epsilon/2^i$. Then $\sum_{i=0}^{\infty}\epsilon/2^i = \epsilon$.
		\item If $A = A_1\cup A_2 \cup A_3 ...$ and each $A_i$ has measure 0, then $A$ has measure 0.
	\end{itemize}
\end{remark}

\begin{theorem}[bounded function with zero-measure discontinuous points implies integrability, Lebesgue's theorem]\cite[242]{abbott2001understanding}
Let	$f$ be a \textbf{bounded} function defined on the interval $[a,b]$. Then $f$ is Riemann-integrable if and only if the sets of points where $f$ is not continuous has measure zero.
\end{theorem}
\begin{proof}
	by properly partition,i.e., let partition endpoint aligned with discontinuity. 
\end{proof}


\subsubsection{limits and integrability}





\begin{note}[limits and Riemann integrability]\cite[1]{cheng2008crash}
In general, the limit of Riemann integrable functions is not necessarily Riemann-integrable. 

For example, let $\{x_n\}$ be the sequence of ordered rational number between 0 and 1.  Then the function 
$$D_n = \sum_{k=1}^n E_n(x)dx, E_n(x) = \begin{cases*}
1, x = x_n\\
0, x \neq x_n
\end{cases*}$$ 
Consider the integral
$$\int_0^1 D_n(x)dx.$$
\begin{itemize}
	\item If $n$ is any finite number, then $\int_0^1 D_n(x)dx = 0$ is Riemann integrable.
	\item As $n\to \infty$, $\int_0^1 D_\infty(x)dx$ is not Riemann integrable. Note that $D_\infty(x)$ is equivalent to the Dirichlet function defined by
	$$D(x) = \begin{cases*}
	1, x\in \Q\\
	0, x \in \R / \Q
	\end{cases*}.$$
	  Because no matter how fine the partition is, in each interval there will be a rational number and an irrational number since they are dense(\autoref{ch:sets:th:rationalIrrationalNumbersDense}); therefore the upper sum will be 1 and the lower sum will be 0 for all possible partitions.
	\item That is, in general
	$$\lim_{n\to \infty} \int_X f_n(x)dx \neq \int_X \lim_{n\to \infty}f_n(x)dx.$$
\end{itemize}	
	
\end{note}

\subsubsection{Algebraic properties}

\begin{theorem}[algebraic properties of Riemann integral]\cite[229]{abbott2001understanding} Assume $f$ and $g$ are integrable functions on the interval $[a,b]$. It follows that
	\begin{itemize}
		\item The function $f + g$ are integrable on $[a,b]$ with 
		$$\int_a^b f+g = \int_a^b f + \int_a^b g.$$
		\item For $k\in \R$, the function $kf$ is integrable with 
		$$\int_a^b kf = k \int_a^b f.$$
		\item If $m\leq f(x)\leq M$ on $[a,b]$, then
		$$m(b-a) \leq \int_a^b f \leq M(b-a).$$
		\item If $ f(x)\leq g(x)$ on $[a,b]$, then
		$$\int_a^b f \leq \int_a^b g.$$
		\item The function $\abs{f}$ is integrable and
		$$\abs{\int_a^b f} \leq \int_a^b \abs{f}.$$
	\end{itemize}
\end{theorem}



\subsection{First Fundamental Theorem of Calculus}
\begin{theorem}[First Fundamental Theorem of Calculus, integrating derivatives]\index{First Fundamental Theorem of Calculus}\cite[161]{fitzpatrick2006advanced}\label{ch:calculus:th:FirstFundamentalTheoremOfCalculus}Let the function $F:[a,b]\to \R$ be continuous on the closed interval $[a,b]$ and be differentiable on the open interval $(a,b)$. Moreover, suppose that its derivative $F':(a,b)\to \R$ is both continuous and bounded. 
	Then
	$$\int_a^b F'(x)dx = F(b) - F(a).$$
\end{theorem}


\begin{remark}[When not to use]\cite[163]{fitzpatrick2006advanced}
Define
$$f(x)=\begin{cases*}
4, if~ 2\leq x\leq 3\\
0, if~ 3\leq x \leq 6
\end{cases*}.$$
The function $f$ is integrable; however, we cannot apply First Fundamental Theorem to calculate the integral $\int_2^4 f(x)dx$ because $f(x)$ is not continuous on $[2,4]$. Note that we can use it to calculate the integral on the continuous segment.
\end{remark}

\subsection{Second Fundamental Theorem of Calculus}
\subsubsection{Fundamentals}
\begin{lemma}[Continuity and differentiability of integral functions]\cite[169]{fitzpatrick2006advanced}\label{ch:calculus:th:conuityOfIntegralFunction}
Suppose that the function $f:[a,b]\to \R$ is integrable. Define
$$F(x) = \int_a^x f(t)dt, \forall x\in [a,b].$$
Then 
\begin{itemize}
	\item the function $F:[a,b]\to \R$ is continuous.
	\item $F$ is differentiable.
\end{itemize}

\end{lemma}
\begin{proof}
(1)Let $u,v\in [a,b], u<v$. Then
$$F(v) = F(u) + \int_u^v f(t) dt,$$
such that
$$F(v)-F(u) = \int_u^v f(t)dt.$$
Because $f(x)$ is continuous on $[a,b]$, then it has a minimum value $f(x_m) = m$ and a maximum value $f(x_M) = M$(\autoref{ch:calculus:th:boundednesscontinuousfunctiononclosedinterval},\autoref{ch:calculus:th:extremevaluetheoremfunctiononclosedinterval}). We have
$$m(v-u) \leq \int_u^v f(x) dx  \leq M(v-u),$$
Let $K = \max(\abs{m},\abs{M})$, then
$$-K(v-u) \leq \int_u^v f(x) dx  \leq K(v-u);$$
therefore
$$-K(v-u) \leq F(v) - F(u)  \leq K(v-u) \implies \abs{F(v)-F(u)}\leq K\abs{v-u}.$$
When $v\to u$, we have $F(v)\to F(u);$that is, $F(x)$ is continuous.
(2) Directly from second fundamental theorem of calculus.
\end{proof}


\begin{theorem}[second fundamental theorem of calculus, differentiate integral]\cite[168]{fitzpatrick2006advanced}\label{ch:calculus:th:SecondFundamentalTheoremOfCalculus}
Suppose that the function $f:[a,b]\to \R$ is continuous. Then
$$\frac{d}{dx}[\int_a^x f(t)dt] = f(x), \forall x\in (a,b).$$	
	If $F$ is defined as
	$$F(t) = \int_a^t f(x)dx$$
	where $f$ is continuous on an open interval $I$, and $a$ is a point inside the interval, 
	then
	$$F'(t) = f(t)$$
\end{theorem}
\begin{proof}
Define
$$F(x) \triangleq \int_a^x f(t)dt, \forall x\in [a,b].$$
Then from \autoref{ch:calculus:th:conuityOfIntegralFunction}, we know that $F(x)$ is continuous. 

Let $x_0\in (a,b)$, we have
$$\int_{x_0}^x f(t)dt = F(x) - F(x_0) = f(c(x))(x-x_0),$$
where $c(x)$ is between $x_0$ and $x$, and we use mean value theorem for integrals(\autoref{ch:calculus:th:meanValueTheoremForIntegral}).
Therefore,

$$F'(x_0) = \lim_{x\to x_0}\frac{F(x)-F(x_0)}{x - x_0} = \lim_{x\to x_0} f(c(x)) = f(x_0),$$
where we use the fact that $f(x)$ is continuous.
\end{proof}

\begin{lemma}[basic generalization of second fundamental theorem]\cite[170]{fitzpatrick2006advanced}\hfill
\begin{itemize}
	\item Suppose that the function $f:[a,b]\to \R$ is continuous. Then
	$$\frac{d}{dx}\int_x^b f(t)dt = -f(x), \forall x\in (a,b).$$
	\item Let $I$ be an open interval and suppose that the function $f:I\to \R$ is continuous. Fix a point $x_0$ in $I$. Then
	$$\frac{d}{dx}[\int_{x_0}^{x} f(t)dt] = f(x), \forall x\in I.$$
\end{itemize}	
\end{lemma}
\begin{proof}
(1) Note that
$$\int_a^b f(t)dt = \int_a^x f(t)dt + \int_x^b f(t)dt.$$
Differentiating both sides and we will get the result.
(2)	Let $[x_0,b]$ be the closed interval such that
$[x_0,b] \subset I, x\in [x_0,b]$. Then we use second fundamental theorem \autoref{ch:calculus:th:SecondFundamentalTheoremOfCalculus}.
\end{proof}
\subsubsection{Differentiating definite integrals}
\begin{lemma}\cite[171]{fitzpatrick2006advanced}
Let $I$ be an open interval on $\R$ and suppose that the function $f:I\to \R$	is continuous. Let $J$ be an open interval on $\R$ and suppose that the function $\psi:J\to \R$	is differentiable and that $\psi(J)\subseteq I$. Fix a point $x_0$ in $I$. Then	
\begin{itemize}
	\item 
	$$\frac{d}{dx}[\int_{x_0}^{\psi(x)} f(t)dt] = f(\psi(x))\psi'(x), \forall x\in J.$$ 
	\item
	$$\frac{d}{dx}[\int^{x_0}_{\psi(x)} f(t)dt] = -f(\psi(x))\psi'(x), \forall x\in J.$$ 
	\item  Let $K$ be an open interval on $\R$ and suppose that the function $\psi:K\to \R$	is differentiable and that $\phi(K)\subseteq I$. Further assume $\psi(x)$ Then
		$$\frac{d}{dx}[\int_{\phi(x)}^{\psi(x)} f(t)dt] = f(\psi(x))\psi'(x)-f(\phi(x))\phi'(x), \forall x\in J.$$ 
\end{itemize}	
\end{lemma}
\begin{proof}
(1)	
Define 
$$G(x) = \int_{x_0}^{\psi(x)} f(t)dt,$$
and
$$F(x) = \int_{x_0}^{x} f(t)dt.$$
Then
$$G = F\circ \psi,$$
and
$$G'(x) = F'(\psi(x))\psi'(x) = f(\psi(x))\psi'(x).$$
(2)
Define 
$$G(x) = \int^{x_0}_{\psi(x)} f(t)dt,$$
and
$$F(x) = \int^{x_0}_{x} f(t)dt.$$
Then
$$G = F\circ \psi,$$
and
$$G'(x) = F'(\psi(x))\psi'(x) = -f(\psi(x))\psi'(x).$$
(3)
Let $\phi(x) \leq x_0 \leq \psi(x)$. Then
	$$\int_{\phi(x)}^{\psi(x)} f(t)dt = \int_{\phi(x)}^{x_0} f(t)dt+\int_{\phi(x)}^{x_0} f(t)dt.$$
Then we use (1) and (2).	
\end{proof}

\begin{lemma}[exchange derivative and integral]
	Let $f:\mathbb{R}\times \mathbb{R}\rightarrow \mathbb{R}$ be a continuous function such that partial derivative $\frac{\partial f}{\partial t}(x,t)$ exists and is continuous in both $x,t$. Then 
	$$\frac{d}{dt}(\int_{a}^{b}f(x,t)dx) = (\int_{a}^{b}\frac{d}{dt}f(x,t)dx)$$
\end{lemma}
\begin{proof}
	see appendix for exchange of derivative and integral.
\end{proof}


\begin{corollary}
	Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be a continuous function such that partial derivative $\frac{\partial f}{\partial t}(x,t)$ exists and is continuous in both $x,t$.	
	Then 
	$$\frac{d}{dt}(\int_{a(t)}^{b(t)}f(x,t)dx) = f(b(t),t)b'(t)-f(a(t),t)a'(t)+ (\int_{a}^{b}\frac{d}{dt}f(x,t)dx)$$
\end{corollary}




\subsubsection{Application to differential equation}
\begin{lemma}Let $I$ be an open interval containing the point $x_0$ and suppose that the function $f:I\to \R$ is continuous. For any number $y_0$, the differential equation
	$$F'(x) = f(x), \forall x\in I; F(x_0) = y_0;$$
has a unique solution $F:I\to \R$ given by the formula
$$F(x) = y_0 + \int_{x_0}^x f(t)dt, \forall x\in I.$$
\end{lemma}
\begin{proof}
From the definition of $F$, we have $F(x_0) = y$. From the second fundamental theorem\autoref{ch:calculus:th:SecondFundamentalTheoremOfCalculus}, we have
$$F'(x) = [y_0 + \int_{x_0}^x f(t)dt]' = f(x).$$

To prove uniqueness, suppose there is another function $G(x)$ satisfying $G'= F'$ and $G(x_0) = F(x_0)$. Then from the identity criterion \autoref{ch:calculus:th:IdentityCriterionForDifferentiableFunctions}, $F$ and $G$ must be the same function.
\end{proof}




\subsection{Essential theorems}
\begin{theorem}[The mean value theorem for integrals]\cite[166]{fitzpatrick2006advanced}\label{ch:calculus:th:meanValueTheoremForIntegral}
Suppose that the function $f:[a,b]\to \R$ is continuous. Then there is a point $x_0$ in the interval $[a,b]$ such that 
$$f(x_0) = \frac{1}{b-a}\int_a^b f(x) dx = f(x_0).$$	
\end{theorem}
\begin{proof}
Since $f$ is continuous on the closed interval, then it is integrable. Also, since $f(x)$ is continuous on $[a,b]$, then it has a minimum value $f(x_m) = m$ and a maximum value $f(x_M) = M$(\autoref{ch:calculus:th:boundednesscontinuousfunctiononclosedinterval},\autoref{ch:calculus:th:extremevaluetheoremfunctiononclosedinterval}). We have
$$f(x_m)(b-a) \leq \int_a^b f(x) dx  \leq f(x_M)(b-a),$$
or equivalently,
$$f(x_m) \leq \frac{1}{b-a}\int_a^b f(x) dx  \leq f(x_M),$$
From intermediate value theorem(\autoref{ch:calculus:th:intermediatevaluetheoremonrealline}), there exists an $x_0 \in[x_m,x_M]$ such that 
$$f(x_0) = \frac{1}{b-a}\int_a^b f(x) dx = f(x_0).$$




\end{proof}



\begin{lemma}[integral mean value theorem, generalization]
Let $w(x)$ be nonnegative and integrable on $[a,b]$, and let $f(x)$ be continuous on $[a,b]$, then 
$$\int_a^b w(x)f(x) dx = f(\xi)\int_a^b w(x)dx,$$
for some $\xi \in [a,b]$. 
\end{lemma}
\begin{proof}
Because $f(x)$ is continuous on a closed interval, then it has a minimum value $m$ and a maximum value $M$(\autoref{ch:calculus:th:boundednesscontinuousfunctiononclosedinterval},\autoref{ch:calculus:th:extremevaluetheoremfunctiononclosedinterval}). Then
$$m\int_a^b w(x)dx \leq \int_a^b w(x)f(x) dx \leq M \int_a^b w(x)dx. $$
Now let
$I = \int_a^b w(x)dx.$ If $I = 0$, then we are done since
$$\int_a^b f(x)w(x) dx = f(c)I = 0,$$
where $c$ is an arbitrary number in $[a,b]$.	
If $I \neq 0$, we have 

$$m \leq \frac{1}{I}\int_a^b f(x)w(x)dx \leq M.$$
From intermediate value theorem(\autoref{ch:calculus:th:intermediatevaluetheoremonrealline}), there exists an $\xi \in[a,b]$ such that 
$$f(\xi) = \frac{1}{I}\int_a^b f(x)w(x)dx.$$
\end{proof}

\subsection{Integration rules}
\begin{lemma}[integration by substitution]\cite[179]{fitzpatrick2006advanced}\label{ch:calculus:th:SingleVariableIntegralIntegrationBySubstitution}
Let the function $f:[d,c] \to \R$ be continuous. Suppose function $g$ has a bounded continuous derivative and has inverse $g^{-1}$. Then	
	$$\int_c^d f(x)dx = \int_{g^{-1}(c)}^{g^{-1}(d)} f(g(y)) g'(y) dy$$
\end{lemma}
\begin{proof}
Define $$H(t) = \int_c^t f(x)dx - \int_{g^{-1}(c)}^{g^{-1}(t)} f(g(y)) g'(y) dy.$$
Then
\begin{align*}
H'(t) &= f(t) - f(g(g^{-1}(t)))g'(g^{-1}(t))(g^{-1}(t))' \\
&= f(t) - f(t)g'(g^{-1}(t))(1/g'(g^{-1}(t))) \\
&= 0
\end{align*}
Note that $H(c) = 0$, therefore $H(d) = H(c) = 0$ based on the identity criterion(\autoref{ch:calculus:th:IdentityCriterionForDifferentiableFunctions}).
\end{proof}

\begin{remark}
When we substitute $x$ by $g(y)$(we can do the substitution since $x$ is dummy variable), the integral domain is changed from $[c,d]$ to $g^{-1}([c,d])$.	
\end{remark}

\subsection{Improper Riemann integrals}\index{improper integral}


\begin{remark}
	There are two types of improper integrals: infinite interval and discontinuous integrand.	
\end{remark}

\begin{definition}[improper integral with infinite integration limits]\cite[580]{larson2009calculus}\hfill
	\begin{itemize}
		\item Given a $f(x)$ defined on $[a,+\infty]$, and $f$ is integrable on any finite interval $[a,u]$. Then
		$$\int_{a}^{\infty}f(x)dx \triangleq \lim_{u\to \infty}\int_a^u f(x)dx =J$$
		if the limit exists.
		\item Given a $f(x)$ defined on $(-\infty,b]$, and $f(x)$ is integrable on any finite interval $[u,b]$. Then
		$$\int_{-\infty}^{b}f(x)dx \triangleq \lim_{u\to -\infty}\int_u^b f(x)dx =J$$
		if the limit exists.
		\item Given a $f(x)$ defined on $(-\infty,\infty]$, and $f(x)$ is integrable on any finite interval $[a,b]$. Then
		$$\int_{-\infty}^{\infty}f(x)dx \triangleq \lim_{u\to -\infty}\int_u^c f(x)dx + \lim_{u\to \infty}\int_c^u f(x)dx$$
		if the both limits exist.
	\end{itemize}	
	If the limits do not exist, we say the improper integral diverge. 
\end{definition}

\begin{example}\cite[581]{larson2009calculus}\hfill
	\begin{itemize}
		\item (diverging integral)$$\int_1^\infty \frac{dx}{x} = \lim_{b\to \infty} \int_1^b \frac{dx}{x} = \lim_{b\to \infty}  \ln b = \infty.$$
		\item (convergent integral)
		$$\int_0^\infty \exp(-x)dx = \lim_{b\to \infty} \int_0^b \exp(-x)dx = \lim_{b\to \infty}  1 - \exp(-b) = 1.$$
	\end{itemize}	
\end{example}

\begin{definition}[improper integral with infinite discontinuity]\cite[583]{larson2009calculus}\hfill
	\begin{itemize}
		\item Consider a $f(x)$ define on $[a,b)$, and $f$ is unbounded in the neighborhood of $b$. Assume $f(x)$ is bounded and integrable on any finite interval $[a,u] \subseteq [a,b)$. Then
		$$\int_{a}^{b}f(x)dx \triangleq \lim_{c\to b^-} \int_a^c f(x)dx,$$
		if the limit exists.
		\item Consider a $f(x)$ define on $(a,b]$, and $f$ is unbounded in the neighborhood of $a$. Assume $f(x)$ is bounded and integrable on any finite interval $[u,b] \subseteq (a,b]$. Then
		$$\int_{a}^{b}f(x)dx \triangleq \lim_{c\to a^+} \int_c^b f(x)dx,$$
		if the limit exists.
		\item Consider a $f(x)$ define on $(a,b)$, and $f$ is unbounded in the neighborhood of $a$ and $b$. Assume $f(x)$ is bounded and integrable on any finite interval $[u,v] \subseteq (a,b)$. Then
		$$\int_{a}^{b}f(x)dx \triangleq \lim_{u\to b^-} \int_c^u f(x)dx + \lim_{u\to a^+} \int_u^c f(x)dx, c\in (a,b)$$
		if both limits exist.
	\end{itemize}	
	If the limits do not exist, we say the improper integral diverge. 
\end{definition}

\begin{example}\cite[584]{larson2009calculus}\hfill
	\begin{itemize}
		\item (diverging integral)$$\int_0^2 \frac{dx}{x^3} = \lim_{b\to 0^+} \int_b^2 \frac{dx}{x^3} = \lim_{b\to 0^+}  (-\frac{1}{8} + \frac{1}{2b^2}) = \infty.$$
		\item (convergent integral)
		$$\int_0^1 \frac{1}{\sqrt[3]{x}}dx = \lim_{b\to 0^+} \int_0^b \frac{1}{\sqrt[3]{x}}dx = \lim_{b\to 0^+}  \frac{3}{2}(1 - b^{2/3}) = \frac{3}{2}.$$
	\end{itemize}	
\end{example}



\section{Riemann-Stieltjes integral}\index{Riemann-Stieltjes integral}
\begin{definition}[partition, upeer sums, and lower sums]\cite[192]{johnsonbaugh2010foundations}\hfill
\begin{itemize}
	\item Let $[a,b]$ be a closed interval. A \textbf{partition} $P$ of $[a,b]$ is a finite set $\{x_0,x_1,...,x_n\}$ such that 
	$$a=x_0 < x_1 ... < x_n =b.$$
	\item Let $f$ be a bounded function on $[a,b]$, let $\alpha$ be an \textbf{increasing} function on $[a,b]$, and let $P$ be the partition. The \textbf{upper sum} of $f$ with respect to $\alpha$ for the partition $P$ is
	$$U(f,P)=\sum_{i=1}^n M_i (\alpha(x_i)-\alpha(x_{i-1}),M_i=lub\{f(x)|x\in[x_{i-1},x_i]\}.$$
	\item  The \textbf{lower sum} is
	$$L(f,P)=\sum_{i=1}^n m_i (\alpha(x_i)-\alpha(x_{i-1}),m_i=glb\{f(x)|x\in[x_{i-1},x_i]\}.$$ 
\end{itemize}	
Note that the increasing nature of $\alpha$ guarantee that $\alpha(x_i)-\alpha(x_{i-1})$ is nonnegative. 
\end{definition}




\begin{definition}
(Riemann-Stieltjes integrable) Let $\mathcal{U},\mathcal{L}$ denotes the set of upper, lower sums for $f$ with respect to $\alpha$ for all possible partitions. The upper Riemann-Stieltjes integral of $f$ with respect to $\alpha$ is the number
$$\bar{\int_a^b} f d\alpha = glb (\mathcal{U})$$
The lower Riemann-Stieltjes integral of $f$ with respect to $\alpha$ is the number
$$\underline{\int_a^b} f d\alpha = lub (\mathcal{L})$$
We say $f$ is Riemann-Stieltjes integrable with respect to $\alpha$ if 
$$\bar{\int_a^b} f d\alpha=\underline{\int_a^b} f d\alpha = \int_a^b f d\alpha$$
We denote the set of functions that are Riemann-Stieltjes integrable with respect to $\alpha$ on $[a,b]$ as $\mathcal{R}_{\alpha}[a,b]$
\end{definition}



\subsection{Riemann integral}
\begin{definition}[Riemann integral]
The Riemann integral of a continuous function $f:[a,b] \rightarrow \mathbb{R}$ is obtained as a limit of the form:
$$\int_a^bf(x)dx = \lim_{n\rightarrow \infty} \sum_{i=1}^n f(y_i(x_i,x_{i-1}))(x_i-x_{i-1}),$$ where $x_{i-1}<=y_i(x_i,x_{i-1}) <=x_i.$	
\end{definition}

\subsection{Riemann condition for integrability}
\begin{theorem}[sufficient and necessary condition for integrability] A bounded function $f:A\rightarrow \mathbb{R}$ is Riemann-Stieltjes integrable iff for every $\epsilon >0$ there is a partition $P$ of $A$, such that $U(f,P)-L(f,P) < \epsilon$.\cite{johnsonbaugh2010foundations}
\end{theorem}
\begin{proof}
\begin{itemize}
	\item If it is integrable, then from least-upper-bound theorem, there exist partitions $S,T$ such that
	$$\underline{\int_a^b}f d\alpha -1/2\epsilon < L(f,T),U(f,S) < \bar{\int_a^b}f d\alpha + 1/2\epsilon$$
	Note that $\underline{\int_a^b}f d\alpha$ is the least uppper bound of $L$. Then consider partition $P=S\cup T$ and the squezzing effect of partition refinement, we can prove $U(f,P)-L(f,P) < \epsilon$.
	\item use inequality to prove the reverse. 
\end{itemize}	
\end{proof}


\subsection{Other conditions for integrability}
\begin{lemma}[continuous function on closed interval implies integrability]
If $f$ is continuous at interval $[a,b]$, and $\alpha$ an increasing function, then $f$ is integrable. \cite{johnsonbaugh2010foundations}
\end{lemma}
\begin{proof}
Because $f$ is continuous on a closed interval,then it is \textbf{uniformly continuous}, given a $\epsilon$, we can find fine enough partition such that the upper bound and lower upper in each partition component is small enough, such that $U(f,T)-L(f,T)<\epsilon$.	
\end{proof}


\begin{lemma}[Integrability implies boundedness, necessary condition]\cite{mathanalysischinese} If function $f$ is integrable at interval $[a,b]$, then $f$ is bounded.
\end{lemma}
\begin{proof}
Note that this is not \textbf{Riemann-Stieltjes} integral. To prove: suppose it is not bounded, then not matter how finer the partition will be, then there exist a component,say $\Delta_k$, in which the function $f$ is unbounded. For any number $M$, there exist a $x^*\in \Delta_k$, such that $f(x^*)\Delta_k > M$, and this make the Riemann sum $\sum f(\eta_i)\Delta_i$ always greater than any given large number.	
\end{proof}


\begin{lemma}[bounded function with finite discontinuous points on closed interval implies integrability ]
(Riemann integral)
$f$ is a bounded function with finite number of discontinous points at interval $[a,b]$, then $f$ is integrable
\end{lemma}
\begin{proof}
	by properly partition,i.e., let partition endpoint aligned with discontinuity. 
\end{proof}




\subsection{Properties of Riemann-Stieltjes integrability}
\begin{itemize}
    \item if $f$ is integrable on $[a,b]$, then $|f|$ is integrable on $[a,b]$.\cite{mathanalysischinese}\cite{johnsonbaugh2010foundations} (Proof: $U(\abs{f},T)-L(\abs{f},T)) \leq U(f,T)-L(f,T)$, where we use the inequality $\abs{a}+\abs{b} \geq \abs{a+b}$, let $a=M_i-m_i>=0$,$b=m_i$ for each component in the partition.)
    
    \item Linearity of definite integral \cite{mathanalysischinese}\cite{johnsonbaugh2010foundations}:
    $$\int_a^b \alpha f + \beta g dx = \alpha \int_a^b f dx+ \beta \int_a^b g dx$$
\end{itemize}





\section{Functions of bounded variations}
\begin{definition}\index{bounded variation}
Let $\alpha$ be a function on $[a,b]$. If the set of all sums$$\sum_{i=1}^n \abs{\alpha(x_i)-\alpha(x_{i-1}}$$
where $\{x_0,x_1,...,x_n\}$ is a partition, is bounded, we say function $\alpha$ is of bounded variation on $[a,b]$ and write $\alpha \in BV[a,b]$.
If $\alpha \in BV[a,b]$, let $V_a^b\alpha = lub\{\abs{\alpha(x_i)-\alpha(x_{i-1}}\}$ where the least upper bound is taken over all partitions. $V_a^b\alpha$ is called the total variations. 
\end{definition}




\section{Basic measure theory}
\subsection{Measurable space}
\subsubsection{$\sigma$ algebra}
\begin{definition}[$\sigma$ algebra]\index{$\sigma$-field}\index{$\sigma$-algebra}
	Given a set $\Omega$, a $\sigma$-field, or $\sigma$-algebra is a collection $\mathcal{F}$ of subsets of $\Omega$, with the following properties:
	\begin{enumerate}
		\item $\emptyset \in \cF$.
		\item if $A \in \cF$, then $A^c \in \cF$.
		\item if $A \in \cF$, then $\cup_{i=0}^{\infty}A_i \in \cF$.
	\end{enumerate}
\end{definition}

\begin{example}\hfill
	\begin{itemize}
		\item The trivial $\sigma$-field is $\cF_0 = \{ \emptyset, \Omega \}$.
		\item Let $\cP(\Omega)$ denote the collection of all subsets of $\Omega$, then $\cP(\Omega)$ is a $\sigma$-field.
		\item For any $\sigma$ algebra defined on $\Omega$, it is always between $\cF_0$ and  $\cP(\Omega)$.
		\item The collection $\cF=\{\emptyset, A, A^c,\Omega \}$, where $A$ is a fixed subset of $\Omega$.
		\item The set of all the subsets of finite set $\Omega$.
		\item For a finite sample space $\Omega$, the power set of $\Omega$ is the largest $\sigma$ field, $\{\emptyset,\Omega\}$ is the smallest $\sigma$ field.
	\end{itemize}
\end{example}

\begin{lemma}[$\sigma$ algebra is closed under countable intersection and relative complement]
Let $\cF$ be the $\sigma$ algebra be a $\sigma$ algebra defined on set $X$. Let $A_1,A_2,...$ be a countable sequence subsets in $\cF$. Then
\begin{itemize}
	\item $$\cap_{n=1}^\infty A_i \in \cF.$$	
	In particular, it is closed under finite intersection; that is
	$$\cap_{n=1}^N A_i \in \cF.$$
	\item Let $B,C\in \cF$ and $B\subseteq C$, then
	$$C - B \in \cF.$$
\end{itemize}
\end{lemma}
\begin{proof}
(1) Use DeMorgan's law(\autoref{ch:sets:th:DemorganLaw}), we have
$$\cup_{n=1}^\infty A_i \in \cF \implies \cap_{n=1}^\infty A_i = (\cup_{n=1}^\infty A_i)^C \in \cF\in \cF.$$
(2) Note that $C - B = C\cap(B^C)$. Since $B,C,B^C \in \cF$, from (1) 	$C - B$ is in $\cF$.
\end{proof}


\begin{definition}[generated $\sigma$ algebra from a collection of subsets]\cite[6]{cheng2008crash}
Let $X$ be a set. Let $\cG$ be a collection of the subsets of $X$, or equivalently we write $\cG \subseteq \cP(X)$. Then the \textbf{$\sigma$ algebra} generated by $\cG$ is the smallest $\sigma$ algebra defined on $X$ containing $\cG$; it is denoted by $\sigma(\cG)$.	
\end{definition}

\begin{remark}[how to generate the $\sigma$ algebra from subsets]
Given $\cG$ a collection of the subsets of $X$, we can do complement, countable union, and countable intersection until we get a $\sigma$ algebra.	
\end{remark}



\subsubsection{Measurable space and positive measure}\index{measure}\index{measurable space}\index{measure space}

\begin{definition}[measurable space, measurable sets]
Let $X$ be a set and $\Sigma$ be the $\sigma$ algebra defined on $X$. 	
The pair $(X,\Sigma)$ is called \textbf{measurable space}, the members  $A \in \Sigma$ are called \textbf{measurable sets} or $\Sigma-$measurable sets. A triple $(X,\Sigma,\mu)$ is called measure space.	
	
\end{definition}




\begin{definition}[positive measure, measure space]\cite[7]{cheng2008crash}
	Given a set $X$ with its $\sigma$ field $\Sigma$, a function $\mu: \Sigma \rightarrow \RE$ is called a positive measure if it satisfies:
	\begin{itemize}
		\item Non-negativity: For all $E \in \Sigma$, $\mu(E)>=0$
		\item $\mu(\emptyset) = 0$
		\item Countable additivity: For all countable collections $\{E_i\}$ of pairwise disjoint sets in $\Sigma$:
		$$\mu(\cup_{i=1}^\infty E_i) = \sum_{i=1}^\infty \mu(E_i)$$
	\end{itemize}
A triple $(X,\Sigma,\mu)$ is called \textbf{measure space}.	
\end{definition}

\begin{example}[counting measure]
Let $X$ be an arbitrary set. Let $\cF$ be its $\sigma$ algebra. Define $\mu:\cF\to [0,\infty]$ as
$$\mu(A) = \begin{cases*}
number~of~elements~in~ A, A\in \cF, A ~is~finite~set\\
\infty, A\in \cF, A ~is~infinite~set
\end{cases*}.$$

$\mu$ is called \textbf{counting measure}. 	
\end{example}

\begin{lemma}[basic properties of measure]
	
\end{lemma}



\subsubsection{Borel algebra and Lebesgue measure}\index{Lebesgue measure}





\begin{definition}[Borel subsets in $\R$]\index{Borel set}\index{Borel algebra}\cite{wiki:Borelset}\cite[7]{shreve2004stochastic2}\hfill
\begin{itemize}
	\item A Borel subset in $\R$ is a set  that can be formed from other open sets in $\R$ (or, equivalently, from closed sets) through the operations of countable union, countable intersection, and relative complement.
	\item Every open interval in $\R$ is a Borel subset of $\R$ because an open interval can be written as the countable union of a sequence of closed intervals.
	
\end{itemize}	
\end{definition}

\begin{note}[open interval close interval conversion]
	Using countable union and intersection properties, we can convert between open interval and close intervals, for example
	\begin{itemize}
		\item $(a,b)=\cup_{n=1}^{\infty}[a+1/n,b-1/n]$
		\item $[a,b]=\cap_{n=1}^{\infty}(a-1/n,b]$
		\item (countable union)$(a,\infty) = \cup_{n=1}^{\infty}[a,a+n]$
		\item singleton: $\{a\} = [a,a]$
	\end{itemize}	
\end{note}

\begin{definition}[Borel algebra]
The collection of all Borel subsets on $\R$ forms a $\sigma$-algebra, known as the Borel $\sigma$-algebra, denoted by $\cB(\R)$. The Borel $\sigma$ algebra on $\R$ is the smallest $\sigma$-algebra that containing all the closed and open intervals in $\R$.	
\end{definition}


\begin{definition}[Lebesgue measure on $\R$]\cite[20]{shreve2004stochastic2}\index{Lebesgue measure}
Let $\mathcal{B}(\R)$ be the $\sigma$ algebra of Borel subset of $\R$. The Lebesgue measure on $\R$, which we denote by $\mathcal{L}$, assigns to each set $B\in \mathcal{B}(\R)$ a number $[0,\infty)$ or the value $\infty$ such that 
\begin{itemize}
    \item $\mathcal{L}[a,b]=b-a, ~ \text{if} ~ a\leq b$
    \item (countable additivity) if $B_1,B_2,...$ are disjoint sets in $\mathcal{B}(\R)$, then
    $$\mathcal{L}(\cup_{n=1}^\infty B_n) = \sum_{n=1}^\infty \mathcal{L}(B_n)$$
\end{itemize}
\end{definition}

\begin{lemma}[Basic properties of Lebesgue measure on $\R$]
For the Lebesgue measure defined on $\R$, we have the following properties:
\begin{itemize}
	\item Singleton sets have measure of 0.
	\item Empty set has measure 0.
	\item (finite additivity)
	 If $B_1,B_2,...,B_n$ are disjoint sets in $\mathcal{B}(\R)$, then
	$$\mathcal{L}(\cup_{i=1}^n B_i) = \sum_{n=1}^n \mathcal{L}(B_i).$$
	\item Any countable set of real numbers has Lebesgue measure of 0.
\end{itemize}	
\end{lemma}
\begin{proof}
(1) From the definition $\{a\} = \cL[a,a] = a-a = 0$. (2) Use countable additivity $$\cL(\cup \emptyset) = \cL(\emptyset) = \sum \cL(\emptyset).$$
Note that $\cL(\emptyset) \geq 0$ then $\cL(\emptyset)$ must be zero.
(3) Straight forward from countable additivity. (4) Use the union of singleton sets.	
\end{proof}

\begin{definition}[Borel algebra, Lebesgue measure on $\R^n$]\cite{fries2007mathematical} \index{Lebesgue measure}\index{Borel algebra}\label{ch:calculus:def:BorelAlgebraLebesgueMeasureonR}
	\begin{itemize}
		\item In $\R^n$, let $a_i < b_i (i=1,2,...,n)$, the \textbf{Borel $\sigma$ algebra} $\mathcal{B}(\R^n)$ is the smallest $\sigma$-algebra generated(\autoref{ch:theory-of-probability:th:sigmafieldGeneration}) by
		\textbf{all} intervals like $$(a_1,b_1)\times (a_2,b_2)...(a_n,b_n)\in \mathcal{B}(\R^n)$$
		\item The measure $\lambda:\mathcal{B}(\R^n)\rightarrow \R$ defined as:
		$$\lambda((a_1,b_1)\times (a_2,b_2)...(a_n,b_n)) = \prod_{i=1}^n (b_i-a_i)$$ is called a \textbf{Lebesgue measure} on $\mathcal{B}(\R^n)$.
	\end{itemize}	
\end{definition}

\begin{remark}
	It can be easily showed that the Lebesgue measure satisfies the requirement for a measure: (1) non-negativity;(2) $\mu(\emptyset) = 0$; (3) countable additivity.
\end{remark}

\subsection{Measurable functions and properties}
\subsubsection{Measurable function and measurability}
\begin{definition}[measurable functions]\index{measurable function}
\cite[361]{johnsonbaugh2010foundations} A function $f$ from a set $X$, with a $\sigma$ algebra $\cF$, into $[-\infty,\infty]$ is called an extended real measurable function on $X$ with respect to $\cF$ if $f^{-1}(V)\in \cF$, for every open set $V\subset [-\infty,\infty]$
\end{definition}


\begin{example}[constant value function is measurable]
A constant value function map from a measurable space $(X,\cF)$ to a constant $c$ in $\R$ is always measurable, since 
$$f^{-1}(V) =\begin{cases*}
\emptyset, if ~ c\notin V\in \cB(\R)\\
X,if ~ c\in V\in \cB(\R)
\end{cases*} \emptyset.$$	
	
\end{example}

\begin{definition}[Borel measurable function]\cite[21]{shreve2004stochastic2}
Let $f:\R\to \R$ be a real-valued function defined on $\R$. If for every $B\in \cB(\R)$, the set $f^{-1}(B)$ is also in $\cB(\R)$, then $f$ is said to be \textbf{Borel measurable function}.	
\end{definition}

\begin{remark}\hfill
\begin{itemize}
	\item Every continuous and piecewise continuous function is Borel measurable.
	\item Many Borel measurable functions are not continuous. For example, step function.
	\item It is usually extremely difficult to find a function that is not Borel measurable.
\end{itemize}	
\end{remark}

\begin{theorem}[criteria for measurable function]
\cite[361]{johnsonbaugh2010foundations}Let $X$ be a set, let A function $f$ from a set $X$, with a $\sigma$ algebra $\cF$, into $[-\infty,\infty]$. Then the following are equivalent:
\begin{itemize}
    \item $f$ is measurable
    \item $\{x|f(x) > a\} \in \cF$ for every $a\in \R$
    \item $\{x|f(x) < a\} \in \cF$ for every $a\in \R$
    \item $\{x|f(x) \geq a\} \in \cF$ for every $a\in \R$
    \item $\{x|f(x) \leq a\} \in \cF$ for every $a\in \R$
\end{itemize}
\end{theorem}
\begin{proof}
(1) implies (2) by definition;\\
(2) implies (3): first the set $\{x|f(x) \geq a\} = \cap_{n=1}^\infty \{x|f(x) > a-1/n\}$, therefore $\{x|f(x) \geq a\} \in \cF$, and therefore $$\{x|f(x) < a\} = X-\{x|f(x) \geq a\} \in \cF$$
other cases can be prove using similar techniques. 	
\end{proof}



\begin{lemma}
	
\end{lemma}


\subsubsection{Properties}

\begin{lemma}[function composition and  measurability]
	\cite[362]{johnsonbaugh2010foundations}\label{ch:calculus:th:continuityPreservesBorelMeasurability} 
\begin{itemize}
	\item If $F:\R\to\R$ is Borel measurable and $f:X\to \R$ is Borel measurable, then $F\circ f$ is measurable.
	\item 	If $F:\R\to\R$ is continuous and $f:X\to \R$ is Borel measurable, then $F\circ f$ is measurable.
\end{itemize}	
\end{lemma}
\begin{proof}
(1) Let $B\in \cB(\R)$, then $F^{-1}(B)$ is in $\cB(\R)$ since $F$ is Borel measurable. Since $f$ is Borel measurable $(F\circ f)^{-1}(B) = f^{-1}(F^{-1}(U)) \in \cB(\R)$.
(2)	
	Let $U$ be an open set in $\R$, the $F^{-1}(U)$ is open since $F$ is continuous. Since $f$ is measurable $(F\circ f)^{-1}(U) = f^{-1}(F^{-1}(U)) \in \cF$. We can also use the fact every continuous function is Borel measurable.	
\end{proof}

\begin{corollary}
	\cite[362]{johnsonbaugh2010foundations} If $f:X\to \R$ is measurable, then the function $cf,f^2,\abs{f},1/f$(if $f\neq 0,\forall x\in X$) are measurable.
\end{corollary}
\begin{proof}
	use continuous function $F=cx,x^2...$ and function composition.	
\end{proof}

\begin{lemma}[algebraic properties of Borel measurable function]\cite[362-364]{johnsonbaugh2010foundations}\label{ch:calculus:th:AlgebraicPropertiesBorelMeasurableFunctions}
If $f,g: (X,\cF)\to (\R, \cB)$ are real-valued measurable functions and $k\in \R$, then
\begin{itemize}
	\item $kf$ is Borel measurable function.
	\item $f+g$ and $f-g$ are Borel measurable functions.
	\item $f^2$ is Borel measurable function.
	\item $fg$ is Borel measurable function.
	\item $1/g, g\neq 0$ is Borel measurable function.
	\item $f/g$ is Borel measurable function.
	\item $f^+(x) = \max(f(x),0)$ and $f^-(x) = -\min(f(x),0)$ are both measurable. Conversely, if $f^+,f^-$ are measurable, so is $f$.
\end{itemize}	
\end{lemma}
\begin{proof}
We will emphasize the proof $f+g$ and $fg$. Others directly from \autoref{ch:calculus:th:continuityPreservesBorelMeasurability}. (1) If $k>0$, then $$\{x:kf(x) < b\} = \{x:f<b/k\} = f^{-1}((-\infty,b/k)).$$
the set $f^{-1}((-\infty,b/k))$ is in $\cF$ because $f$ is Borel measurable. Similarly we can prove the case $k\leq 0$. (2) Note that
$$\{x:f(x)+g(x)<b \} = \cup_{q+r<b;q,r\in \Q} \{x:f(x)<q \}\cap \{x:g(x) < r\}.$$ 
Since 
$$\cup_{q+r<b;q,r\in \Q} (-\infty,q)\cap (-\infty,r)$$
are Borel subsets of $\R$, its inverse image is also in $\cF$. Therefore, $f+g$ is Borel measurable.
(3) The function $f^2$ is measurable since if $b > 0$,
$$\{x:f(x)^2 < b\} = \{x:-\sqrt{b} < f(x) < \sqrt{b}\}$$
and the set $(-\sqrt{b},\sqrt{b}) \in \cB$. 
(4) Note that
$$fg = \frac{1}{2}((f+g)^2 - f^2 - g^2),$$
and then use (1)(2)(3).
(5) Note that if $g\neq 0$,

$$\{x:1/g(x) < b\} = \begin{cases*}
\{x: 1/b < g(x) < 0\}, if b < 0 \\
\{x: -\infty < g(x) < 0\}, if b = 0 \\
\{x: -\infty < g(x) < 0\}\cup \{x: 1/b < g(x) < \infty\}, if b > 0 \\
\end{cases*}.$$


(6) Use (4)(5).
(7)(a) For $B \subset [0,\infty]$, if $B$ does not includes $\{0\}$, $(f^+)^{-1}(B) = f^{-1}(B)\in \cF$; if if $B$  includes $\{0\}$, $(f^+)^{-1}(B) = f^{-1}(B) \cup f^{-1}([-\infty,0]) \in \cF$. Therefore $f^+$ is measurable. Similarly we can prove the case of $f^-$. 
(b) conversely, note that $f = f^+-f^-$.
\end{proof}


\begin{lemma}[measurability and limits]\label{ch:calculus:th:measurabilityAndLimits}
Let $f_1,f_2,...$ be a sequence of measurable $\RE$-valued functions. Then the functions 
$$\sup_n f_n(x), \inf_n f_n(x), \limsup_n f_n(x), \liminf_n f_n(x),$$
are all measurable.	
\end{lemma}
\begin{proof}
(1)	Denote $g(x) =\sup_n f_n(x) $. Then if there exist an $n$ such that $f_n(x) > c$, we have $g(x) > c$. Therefore, $$\{x|g(x) > c\} = [\cap^\infty_{n=1} \{x:f(x)\leq c\}]^C =\cup_{n=1}^\infty \{x:f_n(x) > c\}.$$
Since each $\{x:f_n(x)>c\}$ is measurable, its countable union is also measurable. (2) Similar to (1).

	
\end{proof}


\subsection{Convergence of measurable functions}

\subsection{Almost everywhere convergence}
\begin{definition}[Almost everywhere convergence of Borel measurable function]\cite[24]{shreve2004stochastic2}\index{almost everywhere convergence}
	Let $f_1,f_2,f_3,...$ be a sequence of real-valued, Borel-measurable functions defined on $\R$. Let $f$ be another real-valued, Borel-measurable functions defined on $\R$. We say that $f_1,f_2,...$ converges to $f$ almost everywhere and write
	$$\lim_{n\rightarrow\infty} f_n = f ~ \text{almost everywhere} ~$$
	if the set of $x\in \R$ for which the sequence of numbers $f_1(x),f_2(x),...$ does not have limit $f(x)$ is a set with Lebesgue measure zero.
\end{definition}

\begin{remark}[interpretation]\hfill
\begin{itemize}
	\item For every fixed $x\in \R$, we can evaluate the limit of the sequence $f_1(x),f_2(x),...$. If $\lim_{n\to\infty} f_n(x) = f(x)$, then we say $f_1(x),f_2(x),...$ converges to $f(x)$ at $x$.
	\item When we say almost everywhere convergence, we mean the subset of $\R$ \textbf{where $\lim_{n\to \infty} f_n(x) \neq f(x)$ has measure of zero.}  
\end{itemize}	
\end{remark}

\begin{example}\cite[24]{shreve2004stochastic2}
Consider a sequence of normal density functions
$$f_n(x) = \sqrt{\frac{n}{2\pi}}\exp(-\frac{nx^2}{2}).$$
We have
\begin{itemize}
	\item If $x\neq 0$, then $\lim_{n\to \infty} f_n(x) = 0$.
	\item If $x = 0$, then $\lim_{n\to \infty} f_n(0) = \lim_{n\to \infty} \sqrt{\frac{n}{2\pi}} = \infty$.
\end{itemize}
Therefore, the sequence $f_1,f_2,...$ converge almost everywhere to the function $g(x) = 0,\forall x\in \R$. 
Only at $x = 0$, , which has zero Lebesgue measure, the sequence $f_1(0),f_2(0),...$ not converges to $g(0)$.
\end{example}



\section{Lebesgue integral}

\subsection{Simple function and its Lebesgue integral }



\begin{definition}[simple function]\cite[364]{johnsonbaugh2010foundations}\index{simple function}\hfill
\begin{itemize}
	\item A \textbf{simple function} is a function taking finite number of values.
	\item Let $(X,\Sigma)$ be a measurable space.  A \textbf{simple measurable function} is a function $f:X\rightarrow \R$:
	$$f(x) = \sum_{k=1}^n a_k I_{A_k}(x),$$
	where $A_1,A_2,...A_n$ are disjoint subsets in $\Sigma$ that \textbf{partitions} $X$.
\end{itemize}	
\end{definition}

\begin{remark}[representation of simple functions on $X$]\hfill
\begin{itemize}
	\item A simple function is a finite linear combination of indicator functions of measurable sets.
	\item When we represent a simple function by $f(x) = \sum_{k=1}^n a_k I_{A_k}(x),$ we require $A_1,A_2,...,A_n$ to partition $X$; otherwise for some $x\in X$, its mapping $f(x)$ is not defined.
\end{itemize}	
\end{remark}


\begin{lemma}[different representations of simple functions]\cite[16]{cheng2008crash}\label{ch:calculus:th:DifferentRepresentationSimpleFunctions}
Let $f$ be a simple function mapping from $X$ to $\R$. Let $A_1,A_2,...,A_N$ be a partition of $X$ such that
	$f(x) = \sum_{k=1}^N a_k I_{A_k}(x).$ It follows that
\begin{itemize}
	\item (coarsest representation) Suppose $f$ will take $Q$ different values $q_1,q_2,...,q_Q$. Then the collection of subsets $f^{-1}(q_1),f^{-1}(q_2),...,f^{-1}(q_Q)$ form the coarsest partition, denoted by $P_0$.
	\item Any partition $P$ used to represent $f$ must be obtained from subdividing subsets in $P_0$; that is, $P\subseteq P_0$. Any partition $P \subseteq P_0$ is called \textbf{compatible partition}.
	\item Let $B_1,B_2,...,B_M$ be an arbitrary partition(not necessarily compatible), then 
	$$f(x) = \sum_{i=1}^N\sum_{j=1}^M a_i I_{A_i \cap B_j}(x).$$
	Note that $\{A_i\cap B_j, i=1,...,N,j=1,...,M\}$ is also an compatible partition.
	\item Let $B_1,B_2,...,B_M$ be another compatible partition of $X$, then $f(x) = \sum_{k=1}^M b_k I_{B_k}(x).$
	where $a_i = b_j$ if $A_i \cap B_j \neq \emptyset$. That is,
	$f$ can also be represented by
	$$f(x) = \sum_{i=1}^N\sum_{j=1}^M a_i I_{A_i \cap B_j}(x) = \sum_{i=1}^N\sum_{j=1}^M b_j I_{A_i \cap B_j}(x).$$
	
\end{itemize}	
\end{lemma}
\begin{proof}
(1)(2) obviously. (3) Note that sets $A_i\cap B_j,j=1,...,M$ is obtained from subdividing $A_i$. (4)
$$f(x) = \sum_{i=1}^N a_i I_{A_i}(x) = \sum_{i=1}^N\sum_{j=1}^M a_i I_{A_i \cap B_j}(x) = \sum_{i=1}^N\sum_{j=1}^M b_j I_{A_i \cap B_j}(x) = \sum_{j=1}^M b_i I_{B_j}(x).$$
Note that we use the fact that $a_i=b_j$ must hold on the common set $A_i\cap B_j$;otherwise $f$ will map an element to different values.	
\end{proof}



\begin{definition}[Lebesgue integral of simple functions]\index{Lebesgue integral}\cite[368]{johnsonbaugh2010foundations}\cite[16]{cheng2008crash}
	Let $s$ be a non-negative simple measurable function in $X$, $s=\sum_{i=1}^n c_i I_{A_i}$, where $A_i \in \Sigma$, and partitions $X$. We define the integral of $s$ over $X$ with respect to a measure $\mu$ as:
	$$\int_X s d\mu = \sum_{i=1}^n c_i \mu(A_i)$$
\end{definition}

\begin{theorem}[algebraic properties of Lebesgue integral of simple functions]\hfill
\begin{itemize}
	\item If $f,g: X\to [0,\infty)$ are measurable simple functions and $\alpha in X$. Then $\alpha f, f+g$ are measurable simple functions and 
	$$\int_X \alpha f d\mu = \alpha \int_X f d\mu, \int_X  f+g d\mu = \int_X f d\mu + \int_X g d\mu.$$ 	
	\item If $f(x) \leq g(x)$ for each $x\in X$ we have $$\int_X fd\mu \leq \int_X g d\mu.$$
\end{itemize}		
\end{theorem}
\begin{proof}
(1) The scaling is straight forward. For the sum rule, we have
\begin{align*}
\int_X  f+g d\mu &= \int_X  \sum_{i=1}^N a_i I_{A_i} + \sum_{j=1}^M b_j I_{B_j} d\mu \\
& =\int_X  \sum_{i=1}^N\sum_{j=1}^M a_i I_{A_i\cap B_j} + \sum_{i=1}^N\sum_{j=1}^M b_j I_{A_i\cap B_j} d\mu \\
& =\int_X  \sum_{i=1}^N\sum_{j=1}^M a_i I_{A_i\cap B_j} +  b_j I_{A_i\cap B_j} d\mu \\
& = \sum_{i=1}^N\sum_{j=1}^M a_i \mu(A_i\cap B_j) +  b_j \mu(A_i\cap B_j) \\
& = \sum_{i=1}^N a_i \mu(A_i) + \sum_{j=1}^M b_j \mu(B_j) \\
& = \int_X fd\mu + \int_X gd\mu
\end{align*}
where we use results from \autoref{ch:calculus:th:DifferentRepresentationSimpleFunctions}.
(2) 
Note that $f-g$ is a simple function only takes non-positive values. Therefore 
$$\int_X f d\mu - \int_X g d\mu = \int_X (f-g) d\mu \leq 0.$$
\end{proof}

\subsection{Lebesgue integral of measurable functions}

\subsubsection{Integral of non-negative functions}

\begin{theorem}[any non-negative measurable functions as the limit of simple functions]
	If $f$ is any measurable function from $X$ into $[0,\infty]$, then there exists a sequence $\{s_n\}$ simple functions such that \cite{johnsonbaugh2010foundations}
	$$0\leq s_1 \leq s_2 \leq s_3 \leq ...$$
	and
	$$\lim_{n\rightarrow \infty} s_n(x) = f(x),\forall x\in X.$$
\end{theorem}

\begin{note}[notations of Lebesgue integral]
	The notation for Lebesgue integral is given by
	$$\int_{x\in X} f(x)d\mu, \int_{x\in X} f(x)d\mu, \int_{x\in X} f(x)d\mu(x), \int_X f(x)\mu(dx)$$	
\end{note}




\begin{definition}[Lebesgue integral of non-negative functions] Let $f$ be a non-negative extended real-valued function on $X$. We define the integral of $f$ over $X$ with respect to a measure $\mu$ to be
$$\int_X f d\mu =\sup\{ \int_X s d\mu | s~ \text{is a simple measurable function on} ~ X, 0 \leq s \leq f \} \}.$$

If the integral is a finite value, we say \textbf{$f$ is Lebesgue integrable}.
\end{definition}

\begin{remark}[Lebesgue integral of non-negative functions are well-defined]\hfill
\begin{itemize}
	\item Note that for any non-negative simple function, its Lebesgue integral is wells defined. 
	\item $\sup$ operation on a set of real numbers will produce a unique number.
\end{itemize}	
\end{remark}




\begin{theorem}[monotone convergence theorem]\cite[25]{cheng2008crash}\label{ch:calculus:th:LebesgueMonotoneConvergenceTheorem}Let $(X,\cF,\mu)$ be a measure space. Let $f_n: X\to [0,\infty]$ be non-negative measurable functions increasing pointwise to $f$. Then
	$$\int_X f(x) d\mu = \int_X(\lim_{n\to \infty} f_n(x)) d\mu = \lim_{n\to\infty} \int_X f_n(x)d\mu$$
\end{theorem}


\begin{remark}[limit of infinity value]
	note that for any $x$, $f_1(x) \leq f_2(x) \leq f_3(x) \leq ...$. If $\lim_{n\to\infty} f_n(x) = \infty$. If for a nonzero measure set $A$ we have $f(x) = \infty, \forall x\in A$, then $\int_X f d\mu = \infty$ 	
\end{remark}


\begin{remark}[interpretation of limits]
	Note that $f(x) = \lim_{n\to \infty} f_n(x) \in [0,\infty]$ is guaranteed to exist since the sequence $\{f_n(x)\}$ is monotone increasing sequence. Moreover, $f(x)$ is measurable via \autoref{ch:calculus:th:measurabilityAndLimits} since for monotone sequence, taking $\sup$ is equivalent to taking sequential limits(\autoref{ch:sequences-series:Remark：EquivalenceSupAndSequentialLimitMonotoneSequence}).	
\end{remark}




\begin{theorem}[algebraic properties of Lebesgue integral of non-negative measurable functions]\cite[374]{johnsonbaugh2010foundations}\hfill
	\begin{itemize}
		\item If $f,g: X\to [0,\infty)$ are measurable simple functions and $\alpha in X$. Then $\alpha f, f+g$ are measurable simple functions and 
		$$\int_X \alpha f d\mu = \alpha \int_X f d\mu, \int_X  f+g d\mu = \int_X f d\mu + \int_X g d\mu.$$ 	
		\item If $f(x) \leq g(x)$ for each $x\in X$ we have $$\int_X fd\mu \leq \int_X g d\mu.$$
	\end{itemize}		
\end{theorem}
\begin{proof}

\end{proof}


\subsubsection{Integral of general functions}

\begin{definition}[Lebesgue integral of general functions]\cite[369]{johnsonbaugh2010foundations}
Let $f$ be a measurable function on $(X,\cM)$. Define	$f^+(x) = \max(f(x),0)$ and $f^-(x) = -\min(f(x),0)$(both are non-negative and measurable(\autoref{ch:calculus:th:AlgebraicPropertiesBorelMeasurableFunctions})). It follows that
\begin{itemize}
	\item If \textbf{at least one} of the numbers $\int_X f^+ d\mu$ and $\int_X f^- d\mu$ is finite, then \textbf{the integral of $f$ over $X$ with respect to $\mu$} is given by
	$$\int_X fd\mu = \int_X f^+ d\mu - \int_X f^- d\mu.$$
	\item If \textbf{both} of the numbers $\int_X f^+ d\mu$ and $\int_X f^- d\mu$ are finite, then $f$ is said to be \textbf{Lebesgue integrable}. The set of Lebesgue integrable functions are denoted by $\cL(X,\cM,\mu)$. 
	\item If \textbf{both} of the numbers $\int_X f^+ d\mu$ and $\int_X f^- d\mu$ are $\infty$, then the integral of $f$ is \textbf{ not defined}.
\end{itemize}
\end{definition}



\begin{note}[existence of Lebesgue integral vs. Lebesgue integrability]\hfill
\begin{itemize}
	\item For a general measurable function, the Lebesgue integral might not exist.
	\item If the Lebesgue integral exists and it is finite, then it is Lebesgue integrable.
\end{itemize}	
\end{note}






\begin{theorem}[algebraic properties of Lebesgue integral of general measurable functions]\cite[376]{johnsonbaugh2010foundations}\hfill
	\begin{itemize}
		\item If $f,g: X\to [0,\infty)$ are measurable simple functions and $\alpha in X$. Then $\alpha f, f+g$ are measurable simple functions and 
		$$\int_X \alpha f d\mu = \alpha \int_X f d\mu, \int_X  f+g d\mu = \int_X f d\mu + \int_X g d\mu.$$ 	
		\item If $f(x) \leq g(x)$ for each $x\in X$ we have $$\int_X fd\mu \leq \int_X g d\mu.$$
	\end{itemize}		
\end{theorem}
\begin{proof}
	
\end{proof}






\subsection{Properties of Lebesgue integral}
\begin{definition}
(Almost equal everywhere) Functions $f$ and $g$ are said to be equal almost everywhere if \cite{wiki:lebesgueintegral}
$$\mu(\{x\in X:f(x)\neq g(x)\}) = 0$$
\end{definition}

\begin{theorem}
	
	
\end{theorem}



\subsection{Almost everywhere}
\begin{definition}\index{almost everywhere}
If the set of numbers in $\R$ that fails to have some property is a set with Lebesgue measure zero, we say that the property holds almost everywhere. \cite{shreve2004stochastic2}
\end{definition}



\subsection{Riemann vs. Lebesgue integrals}

\begin{lemma}
If a function is Riemann integrable then it is Lebesgue integrable.	
\end{lemma}





Let $f$ be a bounded function defined $\R$, let $a<b$ be numbers.\cite{shreve2004stochastic2}
\begin{enumerate}
    \item The Riemann integral $\int_a^b f(x)dx$ is defined (i.e. the lower sum and upper Riemann sums converge to the same limit) if and only if the set of points $x\in [a,b]$ where $f(x)$ is not continuous has Lebesgue measure zero.
    \item If the Riemann integral $\int_a^b f(x)dx$ is defined, then $f$ is Borel measurable (so the Lebesgue integral $\int_{[a,b]}f(x) d\mathcal{L}(x)$ is also defined), and the Rieman and Lebesgue integral agree. 
\end{enumerate}


\begin{example}[integration of Dirichlet function]\cite[1]{cheng2008crash}
Define Dirichlet function $D:[0,1]\to \{0,1\}$as
$$D(x) = \begin{cases*}
1, x\in \Q\\
0, x \in \R / \Q
\end{cases*}.$$
For the integral 
$$\int_0^1 D(x)dx,$$
we have
\begin{itemize}
	\item Riemann integral does not exists. Because no matter how fine the partition is, in each interval there will be a rational number and an irrational number since they are dense(\autoref{ch:sets:th:rationalIrrationalNumbersDense}); therefore the upper sum will be 1 and the lower sum will be 0 for all possible partitions.
	\item Lebesgue integral will give integral value of 1 since we can decompose as
	
	$$\int_0^1 D(x)d\mu(x) = \int_{[0,1]\cap Q} D(x)d\mu(x) +　\int_{[0,1]－ Q} D(x)d\mu(x) = 0 + 1 = 1.$$ 
\end{itemize}	
\end{example}

\subsection{Convergence theorems}


\begin{theorem}[Fatou's lemma]\cite[27]{cheng2008crash}\label{ch:calculus:th:LebesgueFatouTheorem}Let $(X,\cF,\mu)$ be a measure space. Let $f_n: X\to [0,\infty]$ be non-negative measurable functions increasing pointwise to $f$. Then
	$$\int_X f(x) d\mu = \int_X(\lim_{n\to \infty} f_n(x)) d\mu = \lim_{n\to\infty} \int_X f_n(x)d\mu$$
\end{theorem}




\begin{theorem}[dominated convergence theorem]\label{ch:calculus:th:LebesgueDominatedConvergenceTheorem}\cite[27]{cheng2008crash}Let $(X,\cF,\mu)$ be a measure space. Let $f_n: X\to [0,\infty]$ be non-negative measurable functions increasing pointwise to $f$. Then
	$$\int_X f(x) d\mu = \int_X(\lim_{n\to \infty} f_n(x)) d\mu = \lim_{n\to\infty} \int_X f_n(x)d\mu$$
\end{theorem}


\begin{example}[application of dominated convergence theorem]
Compute the following integral
$$\lim_{n\to\infty} \int_{\R} \frac{n\sin(x/n)}{x(x^2+1)}dx.$$
\textbf{Solution:}\\
Define $f_n(x)=\frac{n\sin(x/n)}{x(x^2+1)}$. Then
\begin{align*}
\abs{f_n(x)} &= \abs{\frac{n\sin(x/n)}{x(x^2+1)}}\\
& = \abs{\frac{\sin(x/n)}{x/n} \frac{1}{1+x^2}} \\
& \leq \frac{1}{1+x^2} = g(x).
\end{align*}	

Therefore, we have
$$\lim_{n\to\infty} \int_{\R} \frac{n\sin(x/n)}{x(x^2+1)}dx = \int_{\R} \lim_{n\to\infty} \frac{n\sin(x/n)}{x(x^2+1)}dx = \int_{\R} \frac{1}{1+x^2} = \pi.$$
\end{example}



\begin{example}[when dominated convergence condition is not satisfied]
Define a sequence of functions $\{f_n\}$ via
$$f_n(x) = nI_{(0,1/n]} = \begin{cases*}
n, if~ 0 < x \leq 1/n \\
0, else
\end{cases*}.$$
There exists no function $g$ such that $\abs{f_n(x)}\leq g(x)$; therefore the dominated convergence theorem condition is not satisfied.
Then it is easy to see that
$$1 = \lim_{n\to \infty} \int_0^1 f_n(x)dx.$$
However, 
$$\int_0^1 \lim_{n\to \infty} f_n(x)dx = \int_0^1 0 dx = 0.$$
where we use the fact that $f_n(x)$ converges to 0 pointwise.
\end{example}


\subsubsection{Applications}

\begin{theorem}[exchange summation and integral]\cite[30]{cheng2008crash}\label{ch:calculus:th:ExchangeSumAndLebesgueIntegral}\hfill
\begin{itemize}
	\item Let $f_n: X\to [0,\infty]$ be \textbf{non-negative} function. Then
	$$\int_X \sum_{n=1}^\infty f_n(x) dx = \sum_{n=1}^\infty \int_X  f_n(x) dx.$$
	\item Let $f_n:X\to\R$ be measurable function, with $\int_X \sum_{n=1}^\infty \abs{f_n(x) }dx = \sum_{n=1}^\infty \int_X  \abs{f_n(x)} dx$ being finite.
	Then
	$$\int_X \sum_{n=1}^\infty f_n(x) dx = \sum_{n=1}^\infty \int_X  f_n(x) dx.$$
\end{itemize}	
\end{theorem}
\begin{proof}
(1)Let $g_n(x) = \sum_{i=1}^n f_n(x)$. Then $g_n(x)$ is monotone increasing sequence, then it has limit (even though the limit is $\infty$). Then from monotone convergence theorem(\autoref{ch:calculus:th:LebesgueMonotoneConvergenceTheorem}), we have
$$\lim_{n\to\infty} \int g_n = \int \lim_{n\to\infty} g_n.$$ 	
(2)Let $g_n(x) = \sum_{i=1}^n f_n(x)$, then $\abs{g_n(x)} \leq H(x) = \sum_{n=1}^\infty \int_X  \abs{f_n(x)} dx$. Then from dominated convergence theorem(\autoref{ch:calculus:th:LebesgueDominatedConvergenceTheorem}), we have
$$\lim_{n\to\infty} \int g_n = \int \lim_{n\to\infty} g_n.$$
\end{proof}



\begin{theorem}[exchange summation in double series]\cite[373,94]{johnsonbaugh2010foundations}\hfill
	\begin{itemize}
		\item Let $f_n: X\to [0,\infty]$ be \textbf{non-negative} function. Then
		$$\int_X \sum_{n=1}^\infty f_n(x) dx = \sum_{n=1}^\infty \int_X  f_n(x) dx.$$
		\item Let $f_n:X\to\R$ be measurable function, with $\int_X \sum_{n=1}^\infty \abs{f_n(x) }dx = \sum_{n=1}^\infty \int_X  \abs{f_n(x)} dx$ being finite.
		Then
		$$\int_X \sum_{n=1}^\infty f_n(x) dx = \sum_{n=1}^\infty \int_X  f_n(x) dx.$$
	\end{itemize}	
\end{theorem}
\begin{proof}
	(1)Let $g_n(x) = \sum_{i=1}^n f_n(x)$. Then $g_n(x)$ is monotone increasing sequence, then it has limit (even though the limit is $\infty$). Then from monotone convergence theorem(\autoref{ch:calculus:th:LebesgueMonotoneConvergenceTheorem}), we have
	$$\lim_{n\to\infty} \int g_n = \int \lim_{n\to\infty} g_n.$$ 	
	(2)Let $g_n(x) = \sum_{i=1}^n f_n(x)$, then $\abs{g_n(x)} \leq H(x) = \sum_{n=1}^\infty \int_X  \abs{f_n(x)} dx$. Then from dominated convergence theorem(\autoref{ch:calculus:th:LebesgueDominatedConvergenceTheorem}), we have
	$$\lim_{n\to\infty} \int g_n = \int \lim_{n\to\infty} g_n.$$
\end{proof}


\begin{theorem}[differentiation under the integral sign]\cite[35]{cheng2008crash}
Let $(X,\cF,\mu)$ be a measurable space. Let $T$ be an open set of $\R^n$, and $f:X\times T\to \R$, with $f(\cdot,t)$ being measurable for each $t\in T$. 
Then
$$F(t) = \int_X f(x,t)dx,$$
is differentiable with the derivative
$$F'(t) = \frac{d}{dt}\int_X f(x,t) dx = \int_X \frac{\Pa}{\Pa t} f(x,t)dx$$
provided the following condition satisfied
\begin{itemize}
	\item for each $x\in X$, $\frac{\Pa}{\Pa t} f(x,t)$ exists for all $t\in T$.
	\item there is an integrable function $g$ such that $\abs{\frac{\Pa}{\Pa t} f(x,t)} \leq g(x)$ for all $t\in T$. 
\end{itemize}	
\end{theorem}
\begin{proof}
Let $\{h_n\}$ be a sequence converging to 0. We have 	
\begin{align*}
F'(t) &= \lim_{n\to\infty} \frac{F(t+h_n) - F(t)}{h_n} \\
&= \lim_{n\to\infty} \int_X \frac{f(x,t+h_n) - f(x,t)}{h_n} dx \\
&=  \int_X \lim_{n\to\infty}\frac{f(x,t+h_n) - f(x,t)}{h_n} dx \\
&=  \int_X \frac{\Pa}{\Pa t} f(x,t) dx 
\end{align*}
where we use \autoref{ch:calculus:th:LebesgueDominatedConvergenceTheorem} to justify the exchange of limit and integral in the following way:

$$\abs{\frac{f(x,t+h_n) - f(x,t)}{h_n}} = \abs{\frac{\Pa }{\Pa t}f(x,z)} \leq g(x)$$
\end{proof}


\section{Notes on bibliography}
The chapter mainly reference intermediate level real analysis textbooks\cite{johnsonbaugh2010foundations}\cite{abbott2001understanding}\cite{thomson2001elementary}.

\cite{duren2012invitation}

\printbibliography
\end{refsection}
