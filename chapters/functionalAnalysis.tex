
\begin{refsection}
	
\startcontents[chapters]	
\chapter{Function Sequences, Series \& Approximation}\label{ch:function-sequences-series--approximation}
%\minitoc

\printcontents[chapters]{}{1}{}
\section{Pointwise convergence, uniform convergence}
\begin{definition}[sequence and series of functions]\hfill
	\begin{itemize}
		\item A set of functions $\{f_n\}$ are called a sequence of functions if for each positive integer $n$, a function $f_n$ is given. 
		\item A series of functions is given by
		$$\sum_{i=1}^\infty f_i(x)$$ with its partial sum defined as $S_n(x) = \sum_{i=1}^n f_i(x)$.
	\end{itemize}	
	
\end{definition}

\begin{definition}[pointwise and uniformly convergence]\cite{johnsonbaugh2010foundations}
	Let $\{f_n\}$ be a sequence of function on a set $X$. Let $f$ be a function of $X$.
	\begin{itemize}
		\item  We say that $\{f_n\}$ \textbf{converges pointwise} to $f$ on $X$ if  
		$$\lim_{n\rightarrow \infty} f_n(x) = f(x) \forall x\in X.$$
		\item We say that $\{f_n\}$ \textbf{converges uniformly} to $f$ on $X$ if for every $\epsilon > 0$, there exists a positive integer $N$ such that if $n \geq N$, then
		$$\abs{f_n(x)-f(x)} < \epsilon, \forall x\in X.$$
	\end{itemize}
\end{definition}
\begin{remark}
	Note that the key difference between point-wise and uniform is in point-wise, $N$ depends on $x$ and $\epsilon$, whereas in uniform, $N$ only depends on $\epsilon$. \textbf{Note that $N$ cannot be $\infty$ (If $N$ is $\infty$, then no $n>N$ can be found based on the definition of infinity.), otherwise every pointwise convergence will be uniform convergence}
\end{remark}

\begin{example}
	The geometric series $\sum_{i=1}^\infty x^n$ converges pointwise for $-1 < x < 1$. It does not uniformly converge for $-1 < x < 1$, because as $x\to 1,N\to \infty$ to converge.
\end{example}

\begin{example}
	The sequence $f_n = x^n$ converge pointwise to 0 at $-1<x<1$. It is not uniform convergence because given $\epsilon > 0$, we require $N > \frac{\log \epsilon}{\log x} \to \infty$ as $x \to 1$.
\end{example}

\begin{definition}[Cauchy condition]
	Let $\{f_n\}$ be a sequence of function on a set $X$. Let $f$ be a function of $X$. We say that $\{f_n\}$ converges uniformly to $f$ on $X$ if and only if for every $\epsilon > 0$, there exists a $N$, such that if $m,n > N$, we have\cite{johnsonbaugh2010foundations}
	$$\abs{f_n(x)-f_m(x)} < \epsilon, \forall x\in X$$
\end{definition}

\begin{theorem}[Weierstrass M-test]
	\cite[416]{kaplan1973advanced} Let $\sum_{i=1}^\infty u_n(x),x\in E$ be given. If there is a convergent series of constants $\sum_{i=1}^n M_n$ such that
	$$\abs{u_n(x)} \leq M_n, \forall x\in E$$
	then the series $\sum_{n=1}^\infty u_n(x)$ converges absolutely for all $x \in E$ and is uniformly convergent in $E$.
\end{theorem}

Proof: First it is easy to show that $\sum_{n=1}^\infty u_n(x)$ converges absolutely pointwise using the comparison test. Let $S(x)$ be the limit, $R_n(x) = \abs{S_n(x)-S(x)}$, our goal is to show $R_n(x)$ will converge uniformly to zero. Note that $R_n(x) \leq M_{n+1} + M_{n+2} + ...$. Because $\sum_{i=1}^\infty M_i$ converge, then for any $\epsilon > 0$, there must exist an $N$(independent of $x$) such that the residue of $\sum_{i=1}^\infty M_i$ will be smaller than $\epsilon$(use the partial sum of $\sum_{i=1}^\infty M_i$ to prove). Then there exists an $N$, $R_n(x) < \epsilon, \forall n > N$.



\begin{example}
	$\sum_{n=1}^\infty cos(nx)/2^n$ will converge unfirmly by using the M series $\sum_{i=1}^\infty 1/2^n$
\end{example}

\section{Properties of uniform convergence}
\subsection{Uniform convergence preserve continuity}
\begin{theorem}
	Let $\{f_n\}$ a sequence of functions which converges uniformly to $f$ on a metric space $M$. If each $f_n$ is continuous at $a\in M$, then $f$ is continuous at $a$.\cite{johnsonbaugh2010foundations}
\end{theorem}
\begin{proof}
	Use triangle inequality $$\abs{f(x)-f(a)}\leq \abs{f(x)-f_N(x)}+\abs{f_N(x)-f_N(a)} + \abs{f_N(a)-f(a)}.$$	
\end{proof}


\begin{remark}
	The implication is exchange of limits:
	$$\lim_{x\to a} \lim_{n\to \infty} f_n(x) = \lim_{n\to \infty} \lim_{x\to a}  f_n(x)$$
\end{remark}


\subsection{exchange limits and integration}
\begin{theorem}[integration term by term]
	\cite[421]{kaplan1973advanced}A uniformly convergent series of continuous functions $\sum_{i=1}^\infty u_i(x)$ can be integrated term by term; that is, if each $u_i(x)$ is continuous for $a<x<b$, and $\sum_{i=1}^\infty u_i(x)$ converges uniformly to $f(x)$, then
	$$\int_a^b f(x) dx = \int_a^b u_1(x) dx + \int_a^b u_2(x) dx + ...$$
\end{theorem}
\begin{proof}
	Let $S_n$ be the partial sum. Because of uniform convergence, for any $\epsilon > 0$, we can find $N$, such that
	$$\abs{S_n(x) - f(x)} < \epsilon/(b-a),\forall x$$
	Then $\abs{\int_a^b f(x) - \int_a^b S_n(x) dx} < \epsilon/(b-a)\times(b-a) =\epsilon$, therefore
	$$\int_a^b f(x) dx\to \int_a^b S(x) dx.$$	
\end{proof}


\subsection{exchange limits and differential}
\begin{theorem}[differential term by term]
	A convergent series can be differentiated term by term, provided that the functions of the series have continuous derivatives and that the series of derivatives is uniformly convergent; that is, if $u'_n(x)$ is continuous, if the series $\sum_{i=1}^\infty u_i(x)$ converge to $f(x)$, and if the series $\sum_{i=1}^\infty u'_i(x)$ convergent uniformly for $a<x<b$, then
	$$f'(x) = \sum_{i=1}^\infty u'_i(x)$$
\end{theorem}

\subsection{linearity of uniform convergence}
\begin{theorem}
	If $\sum_{i=1}^\infty u_i(x)$  and $\sum_{i=1}^\infty v_i(x)$ are uniformly convergent for $x\in [a,b]$, and $g(x),h(x)$ are continuous for $x\in [a,b]$, then the series
	\begin{enumerate}
		\item $\sum_{i=1}^\infty u_i(x) + v_i(x)$
		\item $\sum_{i=1}^\infty h(x) u_i(x)$
		\item $\sum_{i=1}^\infty h(x)u_i(x)+g(x)v_i(x)$
	\end{enumerate}
	are uniformly convergent. 
\end{theorem}
\begin{proof}
	(1)Let $S_n,T_n$ be the partial sums, then
	$$\abs{S_n+T_n - S -T} \leq \abs{S_n-S} + \abs{T_n-T} \to 0$$
	(2) $h(x)$ is bounded. Let $h(x) \leq M$, then
	$$\abs{h(x)S_n(x) - h(x)S(x)} \leq M\abs{S_n(x)-S(x)} \to 0$$
	(3) directly from (1) and (2).	
\end{proof}


\section{Power series}\index{power series}
\subsection{Fundamentals}
\begin{definition}[power series]
	Let $t$ be a fixed real number. A power series expanded about $t$ is an infinite series of the form
	$$\sum_{n=0}^\infty a_n(x-t)^n.$$
	
	Most often, we refers to
	$$\sum_{n=0}^\infty a_nx^n$$
	as a power series.
\end{definition}

\begin{definition}[domain of convergence for a power series]\cite[255]{fitzpatrick2006advanced}
	Given a sequence of real numbers $\{c_k\}$ indexed by the nonnegative integers, we define the \textbf{domain of convergence} of the series $\sum_{k=0}^\infty c_kx^k$ be the set of all numbers $x$ such that the series
	$\sum_{k=0}^\infty c_kx^k$
	converges. 
	
	Denote the domain of convergence by $D$. We can define a function $f:D\to \R$ by
	$$f(x) \triangleq \lim_{n\to \infty} \sum_{k=1}^n c_k x^k = \sum_{k=1}^\infty c_k x^k.$$	
\end{definition}



\begin{theorem}[radius of convergence and uniform convergence]
	\cite[423]{kaplan1973advanced} \hfill
	\begin{itemize}
		\item Every power series $$\sum_{n=0}^\infty c_n(x-a)^n$$ has a radius of convergence of $r^*$ such that the series converges absolutely when $\abs{x-a} < r^*$ and diverges when $\abs{x-a} > r^*$.
		\item If $\sum_{n=0}^\infty c_n(x-a)^n$ converges absolutely at $x_0$, then $\sum_{n=0}^\infty c_n(x-a)^n$ converges.
		\item If $r^* = 0$, then the series only converge for $x=a$; If $r^* = \infty$, then the series converge everywhere. If $0<r^*<\infty$, for any $0<r_1<r^*$, then the series \textbf{converge uniformly} for $\abs{x-a}\leq r_1$. 
	\end{itemize} 
\end{theorem}
\begin{proof}
	(1)Let $M_n = \abs{c_n}r_1^n$, then based on definition of convergence radius
	the series
	$$\sum_{n=1}^\infty M_n = \sum_{n=1}^\infty \abs{c_n(x_1-a)^n},x_1 = a+r_1$$
	will converge. 
	Because $$c_n(x-a)^n \leq \abs{c_n}r_1^n,\forall (x-a)\leq r_1$$
	therefore the series $\sum_{n=0}^\infty c_n(x-a)^n$ converge uniformly based on M-test.
	(2) Directly from the property of absolute convergence implies convergence(\autoref{ch:sequences-series:th:importantseriesconvergenceresult}).
\end{proof}

\begin{remark}[Why we care about power series] We care about power series is because they usually have uniformly convergence, and therefore have many good properties, e.g., integration term-by-term,.... 
\end{remark}


\begin{lemma}[calculation of radius of convergence]\label{ch:function-sequences-series--approximation:th:calculatingConvergenceradiusForPowerSeries}
	\cite[423]{kaplan1973advanced} For the  power series $\sum_{n=0}^\infty c_n(x-a)^n$ the radius of convergence of $r^*$ can be calcuated as
	$$r^* = \lim_{n\to\infty} 1/ \abs{c_{n+1}/c_n}$$ 
	or
	$$r^* = \lim_{n\to\infty} 1/ \abs{c_{n}}^{1/n}$$
\end{lemma}
\begin{proof}
	use root test and ratio test. 
\end{proof}

\begin{corollary}\cite[256]{fitzpatrick2006advanced}\hfill
	\begin{itemize}
		\item The power series 
		$$\sum_{k=0}^\infty k!x^k$$
		has convergence of radius being 0; that is, the power series does not converge for any non-zero $x$.
		\item The power series 
		$$\sum_{k=0}^\infty \frac{x^k}{k!}$$
		has convergence of radius being $\infty$; that is, the power series converges for any non-zero $x$.
	\end{itemize}	
\end{corollary}




\subsection{Term-by-term operation}
\begin{theorem}[uniform convergent power series term-by-term operation properties]\label{ch:function-sequences-series--approximation:th:uniformconvergentpowerseriestermbytermproperties}
	\cite[424]{kaplan1973advanced}A power series with radius of convergence $r^*$ has the following properties:
	\begin{itemize}
		\item A power series represents a continuous function $f(x) = \sum_{n=0}^\infty c_n(x-a)^n$in $\abs{x-a} < r^*$.
		\item A power series can be integrated term by term, i.e.
		$$\int_{x_1}^{x_2} \sum_{n=0}^\infty c_n(x-a)^n$$
		for $a-r^*<x_1<x_2<a+r^*$.
		\item A power series can be differentiated term by term,i.e., 
		$f(x) = \sum_{n=0}^\infty c_n(x-a)^n, \abs{x-a}<r^*$
		then
		$f(x) = \sum_{n=0}^\infty nc_n(x-a)^{n-1},\abs{x-a}<r^*$
	\end{itemize}
\end{theorem}
\begin{proof}
	directly from the consequence of the uniformly convergence within radius of convergence. Note that when differentiated, the radius of convergence will not change.
\end{proof}


\begin{example}[application of term by term differentiation]
	From $$\frac{1}{1-x} = \sum_{n=0}^\infty x^n, \abs{x}<1$$
	we can differentiate both sides, and get
	$$\frac{1}{(1-x)^2} = \sum_{n=0}^\infty (n+1) x^n, \abs{x}<1.$$
\end{example}


\subsection{Power series and analytic function}\index{analytic function}
\begin{definition}[analytical function]\cite[285]{strichartz2000way}
	\textbf{Analytical functions} are \textbf{infinitely differentiable} functions such that the Taylor series at any point $x_0$ in its domain 
	$$T(x) = \sum_{n=0}^\infty \frac{f^{(n)}}{n!}(x-x_0)^n$$
	converges to $f(x)$ in a neighborhood of $x_0$ pointwise.
\end{definition}

\begin{example}
	Common analytical functions are:
	\begin{itemize}
		\item polynomials
		\item trigonometric functions
		\item exponential functions
		\item logarithm and power functions
	\end{itemize}
\end{example}


\begin{example}
	Common non-analytical functions are:
	\begin{itemize}
		\item absolute value function
		\item piecewise functions (due to the meeting point of different pieces)
	\end{itemize}
\end{example}

\begin{example}[A nonanalytic, infinitely differentiable function]\cite[221]{fitzpatrick2006advanced} 
	Define 
	$$f(x) = \begin{cases*}
	\exp(-1/x^2), if~ x\neq 0\\
	0, if~ x = 0.
	\end{cases*}$$	
	Then the function $f:\R\to \R$ has derivatives of all orders. However, the only at which 
	$$f(x) = \sum_{n=0}^\infty \frac{f^{(n)}}{n!}(x-x_0)^n$$
	is at $x = 0.$
\end{example}


\subsection{Approximation by polynomials}\index{polynomial}\index{Lagrange polynomial}
\begin{lemma}[Approximate finite set of data]\label{ch:functional-analysis:th:Lagrangepolynomialapproximation}
	\cite[297]{strichartz2000way}Given a set of data $\{(x_1,a_1),(x_2,a_2),...,(x_k,a_k)\}$(assuming the data are consistent, i.e., no same $x$ with different $a$), then there exist a polynomial of degree $k-1$ that passing all the points. This polynomial is given as
	$$P(x) = \sum_{i=1}^k a_i Q_i(x)$$
	where $$Q_i(x) = \frac{\prod_{j\neq i}(x-x_j)}{\prod_{j\neq i}(x_i-x_j)}$$
	This polynomial is known as Lagrange polynomial.
\end{lemma}
\begin{proof}
	Notice that $Q_i(x_j) = \delta_{ij}$
\end{proof}


\begin{remark}[implications]\hfill
	\begin{itemize}
		\item The fitting error is 0 at existing data points; However, the polynomial can have terriable prediction error at new $x$. For example, given a data set of size $m$ generated by linear functions. A $m-1$ polynomial can achieve perfect fit.
		\item If the degree number of polynomial chosen is smaller than the data number, then perfect fit might not be achieved, but might have better prediction. This is an example of bias-variance trade-off. 
	\end{itemize}
\end{remark}


\begin{lemma}[Approximation accuracy]\cite[135]{atkinson1989introduction}
	Let $x_0,x_1,...,x_n$ be distinct real numbers, and let $f$ be a given real-valued function with $n+1$ continuous derivatives on $H(t,x_0,...,x_n)$(denotes the smallest interval containing $t,x_0,...,x_n$). Then there exists $\xi \in I$ such that
	$$f(t) - \sum_{i=1}^{n} f(x_i) Q_i(t) = \frac{(t-x_0)\cdots (t-x_n)}{(n+1)!}f^{(n+1)}(\xi).$$ 
\end{lemma}


\begin{theorem}[Weierstrass Approximation Theorem]\index{Weierstrass Approximation Theorem}\index{approximation}
	\cite[301]{strichartz2000way}Let $f$ be any continuous function on a compact interval $[a,b]$. Then there exists a sequence of polynomials converging \textbf{uniformly} to $f$ on $[a,b]$.
\end{theorem}

\section{Taylor polynomial and Taylor series}
\subsection{Taylor polynomial and approximation}

\begin{definition}[order of contact of two functions]\cite[200]{fitzpatrick2006advanced}
	Let $I$ be a neighborhood of the point $x_0$. Two functions $f:I\to \R$ and $g:I\to \R$ are said to \textbf{have contact of order 0} at $x_0$ provided that $f(x_0) = g(x_0)$.
	
	In particular, the function $f$ and $g$ are said to \textbf{have contact of order $n$} at $x_0$ if	
	$$f^{(k)}(x_0) = g^{(k)}(x_0), \forall 0\leq k\leq n.$$
\end{definition}


\begin{definition}[Taylor polynomial]
	A nth degree \textbf{Taylor polynomial} $P_n$ for $f$ at $c$ is given as
	$$P_n(x) = f(c) + f^{(1)}(c)(x-c) + \frac{f^{(2)}}{2!}(x-c)^2+...+\frac{f^{(n)}}{n!}(x-c)^n.$$
\end{definition}

\begin{lemma}[existence of unique Taylor polynomial with contact]\cite[200]{fitzpatrick2006advanced}
	Let $I$ be an open interval and let $n$ be a nonnegative integer and suppose that the function $f:I\to \R$ has $n$ derivatives.
	
	Then there is a unique polynomial of degree at most $n$ that has contact of order $n$ with $f$. This polynomial is given by
	$$p_n(x) = f(x_0) + f'(x_0)(x-x_0)+\cdots + \frac{f^{(k)}}{k!}(x-x_0)^k+\cdots \frac{f^{(n)}}{n!}(x-x_0)^n.$$ 	
\end{lemma}


\begin{theorem}[Lagrange Remainder Theorem, Taylor's theorem]\index{Lagrange Remainder Theorem}\index{Taylor's theorem}\cite[186]{johnsonbaugh2010foundations}\cite[203]{fitzpatrick2006advanced}\label{ch:function-sequences-series--approximation:th:LagrangeRemainderTheorem}
	Let $I$ be an open interval and let $n$ be a nonnegative integer and suppose that the function $f:I\to \R$ has $n+1$ derivatives.
	\begin{itemize}
		\item consider a special $f$ such that at the point $x_0$ in $I$,
		$$f^{(k)}(x_0) = 0 , \forall 0\leq k\leq n. $$
		
		Then for each point $x\neq x_0$ in $I$, there is a point $c$ strictly between $x$ and $x_0$ which
		$$f(x) =\frac{f^{(n+1)}(c)}{(n+1)!}(x-x_0)^{n+1}. $$
		
		\item For general $f$, for each point $x\neq x_0$ in $I$, there is a point $c$ strictly between $x$ and $x_0$ such that
		$$f(x) = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k + \frac{f^{(n+1)}(c)}{(n+1)!}(x-x_0)^{n+1}.$$ 
	\end{itemize}
\end{theorem}
\begin{proof}
	(1) Define $g(x) = (x-x_0)^{n+1}$ such that $g(x_0) = g^{(1)}(x_0) = g^{(2)}(x_0) ...= g^{(n)}(x_0) = 0$. 
	Based on Cauchy mean value theorem(\autoref{ch:calculus:th:meanValueTheorem}), we know that there exists a $x_1$ between $x_0$ and $x$ such that
	$$\frac{f(x)-f(x_0)}{g(x)-g(x_0)} = \frac{f(x)}{g(x)} =\frac{f^{(1)}(x_1)}{g^{(1)}(x_1)}.$$
	Similarly, use the mean value theorem again, there exists a $x_2$ between $x_0$ and $x_1$ such that
	$$\frac{f^{(1)}(x_1)-f^{(1)}(x_0)}{g^{(1)}(x_1)-g^{(1)}(x_0)} = \frac{f^{(1)}(x_1)}{g^{(1)}(x_1)} =\frac{f^{(2)}(x_2)}{g^{(2)}(x_2)}.$$
	Continue this process, we will get
	$$\frac{f(x)}{g(x)} =\frac{f^{(n+1)}(x_{n+1})}{g^{(n+1)}(x_{n+1})} =\frac{f^{(n+1)}(x_{n+1})}{(n+1)!}.$$
	and finally
	$$f(x) = g(x)\frac{f^{(n+1)}(x_{n+1})}{(n+1)!}$$
	(2) For general $f$, construct the function $F = f- p_n$ and use (1).
	
\end{proof}


\begin{theorem}[Cauchy integral Remainder Theorem]\index{Cauchy integral Remainder Theorem}\cite[216]{fitzpatrick2006advanced}
	Let $I$ be an open interval and let $n$ be a nonnegative integer and suppose that the function $f:I\to \R$ has $n+1$ derivatives. Then for each point $x\neq x_0$ in $I$, there is a point $c$ strictly between $x$ and $x_0$ such that
	$$f(x) = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k + \frac{f^{(n+1)}}{(n+1)!}(x-x_0)^{n+1}.$$ 	
\end{theorem}
\begin{proof}
	By the first  fundamental theorem(\autoref{ch:calculus:th:FirstFundamentalTheoremOfCalculus}),we have
	$$f(x) = f(x_0) + \int_{x_0}^x f'(t)dt.$$
	Integrating by parts, we have
	\begin{align*}
	\int_{x_0}^x f'(t)dt &= - \int_{x_0}^x f'(t)\frac{d}{dt}(x-t)dt \\
	&= -f'(t)(x-t)|^{t=x}_{t=x_0} +  \int_{x_0}^x f''(t)(x-t)dt \\
	&= f'(x_0)(x-x_0) + \int_{x_0}^x f''(t)(x-t)dt.
	\end{align*}	
	More generally, we have
	\begin{align*}
	\frac{1}{k!}\int_{x_0}^x f^{(k+1)}(t)(x-t)^k dt &= -\frac{1}{(k+1)!} \int_{x_0}^x f^{(k+1)}(t)\frac{d}{dt}(x-t)^{k+1}dt \\
	&= \frac{1}{(k+1)!} f^{(k+1)}(x_0)(x-x_0)^{k+1} + \frac{1}{(k+1)!} \int_{x_0}^x f^{(k+2)}(t)\frac{d}{dt}(x-t)^{k+1}dt.
	\end{align*}	
	
\end{proof}

\subsection{Taylor series and Taylor's theorem}
\begin{definition}[Taylor series]\index{Taylor series}
	\cite[428]{kaplan1973advanced}
	We denote
	$$f(x) = \sum_{n=0}^\infty c_n(x-a)^n, \abs{x-a} < r^*$$
	as the \emph{Taylor series} of $f(x)$ at $x=a$ if the coefficients $x_n$ are given by the rule
	$$c_0 = f(a),c_1 = f'(a),c_2 = \frac{f''(a)}{2!} ...$$
\end{definition}


\begin{lemma}[convergent power series is its own Taylor series]
	Every power series with nonzero convergence radius is the Taylor series of its sum.
\end{lemma}
\begin{proof}
	Because we can differentiate term by term, we can verify
	$$c_0 = f(a),c_1 = f'(a),c_2 = \frac{f''(a)}{2!} ...$$
\end{proof}



\begin{lemma}[uniqueness of coefficients]
	\cite[429]{kaplan1973advanced}
	If two power series
	$$\sum_{n=0}^\infty c_n(x-a)^n,\sum_{n=0}^\infty C_n(x-a)^n$$
	have non-zero convergence radii and have equal sums wherever both series converge, then
	$$c_n = C_n, n=0,1,2,...$$
\end{lemma}
\begin{proof}
	By assumption
	$$\sum_{n=0}^\infty c_n(x-a)^n=\sum_{n=0}^\infty C_n(x-a)^n = f(x)$$
	then
	$$f^{(n)}(a)/n! = c_n = C_n$$
\end{proof}

\begin{theorem}[convergence of Taylor series]
	Let $I$ be a neighborhood of the point $x_0$ and suppose that the function $f:I\to \R$ has derivatives of all orders. Suppose also that there are positive numbers $r$ and $M$ such that the interval $[x_0-r,x_0+r]$ contained in $I$ and that
	$$\abs{f^{(n)}(x)}\leq M^n, \forall x\in [x_0-r,x_0+r].$$
	
	Then 
	\begin{itemize}
		\item The Taylor polynomial
		$$\lim_{n\to \infty}p_n(x) =\lim_{n\to \infty} \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k$$
		will converge absolutely.
		\item 
		$$\lim_{n\to\infty} p_n(x) =\sum_{k=0}^\infty \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k = f(x),, \forall x\in [x_0-r,x_0+r].$$
	\end{itemize}
\end{theorem}
\begin{proof}
	(1)
	Note that
	$$\abs{\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k} \leq \frac{M^kr^k}{k!},$$
	and the series $\sum_{k=1}^\infty (Mr)^k/k!$ converges(\autoref{ch:sequences-series:th:commonSeriesConvergence}).Then based on comparison test(\autoref{ch:sequences-series:th:ConvergenceComparisonTestForSeries}), the series $sum_{k=0}^\infty \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k$	will converge absolutely.
	
	(2) To show that the limit is $f(x)$, from Lagrange Remainder Theorem(\autoref{ch:function-sequences-series--approximation:th:LagrangeRemainderTheorem}), we have
	$$\abs{f(x) - p_n(x)} = \abs{\frac{f^{(n+1)}(c)}{(n+1)!}(x-x_0)^{n+1}}\leq \frac{(Mr)^{n+1}}{(n+1)!} , $$
	where $c$ is between $x$ and $x_0$. Note that for all $ x\in [x_0-r,x_0+r]$, we can make the right hand side as small as we want when $n$ is sufficiently large. Therefore, $\lim_{n\to\infty} p_n(x) = f(x)$.
\end{proof}


\begin{example}
	The Taylor expansion of $e^x$ and $\cos(x)$ are given by
	$$e^x = \sum_{k=0}^\infty \frac{x^k}{k!}, \cos(x) = \sum_{k=0}^\infty \frac{(-1)^kx^k}{(2k)!}x^{2k},$$
	with convergence domain being the whole $\R$. This is because for $x\in \R$, $f^{(k)}(x_0 = 0) \leq 1$.	
\end{example}


\subsection{Common Taylor series}
\begin{lemma}[common Taylor series]\label{ch:functional-analysis:th:commonTaylorSeries}
	\begin{align*}
	\frac{1}{1-x} &= \sum_{n=0}^\infty x^n = 1 + x + x^2 + x^3 + ... && R=1 \\
	e^x &= \sum_{n=0}^\infty \frac{x^n}{n!} = 1 + \frac{x}{1!} + \frac{x^2}{2!} + \frac{x^3}{3!} + ... && R=\infty \\
	\sin x &=\sum_{n=0}^\infty (-1)^n \frac{x^{2n+1}}{(2n+1)!} = x - \frac{x^3}{3!} + \frac{x^5}{5!} + ... && R=\infty \\
	\cos x &=\sum_{n=0}^\infty (-1)^n \frac{x^{2n}}{(2n)!} = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} + ... && R=\infty \\
	\tan^{-1} x &=\sum_{n=0}^\infty (-1)^n \frac{x^{2n+1}}{(2n+1)} = x - \frac{x^3}{3} + \frac{x^5}{5} + ... && R=\infty \\
	\ln (1 + x) &=\sum_{n=1}^\infty (-1)^{n-1} \frac{x^{n}}{n} = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + ... && R=1 \\
	[binomial~series](1 + x)^k &=\sum_{n=0}^\infty \binom{k}{n}x^n = 1 +kx + \frac{k(k-1)}{2!}x^2 + \frac{k(k-1)(k-2)}{3!} +... && R=1 \\
	\sqrt{1+x} &= 1 + \frac{1}{2}x + \frac{x^2}{2!}(-\frac{1}{4})+\frac{x^3}{3!}(-\frac{3}{8}) + ... && R=1
	\end{align*}
\end{lemma}

\begin{remark}[How convergent radius are determined?]Note that the Taylor series in nature is power series. The convergence radius can be determined using ratio test for power series on $$\frac{\abs{a_{n+1}}}{\abs{a_n}},$$
	as given by \autoref{ch:function-sequences-series--approximation:th:calculatingConvergenceradiusForPowerSeries}.
\end{remark}

\begin{remark}[binomial series vs binomial theorem]\hfill
	\begin{itemize}
		\item If $k$ is a positive integer, then binomial series reduce to binomial theorem, since all later terms $n > k$ are zero.
		\item If $k$ is a positive integer, then the convergence radius is $\infty$.
	\end{itemize}
\end{remark}


\begin{corollary}\hfill
	\begin{itemize}
		\item $$e = \sum_{n=0}^\infty \frac{1}{n!} = 1+ \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + ...$$
		\item $$\lim_{n\to \infty} \frac{x^n}{n!} = 0, \forall x\in \R$$
		\item 
		$$\ln x =  \sum_{n=1}^\infty (-1)^{n-1} \frac{(x-1)^{n}}{n} $$
	\end{itemize}
\end{corollary}
\begin{proof}
	(2) because $e^x$ is convergent for all $x\in \R$. From \autoref{ch:sequences-series:th:importantseriesconvergenceresult} , we know that 
	if the $\sum_{n=1}^\infty a_n$ converges, then $\lim_{n\rightarrow \infty}a_n = 0$.
\end{proof}

\begin{example}[some conversion techniques]\hfill
	\begin{itemize}
		\item $$\frac{1}{\sqrt{4-x}} = \frac{1}{\sqrt{4(1-\frac{x}{4})}} = \frac{1}{2}(1 - \frac{x}{4})^{-1/2}$$
		\item $$e^{-x^2} = \sum_{n=0}^\infty \frac{(-x^2)^n}{n!}$$
		\item Expand $\sqrt{x}$ at $x_0$ as
		$$\sqrt{x - x_0 + x_0} = \sqrt{x_0}\sqrt{1 + \frac{x-x_0}{x_0}}$$
	\end{itemize}
	
\end{example}



\subsection{Useful approximations}
\begin{theorem}
	Let $S\subset \R^n,s\in \R^n$, and suppose that $f:S\to \R$ is continuously differentiable and $g = \nabla f$ is Lipschitz continuous at $x$ with Lipschitz constant $\gamma(x)$ for some appropriate vector norm. It follows that if the segment $x+\theta s \in S$ for all $\theta \in [0,1]$, then
	$$\abs{f(x+s) - m^L(x+s)} \leq \frac{1}{2} \gamma (x)\norm{s}^2$$
	where $m^L(x+s) = f(x) + g(x)^Ts$
\end{theorem}
\begin{proof}
	\begin{align*}
	\abs{f(x+s) - m^L(x+s)} &= \abs{f(x+s)-f(x) - g(x)^Ts}\\
	&=\abs{g(x')^Ts - g(x)^Ts} \leq \gamma(x) \abs{x' - x} 
	\end{align*}
	
\end{proof}


\begin{theorem}
	Let $S\subset \R^n,s\in \R^n$, and suppose that $f:S\to \R$ is twice continuously differentiable and $H = \nabla^2 f$ is Lipschitz continuous at $x$ with Lipschitz constant $\gamma(x)$ for some appropriate vector norm. It follows that if the segment $x+\theta s \in S$ for all $\theta \in [0,1]$, then
	$$\abs{f(x+s) - m^Q(x+s)} \leq \frac{1}{6} \gamma (x)\norm{s}^3$$
	where $m^Q(x+s) = f(x) + g(x)^Ts + \frac{1}{2}s^THs$
\end{theorem}
\begin{proof}
	
\end{proof}






\section{Notes on bibliography}


For approximation theory in Hilbert space, see \cite{moon2000mathematical}. For comprehensive treatment of Fourier series, see\cite{kaplan1973advanced}\cite{stein2011fourier}
For other general orthogonal functions, see \cite{moon2000mathematical}\cite{kaplan1973advanced}.

For various convergence result for Fourier series, see \cite{wiki:fourierconvergence}.

A good book on general approximation theory and method, see \cite{powell1981approximation}.

For excellent treatment in orthogonal functions and expansions, see\cite{sansone2004orthogonal}, including special functions of Fourier series, Legendre poynomials, spherical harmonics, Laguerre and Hermite Series. 

An excellent treatment on Hibert space and its applications, see \cite{debnath2005hilbert}\cite{holland2007applied}\cite{duren2012invitation}

For treatment on Fourier transform, see \cite{holland2007applied}.

For treatment on interpolation theory, see \cite{atkinson1989introduction}.

\printbibliography

\end{refsection}
\begin{refsection}
\startcontents[chapters]	
\chapter{Functional Analysis}\label{ch:functional-analysis}
%\minitoc

\printcontents[chapters]{}{1}{}

\section{Normed vector space}\label{ch:functional-analysis:sec:normed-vector-space}
\subsection{Basic properties}
\begin{definition}[normed vector space]
A vector space equipped with a norm is a normed vector space. Let $V$ be a vector space over the field $\mathbb{F}$. A norm of $V$ is a function $\norm{\cdot}: V\rightarrow  \R^+$ that satisfies the following conditions:
\begin{enumerate}
    \item positive-definiteness: $\norm{v}\geq 0, \forall v \in V$ with equality iff $v=0$
    \item Homogeneity: $\norm{aV}=\abs{a}\norm{v},\forall v\in V,\forall a\in \mathbb{F}$
    \item Triangle inequality: $\norm{u+v} \leq \norm{v} + \norm{u},\forall u,v\in V$
\end{enumerate}
\end{definition}


\begin{lemma}[normed as a metric]\label{ch:functional-analysis:th:normAsmetric}
Let $\norm{\cdot}$ denote the norm of a normed vector space. Then we can define a metric $d$ via
$$d(x,y) = \norm{x-y}.$$	
\end{lemma}
\begin{proof}
We can verify $d(x,y)$ satisfies
\begin{enumerate}
	\item $d(p,q) > 0 ~\text{if}~ p \neq q;$
	\item  $d(p,q)=0$ if and only if $p=q$;
	\item $d(p,q) = d(q,p);$
	\item (triangle inequality) we have
	\begin{align*}
	d(x,z) &= \norm{x-z} \\
	 		&= \norm{x - y + y-z} \\
	 		&\leq \norm{x - y} + \norm{y-z} \\
	 		& = d(x,y) + d(y,z)
	\end{align*}
\end{enumerate}		
\end{proof}

\begin{note}[inner product, norm, and metric]\hfill
\begin{itemize}
	\item Let $\norm{\cdot}$ denote the norm, we can define a metric via
	$$d(x,y) = \norm{x-y}.$$
	Such metric will turn a normed vector space into more general metric space.
    \item Every inner product on a vector space induces a norm that can be used to measure the 'length' of the a vector(\autoref{ch:functional-analysis:th:innerProductAsInducedNorm}). 
    \item Note that a norm can be simply defined using the inner product; but it can also be defined as other mapping as long as it satisfies above conditions.    
\end{itemize}
\end{note}

\begin{example}\hfill
\begin{itemize}
    \item norm in $\R^n$ vector space includes 1-norm, $\infty$-norm, $p$-norm. They are given by 
    $$\norm{x}_p = (\sum_{i=1}^n \abs{x_i}^p)^{1/p}$$
    \item For vector space of matrix $M_{m\times n}(\R)$, Frobenius norm
    \item Let $p \geq 1$ be a real number. The space $L^p([a,b])$ is normed vector space with $L^p-$ norm given by
    $$\norm{f}_p = (\int_a^b \abs{f(t)}^p dt)^{1/p}$$
\end{itemize} 
\end{example}


\begin{lemma}[triangle inequality minus sign]
In a normed linear space $$\abs{\norm{x}-\norm{y}}\leq \norm{x-y},$$that is, 
$$\norm{x}-\norm{y}\leq \norm{x-y},\norm{y}-\norm{x}\leq \norm{x-y}$$
and
$$\norm{x}-\norm{y}\geq -\norm{x-y}$$
for any two vectors $x,y$.


\end{lemma}
\begin{proof}
use triangle inequality for vector $x-y$ and $y$.
\end{proof}

\begin{theorem}[continuity of norm ]
The norm as a function in a normed vector space $E$ is a continuous function; that is, let $\{x_n\}\in E$ and 
$$\norm{x_n-x}\to 0 $$
implies
$$\abs{\norm{x}-\norm{x_n}} = 0$$
\end{theorem}
\begin{proof}
$$\abs{\norm{x}-\norm{x_n}} \leq \norm{x-x_n} \to 0$$
\end{proof}

\begin{remark}[intuition]\hfill
\begin{itemize}
	\item 
	\item In other words, if $y$ and $x$ are sufficiently close, then $\norm{y}$ and $\norm{x}$ are also sufficiently close.
\end{itemize}	
	
\end{remark}


\subsection{Equivalence of norms}
\begin{definition}[equivalent norms]\cite[75]{kreyszig1989introductory}\index{equivalent norm}
	cA norm $\norm{\cdot}_p$ on a vector space $X$ is said to be \textbf{equivalent} to a norm $\norm{\cdot}_q$ on $X$ if there are positive numbers $a$ and $b$ such that for all $x\in X$, we have
	$$a\norm{x}_q \leq a\norm{x}_p \leq a\norm{x}_q.$$
\end{definition}

\begin{remark}[equivalent relation]
	It can be easily showed that equivalent norms satisfies three properties: reflective, transitive and symmetric relation(we can scale $p$ norm properly such that $q$ norm will be squeezed instead.).
\end{remark}


\begin{theorem}[equivalent norms]\cite[75]{kreyszig1989introductory}
On a \textbf{finite} dimension vector space $X$, any norm is equivalent to any other norm. 	
\end{theorem}

\begin{remark}[interpretations]\hfill
\begin{itemize}
	\item This theorem only holds for finite dimensional vector space, and might not hold for infinite dimensional space.
	\item This theorem implies that convergence or divergence of a sequence in a finite dimensional vector space does not depend on the particular choice of a norm on that space. 
\end{itemize}
\end{remark}



\begin{lemma}[useful norm inequalities]
Let $V$ be a $n$ dimensional normed space. Then, for different norms, we have
\begin{itemize}
	\item $$\frac{1}{\sqrt{n}} \norm{x}_1 \leq \norm{x}_2  \leq \norm{x}_1  $$
	\item $$\frac{1}{\sqrt{n}} \norm{x}_2  \leq \norm{A}_\infty  .$$
	\item More generally, for $0 < p < q$, 
	$$\norm{x}_p \geq \norm{x}_q.$$
\end{itemize}
\end{lemma}
\begin{proof}
(1) 
$$\sqrt{x_1^2 + x_2^2 + ... + x_n^2} \leq \sum_{i=1}^n \abs{x_i}$$
can be showed by squaring both sides. 
To show 
$$\frac{1}{n}\norm{x}_1^2 \leq \norm{x}_2^2, $$
we use the fact that
$\norm{x}_1^2 = x^TJx/n$, $\norm{x}_2^2 = x^TIx$, where $J$ is the all 1 matrix and $I$ is the identity matrix. It can be showed that 
$$\frac{1}{n}J, I, (\frac{1}{n}J - I),$$
are all symmetric and idempotent(i.e. they are orthogonal projection matrix.). Then $$x^T\frac{J}{n}x - x^TIx \geq 0$$
since an orthogonal projection matrix is semi-positive definite. \autoref{ch:linearalgebra:th:spectralpropertyorthogonalprojector}.

(2)(3) need to use Holder's inequality.

A good reference is \url{https://math.stackexchange.com/questions/218046/relations-between-p-norms}.
\end{proof}


\subsection{Sequence and series in normed space}

\begin{definition}[convergence in normed space]\cite[32]{christensen2010functions}
A sequence $\{v_k\}$ in a normed vector space $V$ converges to $p\in V$ if for any $\epsilon > 0$, there exists an $N$ such that for all $k > N$
$$\norm{v_k - p} < \epsilon.$$	
\end{definition}

\subsection{Topology in normed vector space}
\subsubsection{Closure and dense subset}
\begin{definition}[closure]\index{closure}
\cite[15]{debnath2005hilbert}
Let $S$ be a subset of a normed space $E$. By the closure of $S$,denoted as $cl(S)$ we mean the interactions of all closed sets containing $S$. In otherwise, $cl(S)$ is the smallest closed set containing $S$.
\end{definition}



\subsubsection{Dense subset and approximation}\label{ch:functional-analysis:sec:denseSubsetAndApproximation}

\begin{definition}[dense subset]\index{dense subset}
A subset $S$ of a normed space $E$ is called \textbf{dense} if $cl(S) = S$.
\end{definition}

\begin{theorem}[characterization by sequence]
\cite[15]{debnath2005hilbert}Let $S$ be a subset of a normed space $E$, then $S$ is dense in $E$ if and only if for every $x\in E$, there exists $x_1,x_2,...\in S$ such that $x_n\to x$. 
\end{theorem}
\begin{proof}
directly from the definition and closed sets.
\end{proof}

\begin{theorem}[characterization by neighborhood]
\cite[15]{debnath2005hilbert}Let $S$ be a subset of a normed space $E$, then $S$ is dense in $E$ if and only if every non-empty open subsets in $E$ contains an element in $S$ 
\end{theorem}
\begin{proof}
Because every element in $E$ can be approximated by sequence in $S$, therefore every neighborhood of $x\in E$ must contains at least one element in $S$.
\end{proof}

\begin{remark}
\textbf{dense subset is a critical concept in approximation theory}, for example, if we can have a subset $S$ for $E$, then we can approximate any element in $E$ using elements in $S$.
\end{remark}

\begin{example}
The set of all polynomials $\cP[a,b]$ is dense in $C[a,b]$(continuous function), which means any continuous function on $[a,b]$ can be approximate by polynomials.
\end{example}

\begin{theorem}[dense set from complete orthonormal sequence]
\cite[110]{debnath2005hilbert} If $\{x_n\}$ is a complete orthonormal sequence in an inner product space $E$, then the set
$$S=span\{x_1,...,x_n,...\} = =\{\sum_{k-1}^\infty a_k x_k,a_k \in \C\}$$
is dense in $E$.
\end{theorem}
\begin{proof}
Because every element in $E$ can be approximated by a element in $S$ based on the definition of completeness.
\end{proof}






\section{Banach space}
\subsection{Completeness}
\begin{definition}[complete]\cite[48]{christensen2010functions}
A normed vector space $V$ is \textbf{complete} if the following two requirements are satisfied:
\begin{itemize}
	\item Every Cauchy sequence of elements in $V$ must be convergent.
	\item The limit of the Cauchy sequence must belong to $V$.
\end{itemize}

A complete normed space is called \textbf{Banach space}. 
\end{definition}





\begin{note}[normed vector space vs. Banach space for sequences]\cite[48]{christensen2010functions}
\begin{itemize}
	\item In any normed vector space, a convergent sequence is automatically a Cauchy sequence.
	\item However, for some normed vector space, there exists nonconvergent Cauchy sequence.
	\item In a Banach space, a sequence is convergent if and only if it is Cauchy. 
\end{itemize}	
\end{note}




\begin{theorem}[closedness and completeness of finite dimensional normed space]\cite[74]{kreyszig1989introductory}\label{ch:functional-analysis:th:ClosednessAndCompletenessOfFiniteDimensionalNormedSpace}
	Consider any  finite dimensional subspace $Y$ of a normed space $X$. It follows that
\begin{itemize}
	\item If $X$ is finite dimensional, then $X$ is complete. 
	\item $Y$ is closed in $X$ .
\end{itemize}	
\end{theorem}
\begin{proof}
see \href{https://math.stackexchange.com/questions/168275/proof-that-every-finite-dimensional-normed-vector-space-is-complete}{link}
\end{proof}


\begin{lemma}
	In a Banach space, a subset is complete if and only if it is closed.
\end{lemma}

Proof: If a subset is closed, then a convergent sequence (hence Cauchy sequence) in it will have a limit point, therefore it is complete. If a subset if complete, then every Cauchy sequence in it will have a limit in it, i.e., every Cauchy sequence will converge to this limit, therefore every convergent sequence will have its limit in it, therefore the subset is closed.

\begin{remark}
	The Banach space itself is always closed by definition of closed set. See metric space for details.
\end{remark}


\subsection{Common Banach spaces}
\subsubsection{$\R^n$}

\begin{theorem}\label{ch:functional-analysis:th:RnisBanachSpace}\cite[48]{christensen2010functions}
	 The space $\R^n$ with Euclidean norm is a Banach spaces. 
\end{theorem}
\begin{proof}
Note we use the Euclidean norm as the metric(\autoref{ch:functional-analysis:th:normAsmetric}), we can use the result that $\R^n$ is a complete metric space(\autoref{ch:metric-space:th:CompletenessOfRnWithEuclideanMetric}), which implies that every Cauchy sequence in $\R^n$ converges to a limit in $\R^n$. Further $\R^n$ is a vector space and therefore, $\R^n$ is a Banach space.
\end{proof}

\begin{remark}
Note that any proper closed subsets of $\R^n$ are complete metric space; but they are not Banach space because these subsets are not vector space.	
\end{remark}


\subsubsection{$l^p$ spaces}

\begin{theorem}
\cite[50]{christensen2010functions} The space $l^p(\N),1\leq p < \infty$ are Banach spaces. 
\end{theorem}




\subsubsection{$L^p$ spaces}


\begin{theorem}\cite[95]{christensen2010functions}
	 The space $L^p,L^p[0,1],1\leq p \leq \infty$ are Banach spaces. 
\end{theorem}


\subsubsection{$C[a,b]$ spaces}

\begin{lemma}
	\cite[35]{luenberger1969optimization}
	The space $C[a,b]$ of continuous functions on $[a,b]$ is a Banach space.
\end{lemma}
\begin{proof}
We need to prove every Cauchy sequence converge to an element in $C[a,b]$. Let $\{x_n\}$ be a Cauchy sequence in $C[a,b]$. Let $t\in [a,b]$, then $\abs{x_n(t) - x_m(t)} \leq \norm{x_n-x_m} \to 0$, therefore, $\{x_n(t)\}$ converges pointwise to $x(t) \in \R$.(because $\R$ is complete, and every Cauchy sequence will converge) 

Now we show that the convergence is uniform: $$\abs{x_n(t) - x(t)} \leq \abs{x_n(t) - x_m(t)} + \abs{x_m(t) - x(t)} \leq \norm{x_n-x_m} + \abs{x_m(t) - x(t)} \to 0$$
since for a Cauchy sequence, any $m > N$, including $m=\infty$ is legitimate, therefore the first term is bounded and the second term can make to arbitrarily small.

\textbf{Because uniform convergence preserve continuity}, since $x_m(t)$ is continuous, therefore the limit $x(t)$ is continuous, therefore $x \in C[a,b]$.	
\end{proof}





\begin{remark}[motivations for Banach space]\cite[35]{luenberger1969optimization}
The principal advantage of Banach space in optimization problem is that when seeking an optimal vector, we often construct a sequence of vectors, the desired optimal vector is the limit of such sequence. If the underlying space is complete, then we can use Cauchy criterion to test the convergence of the sequence.	
\end{remark}
 





\subsection{Contraction mapping and fixed point theorems}\label{ch:functional-analysis:sec:contraction-mapping-and-fixed-point-theorems}
\subsubsection{Definitions}

\begin{definition}[Contraction mapping]\index{contraction mapping}
	\cite[121]{griffel2002applied}\cite[29]{debnath2005hilbert}
	An operator/mapping $f:X\rightarrow X$ is called a \emph{contraction mapping} if there exists a constant $\beta,0<\beta < 1$, such that
	$$\norm{f(x)-f(x_0)} \leq \beta \norm{x-x_0}$$
\end{definition}


\begin{lemma}[continuity of contraction mappings]
	Contraction mappings are continuous.
\end{lemma}
\begin{proof}
	$\norm{f(x)-f(x_0)} \leq \norm{x-x_0} \to 0$
\end{proof}


\begin{remark}\hfill
	\begin{itemize}
		\item \textbf{contraction mappings need not be linear mapping.}
		\item we can usually use bounds on first-order derivative and Lipschitz constant to check whether a mapping is contraction mapping.
	\end{itemize}
\end{remark}

\begin{lemma}
	For a continuous differentiable function $g$ defined on $[a,b]$, if $\abs{g'(x)} < 1$, then $g$ is a contraction mapping.
\end{lemma}
\begin{proof}
	$\abs{g(x)-g(y)} \leq \abs{g'(x')}\abs{x-y}$
\end{proof}




\begin{definition}[Fixed point]\index{fixed point}
	$x$ is a fixed point of $f:X\rightarrow X$ if  $f(x)=x$.
\end{definition}


\subsubsection{Banach fixed point theorem}
\begin{theorem}[Banach fixed point theorem]\label{ch:functional-analysis:th:BanachFixedPointTheorem}\index{fixed point theorem}
	\cite{carter2001foundations}\cite[122]{griffel2002applied}\cite[771]{bertsekas2016nonlinear}Let $f: X\rightarrow X$ be a contraction mapping with contraction parameter $\beta < 1$ defined on a closed subset $X$ of a Banach space. Then there exists a unique fixed point $x$; That is, for any $x_0 \in X$, the sequence $\{x_n\}$ generated by $x_{n+1} = f(x_n)$ converges to $x$.
\end{theorem}
\begin{proof}
	Select an arbitrary $x_0 \in X$. Define the sequence $\{x_n\}$ by $x_{n+1} = f(x_{n})$. Denote $\rho$ as the distance metric function. We have
	\begin{itemize}
		\item $\{x_n\}$ is the Cauchy sequence:
		\begin{itemize}
			\item $\rho(x_{n+1},x_n) = \rho(f(x_n),f(x_{n-1})) \leq \beta^n\rho(x_1,x_0))$
			\item Using triangle inequality, we have for any $m > 0$
			\begin{align*}
			\rho(x_n,x_{n+m}) \leq \rho(x_n,x_{n+1}) + \rho(x_{n+1},x_{n+2})... + \rho(x_{n+m-1},x_{n+m}) \leq \\ \beta^n/(1-\beta)\rho(x_0,x_1) \rightarrow 0, n\rightarrow \infty   
			\end{align*}
			
		\end{itemize}
		\item $\{x_n\}$ converges to $x \in X$: since $X$ is complete and closed.
		\item $x$ is a fixed point: since $f$ is a contraction, and thus uniformly continuous, therefore
		$$f(x) = \lim_{n\rightarrow \infty} f(x_{n}) = \lim_{n\rightarrow \infty }x_{n+1} = x$$
		\item $x$ is unique. Suppose that $f(z)=z,f(x)=z$.Then
		$$\rho(x,z) = \rho(f(x),f(z)) \leq \beta \rho(x,z)$$
		since $\beta < 1$, we can only have $\rho(x,z)=0,x=z$.
	\end{itemize}	
\end{proof}


\begin{remark}[global convergence property]
	The convergence is ensured from any starting point in $X$. And we use $\rho(x,y) = \norm{x-y}$.
\end{remark}



\begin{corollary}[The rate of convergence] Let $x$ be the fixed point of $T$. Then
	$\rho(x_n,x) \leq \beta^n \rho(x_0,x)$ 
\end{corollary}
\begin{proof}
	$\rho(x_n,x) = \rho(Tx_{n-1},Tx) \leq \beta \rho(x_{n-1},x) \leq \beta^2 \rho(x_{n-2},x) $	
\end{proof}



\subsubsection{Applications in root finding}

\begin{theorem}
	Let $g\in C^{1}[a,b]$ be such that $g(x)\in [a,b]$. In addition, suppose that $g$ is globally Lipshitz with a constant $0<k<1$. 
	Then for any initial number $p_0 \in [a,b]$, the sequence defined by 
	$$p_n = g(p_{n-1}),n \geq 1$$
	converges to an unique fixed point $p\in [a,b]$
\end{theorem}

\begin{corollary}
	Let $g\in C^{1}[a,b]$ be such that $g(x)\in [a,b]$. In addition, suppose that $g'$ exists on $(a,b)$ and that a constant $0<k<1$ exists with 
	$$\abs{g'(x)} \leq k, \forall x \in (a,b)$$
	Then for any initial number $p_0 \in [a,b]$, the sequence defined by 
	$$p_n = g(p_{n-1}),n \geq 1$$
	converges to an unique fixed point $p\in [a,b]$
\end{corollary}
proof: use mean value theorem to prove the magnitude is bounded.



\begin{example}
	Consider $f(x) = 0.5 x - \exp(-x)$ defined on $\R_{++}$, it can be showed that its derivative is bounded by 1.
\end{example}

\begin{remark}
	For nonlinear equations, if we can write the equation as $x = f(x)$ and show that $f(x)$ satisfies above conditions, then we can use fixed point iteration to solve the equation.
\end{remark}


\subsubsection{Applications to integral and differential equations}

\begin{theorem}
	\cite[225]{debnath2005hilbert}If $A$ is a bounded linear operator on a Banach space $E$, and $\phi$ is an arbitrary element of $E$, for the operator equation $x = \alpha A x + \phi = Tx$, fi $\norm{Af} \leq k\norm{f},\forall f\in E$(or $\norm{A} = k$) then $Tx=x$ has a unique solution whenever $\abs{\alpha} k < 1$. \\
	Moreover, the solution is given as
	$$x = \phi + \alpha A\phi + \alpha^2A^2\phi ...$$
\end{theorem}
Proof:(1) \textbf{We need to show the operator $Tx = \alpha A x + \phi$ is a contraction mapping}. 
$$\norm{Tx_1 - Tx_2} = \norm{\alpha A (x_1 - x_2)} \leq \abs{\alpha}\norm{A}\norm{x_1 - x_2} = \abs{x}k\norm{x_1-x_2}$$
(2) Since $T$ is a contraction mapping, we can generate the solution by iterating 
$$x_{n+1} = Tx_n$$
with $x_0$ be an arbitrary element in $E$.
We have
\begin{align*}
x_0 &= f\\
x_1 &= \alpha A x_0 + \phi\\
x_2 &= \alpha A x_1 + \phi = \alpha^2 A^2 x_0 + \alpha A \phi + \phi\\
\dots
\end{align*}

\begin{theorem}[Neumann series]\index{Neumann series}
	\cite[226]{debnath2005hilbert}If $A$ is a bounded linear operator in a Banach space $E$ and $\norm{A} < \lambda$, then
	$$A_\lambda = (A - \lambda I)^{-1} = -\sum_{i=0}^\infty \frac{A^n}{\lambda^{n+1}}$$
	and
	$A_\lambda$ is bounded as
	$$\norm{A_\lambda} \leq \frac{1}{\abs{\lambda} - \norm{A}}$$
\end{theorem}

\begin{corollary}
	If $A$ is a bounded linear operator in a Banach space $E$ and $\norm{A} < 1$, then
	$$B = (I - A)^{-1} = \sum_{i=0}^\infty A^i$$
	and $B$ is bounded by 
	$$\norm{B} \leq \frac{1}{1-\norm{A}}$$
\end{corollary}
Proof: (1) $(I - A)^{-1} (\sum_{i=0}^\infty A^i) = I-A^\infty = I$(since the infinite power of a contraction mapping is zero mapping in Banach space) (2) use the inequality 
$$\norm{B} = \norm{\sum_{i=0}^\infty A^i} \leq \sum_{i=0}^\infty \norm{A}^i = \frac{1}{1-\norm{A}} $$

\begin{corollary}
	If $A$ is a bounded linear operator in a Banach space $E$ and $\norm{A} < 1$, then the equation
	$$x = x_0 + Ax$$
	has a unique solution given by
	$$x = \sum_{n=0}^\infty A^n x_0$$
\end{corollary}
Proof: $x=x_0+Ax \Leftrightarrow (I-A)x = x_0$



\begin{theorem}[Picard's existence and uniqueness theorem]\index{Picard's existence and uniqueness theorem}\index{initial value problem}
	\cite[228]{debnath2005hilbert} Consider the initial value problem for the ordinary differential equation 
	$$\frac{dy}{dt} = f(t,y)$$
	with the initial condition $y(t_0) = y_0$, where $f$ is a continuous function in some closed domain
	$$R = \{(t,y): a\leq t \leq b,c\leq y\leq d\}$$
	containing the point $(t_0,y_0)$ in its interior. If $f$ satisfies the Lipschitz condition
	$$\abs{f(t,y_1) - f(t,y_2)} \leq K\abs{y_1-y_2}$$
	for $K>0$ and all $t,y_1,y_2 \in R$, then there exist a unique solution of $y=\phi(x)$ defined in \textbf{some neighborhood of $t_0$}
\end{theorem}
Proof: define $S = C[t_0-\epsilon,t_0+\epsilon]$ to be Banach space with the sup-norm
$$\norm{\phi} = \sup_x \abs{\phi(x)}$$
Define operator $T$ on $S$, such that $(Ty)(t) = y_0 + \int_{t_0}^t f(\tau,y) d\tau$

For any $\phi_1,\phi_2\in S$, we have
$$\norm{T\phi_1 - T\phi_2} = \sup_x \abs{\int_{x_0}^x(f(t,\phi_1) - f(t,\phi_2))dt} \leq K\epsilon \abs{\phi_1-\phi_2}$$
where $\epsilon$ is chosen such that $K\epsilon <1$, then $T$ is the contraction mapping, and therefore $Tx=x$ has a solution in $S$.


\begin{remark}\hfill
	\begin{itemize}
		\item The requirement on $f(t,y)$ can be summarized as continuous and 'slow'-changing(reflected by Lipschitz condition.)
		\item Other 'restrictive' condition could be $f(t,y)$ continuous and continuously differentiable on closed interval.
	\end{itemize}
\end{remark}





\section{Inner product space and Hilbert space}\label{ch:functional-analysis:sec:hilbert-space}\index{Hilbert space}
\subsection{Inner product space (pre-Hilbert space) and Hilbert space}\label{ch:functional-analysis:sec:inner-product-space-pre-hilbert-space}
\subsubsection{Foundations}
\begin{definition}\index{inner product}
	An inner product space is a vector space $V$ over $\F$ equipped with an additional structure called inner product that assigns each pair of vectors $v_1,v_2 \in V$ a number $\ip{v_1,v_2} \in \mathbb{F}$, and its assignment satisfies
	\begin{enumerate}
		\item conjugate symmetry: $\ip{v_1,v_2} = \conj{\ip{v_2,v_1}}$
		\item linearity in the first slot: $\ip{c_1v_1 + c_2v_2,v_3}=c_1\ip{v_1,v_2}+c_2 \ip{v_2,v_3}$, for all $c_1,c_2 \in \F$ (many physicists define linearity in the second slot)
		\item $\ip{v,v} \geq 0 ~\text{and} ~ \ip{v,v} = 0 ~\text{iff} ~ v=0$
	\end{enumerate}
\end{definition}


\begin{remark}
	Inner product space is sometimes called pre-Hilbert space. Compared with inner product space, the pre-Hilbert space is incomplete with respect to the metric defined by the induced norm. 
\end{remark}

\begin{remark}\textbf{The standard inner product on complex vector fields} is usually defined as $$\ip{u,v}=u^T\conj{v} $$ by mathematicians
	or
	$$\ip{u,v}=u^Hv = \conj{u}^T v$$
	by physicists. The result are conjugate to each other. The second is simplier in writing. We prefer the second. 
\end{remark}





\begin{theorem}[Cauchy-Schwarz inequality]\index{Cauchy-Schwarz inequality}\label{ch:functional-analysis:th:Cauchy-SchwarzInequalityInnerProductSpace}
	\cite{johnsonbaugh2010foundations}\cite[100]{moon2000mathematical}In an inner product space $S$ with induced norm, we have
	Cauchy-Schwarz inequality : for $x,y \in V$, we have
	$$\ip{x,y}^2 \leq \ip{x,x}\ip{y,y}$$
	with equality if and only if $x=\lambda y$. 
	
	If use the inner product as the induced norm(\autoref{ch:functional-analysis:th:innerProductAsInducedNorm}), we can write the inequality as
	$$\abs{\ip{x,y}} \leq \norm{x}\norm{y}.$$
\end{theorem}
\begin{proof}
	use the fact the $\ip{x-\lambda y,x-\lambda y} \geq 0$ is valid for all real $\lambda$,then require quadratic equation determinant to be less than 0.	Specifically, 
	$$\ip{x,x} - 2\lambda \ip{x,y} + \lambda^2 \ip{y,y} \geq 0, \forall \lambda.$$
	The left reaches minimum when $\lambda = \ip{x,y}/\ip{y,y}$ with minimum value
	$$\ip{x,x} - \frac{\ip{x,y}^2}{\ip{y,y}} \geq 0.$$
\end{proof}



\begin{corollary}[Cauchy-Schwarz inequality for finite real-valued terms]\cite[120]{johnsonbaugh2010foundations}\label{ch:functional-analysis:th:Cauchy-SchwarzInequalityFiniteRealTerms} Let $a_1,a_2,...,a_n \in \R$ and $b_1,b_2,...,b_n \in \R$. Then 
	$$\abs{\sum_{k=1}^n a_kb_k} \leq \sqrt{(\sum_{k=1}^n a_k^2)\sum_{k=1}^n a_k^2)}$$
\end{corollary}
\begin{proof}
	Consider inner product space $\R^n$ with inner product defined by 
	$$\ip{a,b} = \sum_{i=1}^n a_ib_i. $$	
\end{proof}

\begin{lemma}[inner product as induced norm]\index{induced norm}\label{ch:functional-analysis:th:innerProductAsInducedNorm}\cite[290]{johnsonbaugh2010foundations}
	\cite[172]{axler2015linear} When inner product is used as norm, we have
	$$\norm{u + v} \leq \norm{u} + \norm{v}$$
\end{lemma}
\begin{proof}
	\begin{align*}
	\norm{u+v}^2 & = \norm{u}^2 + \norm{v}^2 + \ip{u,v} + \ip{v,u} \\
	& = \norm{u}^2 + \norm{v}^2 + 2Re(\ip{u,v})\\
	& = \norm{u}^2 + \norm{v}^2 + 2\abs{\ip{u,v}}\\
	& \leq \norm{u}^2 + \norm{v}^2 + 2\norm{u}\norm{v}
	\end{align*}
	where Cauchy-Schwarz inequality (\autoref{ch:functional-analysis:th:Cauchy-SchwarzInequalityInnerProductSpace})is used. 	
\end{proof}

\begin{lemma}[parallelogram law]\cite[64]{christensen2010functions}\label{ch:functional-analysis:th:InnerProductSpaceParallelogramLaw}
	Let $V$ be an inner product space with induced norm. For all $v,w\in V$, we have
	$$\norm{v+w}^2 + \norm{v-w}^2 = 2(\norm{v}^2 + \norm{w}^2).$$
\end{lemma}
\begin{proof}
Note that
$$\norm{v+w}^2 = \ip{v+w,v+w} = \norm{v}^2 + \norm{w}^2 + 2\ip{v,w},$$
and
$$\norm{v-w}^2 = \norm{v}^2 + \norm{w}^2 - 2\ip{v,w}.$$	
\end{proof}

\begin{remark}[caution!]
For general normed vector spaces, we usually do not have such parallelogram law, since these norms do not have an associated inner product space inducing it. 	
\end{remark}


\begin{lemma}[basic properties of inner product]\hfill
	\begin{itemize}
		\item For fixed $u\in V$, $\ip{v,u}$ is linear map from $U$ to $\F$ (for the second slot, it is not)
		\item $\ip{u,\lambda v} = \bar{\lambda}\conj{\ip{u,v}},\forall \lambda $
		\item $\ip{u,v+w} = \ip{u,v} + \ip{u,w}$
	\end{itemize}
\end{lemma}
\begin{proof}
	Proof of (2): $\ip{u,\lambda v} = \conj{\ip{\lambda v,u}} = \conj{\lambda}\conj{\ip{v,u}} = \conj{\lambda}\ip{u, v}$
\end{proof}

\begin{lemma}[continuity of inner product]
	In an inner product space, if $x_n\to x$ and $y_n\to y$, then $\ip{x_n,y_n}\to \ip{x,y}$.
\end{lemma}
\begin{proof}
	\begin{align*}
	\abs{\ip{x_n,y_n}-\ip{x,y}}&=\abs{\ip{x_n,y_n}-\ip{x_n,y}+\ip{x_n,y}-\ip{x,y}}\\& \leq \abs{\ip{x_n,y_n}-\ip{x_n,y}}+\abs{\ip{x_n,y}-\ip{x,y}} \\
	& = \abs{\ip{x_n,y_n - y}}+\abs{\ip{x_n - x,y}} \\
	& \leq \norm{x_n}\norm{y_n-y}+\norm{y_n}\norm{y_n-y} \\
	&  \to 0   
	\end{align*}
	where we use Cauchy-Schwarz inequality (\autoref{ch:functional-analysis:th:Cauchy-SchwarzInequalityInnerProductSpace}) and the fact that convergent sequence is bounded.	
\end{proof}




\subsubsection{Strong and weak convergence}
\begin{definition}[strong convergence]\index{strong convergence}
	\cite[98]{debnath2005hilbert}
	A sequence $\{x_n\}$ in inner product space $E$ is called strongly convergent to $x$ if
	$$\norm{x_n-x}\to 0$$
	as $n\to \infty$
\end{definition}


\begin{definition}[weak convergence]\index{weak convergence}
	\cite[98]{debnath2005hilbert}A sequence $\{x_n\}$ in inner product space $E$ is called strongly convergent to $x$ if
	$$\ip{x_n,y} \to \ip{x,y}, \forall y \in E$$
\end{definition}


\begin{theorem}[strong convergence implies weak convergence]
	\cite[98]{debnath2005hilbert}A strongly convergent sequence weakly convergent.
\end{theorem}
\begin{proof}
	$\ip{x_n-x,y}\leq \norm{x_n-x}\norm{y} \to 0$
\end{proof}

\begin{theorem}
	\cite[98]{debnath2005hilbert}If $\{x_n\}$ converges to $x$ weakly and $\norm{x_n}\to \norm{x}$, then $$\norm{x_n - x} \to 0$$ that is strong convergence is implied.
\end{theorem}
\begin{proof}
	use the fact of weak convergence of 
	$$\ip{x_n,x}\to \ip{x,x} = \norm{x}^2$$
\end{proof}

\subsection{Common Hilbert spaces}
\subsubsection{Hilbert space}
\begin{definition}\cite[65]{christensen2010functions}A vector space $V$ with an inner product is called a \textbf{Hilbert space} if the vector space  $V$ equipped with the induced norm(\autoref{ch:functional-analysis:th:innerProductAsInducedNorm}) is a Banach space.
\end{definition}











\subsubsection{$\R^n$ space}
\begin{theorem}[$\R^n$ is Hilbert space]\cite[65]{christensen2010functions}
	The $\R^n$ with the inner product is Hilbert spaces. 
\end{theorem}
\begin{proof}
	Note that this inner product induces a norm(\autoref{ch:functional-analysis:th:innerProductAsInducedNorm}). Therefore, since $\R^n$ is a complete Banach space(\autoref{ch:functional-analysis:th:RnisBanachSpace}), this inner product will make it a Hilbert space.	
\end{proof}



\subsubsection{$l^p(\N)$ space}

\begin{theorem}\cite[65]{christensen2010functions}
	The $l^2(\N)$ is a Hilbert spaces. 
\end{theorem}


\begin{note}
	Suppose $l^p(\N)$ is an Hilbert space. So it must satisfy the parallelogram law(\autoref{ch:functional-analysis:th:InnerProductSpaceParallelogramLaw}). That is, for all $u,v \in l^p(\N)$:
	$$2\norm{u}_p^2 + 2\norm{v}_p^2 = 2\norm{u+v}_p^2 + 2\norm{u-v}_p^2.$$
	
	If we take $u = e_1 = (1,0,...,0,...)$ and $v = e_2 = (0,1,0,...,0,...)$, we have
	$$2 + 2 = 2^{2/p} + 2^{2/p},$$
	which only holds when $p=2$. 
	
	That is, only $l^p(\N),p=2$ is Hilbert space.	
\end{note}




\subsubsection{The $L^p$ space}
\begin{definition}[$L^p$ space] \index{$L^p$ space}
	\cite{wiki:Lpspace} Let $(X,\Sigma,\mu)$ be a positive measure space and define $$L^p(X,\Sigma,\mu)=\{f|f ~ \text{is measurable function on $X$ and } ~ \int_X \abs{f}^p d\mu < \infty\}$$
	and we denote the $p$-norm as
	$$\norm{f}_p = \norm{\int_X \abs{f}^p d\mu}^{1/p}$$
\end{definition}


\begin{lemma}[pointwise inequality]
	\cite{wiki:Lpspace}\cite[32]{sansone2004orthogonal} Let $f,g\in L^p(X)$, then
	$$\abs{c_1f+c_2g}_p^p \leq 2^{p}(\abs{c_1}\abs{f}^p + \abs{c_2}^p\abs{g}^p)$$
\end{lemma}
\begin{proof}
	$$\abs{c_1 f + c_2 g}/2 \leq \max{\abs{c_1}\abs{f},\abs{c_2}\abs{g}} $$
	$$\abs{c_1 f + c_2 g}^p \leq 2^p\max{\abs{c_1}^p\abs{f}^p,\abs{c_2}^p\abs{g}^p} $$
	$$\abs{c_1 f + c_2 g}^p \leq 2^p\abs{c_1}^p\abs{f}^p+\abs{c_2}^p\abs{g}^p $$
	
\end{proof}
\begin{lemma}
	If $f,g \in L^p(X)$, then $c_1f+c_2g \in L^p(X), \forall c_1,c_2 \in \F$	
\end{lemma}
\begin{proof}
	first measurable functions form a vector space (see previous sections). Then we need to show that (1) $c_1f + c_2g$ is integrable (can be showed by directly use the pointwise inequality above)	
\end{proof}


\begin{lemma}[absolute integrable implies integrable]\index{absolute integrable}
	If $\int_X \abs{f} d\mu < \infty$ then $$\int_X f d\mu < \infty$$
\end{lemma}

\begin{theorem}\cite[118]{christensen2010functions}
	The $L^2(R)$ and $L^2([a,b])$ are all Hilbert spaces. 
\end{theorem}

\begin{note}\cite[131]{christensen2010functions}
	Suppose $L^p(\R)$ is an Hilbert space. So it must satisfy the parallelogram law(\autoref{ch:functional-analysis:th:InnerProductSpaceParallelogramLaw}). That is, for all $u,v \in L^p(\R)$:
	$$2\norm{u}_p^2 + 2\norm{v}_p^2 = 2\norm{u+v}_p^2 + 2\norm{u-v}_p^2.$$
	
	If we take $u = \bm{1}_{[0,1]},v = \bm{1}_{[1,2]}$, we have
	$$2 + 2 = 2^{2/p} + 2^{2/p},$$
	which only holds when $p=2$. 
	
	That is, only $L^p(\R),L^p([a,b]),p=2$ are Hilbert spaces.	
\end{note}



\subsubsection{The Hilbert space $L^2(X,\Sigma,\mu)$}
\begin{definition}[$L^2$ space] \index{$L^2$ space}
	\cite[397]{johnsonbaugh2010foundations} Let $(X,\Sigma,\mu)$ be a positive measure space and define $$L^2(X,\Sigma,\mu)=\{f|f ~ \text{is measurable function on $X$ and } ~ \int_X \abs{f}^2 d\mu < \infty\}$$
\end{definition}

\begin{remark}\index{square integrable function}
	Every element in $L^2$ is also known as \textbf{square integrable function}.
\end{remark}

\begin{lemma}
	\cite[1]{sansone2004orthogonal}If $f_1,f_2 \in L^2(X)$, then $f_1f_2\in L^1(X)$.
\end{lemma}
\begin{proof}
	In the pointwise sense, we have
	$$\abs{f_1f_2} \leq \frac{1}{2}(f_1^2 + f_2^2).$$	
\end{proof}

\begin{lemma}
	\cite[1]{sansone2004orthogonal}If $f_1,f_2 \in L^2(X)$, then $c_1f_1 + c_2f_2\in L^2(X),\forall c_1,c_2 \in \F$.
\end{lemma}
\begin{proof}
	In the pointwise sense, we have
	$$\abs{c_1f_1 + c_2f_2}^2 = \abs{c_1f_1^2} + \abs{c_2f_2^2} +2\abs{c_1c_2f_1f_2}$$	
\end{proof}


\begin{theorem}\label{ch:functional-analysis:th:SquareLebegueIntegrableFunctionSpaceIsHilbertSpace}
	\cite[397]{johnsonbaugh2010foundations} The $L^2(X,\Sigma,\mu)$ is a Hilbert space, with the inner product defined as
	$$\ip{f,g} = \int_X f\conj{g} d\mu$$
\end{theorem}
\begin{proof}
	We must show it is a vector space(already showed in above lemma), and it is complete. 	
\end{proof}


\begin{remark}
	The completeness of this space has deep implications in convergence of random variables, since we can treat each random variable as an element in the $\mathcal{L}^2(X,\Sigma,\mu)$.
\end{remark}
\subsubsection{Linear independence of functions}
\begin{definition}[linear dependence]\index{linear dependence}
	\cite[2]{sansone2004orthogonal}  $n$ functions $\{f_i\}$ defined on a common domain set $g$ are called linearly dependent if there exist $n$ constants $c_i$, not all zeros, such that $\sum_{i=1}^n c_i f_i(x) = 0$ almost everywhere. If such constants do not exist, then they are called linearly independent.
\end{definition}


\begin{definition}[Gram matrix]\index{Gram matrix}
	The Gram matrix for $n$ functions from the $L^2(X)$ inner product space is defined with elements
	$$[G]_{ij} = \ip{f_i,f_j} = \int_X f_i\conj{f_j}d\mu$$
\end{definition}


\begin{lemma}
	The Gram matrix $G$ is positive semi-definite. 
\end{lemma}
\begin{proof}
	Let $f= \sum_{i=1}^n c_i f_i$, then $0\leq \ip{f,f} = c^T G c$	
\end{proof}



\begin{theorem}[linear dependence criterion from Gram matrix]
	$n$ functions from the $L^2(X)$ inner product space are linear dependent if and only the Gram matrix $G$ has $det(G) = 0$
\end{theorem}
\begin{proof}
	If $det(G) = 0$, then $Gc = 0$ has non-trivial solution $c_0$. Then $c_0^T G c_0 = 0 =\ip{f,f} = 0\Rightarrow f = 0 \Rightarrow 0 = \sum_{i=1}^n c_{0,i} f_i$ has non-trivial linear combination coefficients. 	
\end{proof}




\subsubsection{Inequalities}
\begin{lemma}[Cauchy Schwarz inequality]\index{Cauchy Schwarz inequality}
	let $f_1,f_2 \in L^2(X)$, then $\ip{f_1,f_2}^2 \leq \ip{f_1,f_1}\ip{f_2,f_2}$ 
\end{lemma}
\begin{proof}
	Because $det(G) = \prod \lambda_i \leq 0$, and
	$$det(G) = \ip{f_1,f_2}^2 - \ip{f_1,f_1}\ip{f_2,f_2}$$	
\end{proof}



\begin{corollary}[integral inequality]\index{integral inequality}
	\cite[5]{sansone2004orthogonal} Let $f\in L^2(X)$, then
	$$(\int_X f d\mu)^2 \leq \mu(X) \int_X f^2 d\mu$$
\end{corollary}
\begin{proof}
	use Cauchy inequity and set $f_2 = 1$.	
\end{proof}



\subsection{Orthogonal decomposition}

\subsubsection{Orthogonality}
\begin{definition}[complementary subspaces, orthogonal complementary subspaces]\index{complementary subspace}
\cite[392]{meyer2000matrix}\cite[108]{moon2000mathematical}\cite[403]{meyer2000matrix}
\begin{itemize}
	\item Subspaces $A,B$ of a Hilbert space $H$ are said to be \textbf{complementary subspaces} if
	$$H = A + B, A\cap B = 0.$$
	We can also denoted as
	$$H = A \oplus B.$$
	\item Let $x,y$ be vectors in a Hilbert space $H$, we say $x$ and $y$ are \textbf{orthogonal} if $\ip{x,y} = 0$. We say subspaces $A$ and $B$  are orthogonal to each other if for every vector $x\in A,y\in B$, we have $\ip{x,y} = 0$.
	\item 	The \textbf{orthogonal complement} of a subspace $A$, denoted by $A^\perp$, is the set of vectors orthogonal to $A$, i.e.,
	$$A^\perp = \{x\in H| \ip{x,y} = 0 \forall y\in A\}.$$
\end{itemize}
\end{definition}

\begin{remark}[orthogonality does not implies complementary]
\textbf{Caution! Two subspaces orthogonal to each other does not imply they are complementary to each other.} For example, any subspace is orthogonal to $\{0\}$, but they are not necessarily complementary to each other. 
\end{remark}

\begin{theorem}[Orthogonality relation in inner product space]\label{ch:functional-analysis:th:OrthogonalityRelationInInnerProductSpace}
\cite[108]{moon2000mathematical}
Let $V$ and $W$ be \textbf{subsets} of inner product space $S$ (not necessarily complete), then 
\begin{itemize}
    \item The orthogonal complementary subspace of $V$, denoted by $V^{\perp}$, is closed subspace of $S$.
    \item $V\subset V^{\perp\perp}$
    \item If $V\subset W$, then $W^{\perp} \subset V^{\perp}$
    \item $V^{\perp\perp\perp} = V^{\perp}$
    \item If $x\in V \cap V^{\perp}$, then $x = 0$
    \item $\{0\}^{\perp} = S,S^{\perp} = \{0\}$
\end{itemize}
\end{theorem}
\begin{proof}
(1) Let $\{x_n\}$ be a convergent sequence in $V^\perp$, then $\ip{x_n,v} = 0$, for any $v\in V$. Then we can show $$\lim_{n\to \infty} \ip{x_n,v} = \ip{\lim_{n\to \infty}x_n,v} = 0$$ which is due to the continuity of inner product. Therefore, $\lim_{n\to \infty}x_n$ is lying in $V^\perp$. And therefore $V^\perp$ is closed. (2) If $x\in V$, then $\ip{x,y} = 0, \forall y\in V^\perp$, therefore $x\in V^{\perp\perp}$. (3) Let $w' \in W^\perp$, then $w'\perp w,\forall w \in W \Rightarrow w'\perp v\in V$ since $V\subset W$, therefore $w'\in V^\perp$, therefore $W^\perp \subset V^\perp$. (4) From (2) $V^\perp \subset V^{\perp\perp\perp}$, from (3)  $  V^{\perp\perp\perp}\subset V^\perp$. (5) $\ip{x,x} = 0\Rightarrow x = 0$
\end{proof}



\subsubsection{Projection and orthogonal decomposition}

\begin{theorem}[projection theorem in inner product space]\label{ch:functional-analysis:th:ProjectionTheoremInInnerProductSpace}
	\cite[50]{luenberger1969optimization}
	Let $X$ be an inner product space in $X$, and let $M$ be a subspace of $X$. 
	It follows that
	\begin{itemize}
		\item \textbf{If there exists a vector} $m_0\in M$ such that $\norm{x-m_0} \leq \norm{x-m},\forall m\in M$, then $m_0$ is unique and $\ip{x,x-m_0} = 0.$ 
		\item Moreover, a necessary and sufficient condition for $m_0\in M$ being the unique minimizing vector in $M$ is that $$(x-m_0)\perp M$$
	\end{itemize}
\end{theorem}
\begin{proof}
 (1) \textbf{$m_0$ is minimazing vector implies $x-m_0$ is orthogonal to $M$}. We prove by \textbf{contraposition}. We suppose $x-m_0$ is not orthogonal to $M$, then there is $m\in M$ such that $\ip{x-m_0,m}=\delta \neq 0$. WLOG, we let $\norm{m} = 1$. Let $m_1\in M$ be $m_1 = m_0 + \delta m$, then

$$\norm{x-m_1}^2 = \norm{x-m_0}^2 - \delta^2 < \norm{x-m_0}^2$$

Therefore, if $x-m_0$ is not orthogonal to $M$, $m_0$ is not a minimizing vector.
(2) $x-m_0$ is orthogonal to $M$ implies $m_0$ is unique minimizer. For any $m\in M$,
\begin{align*}
\norm{x-m}^2 &= \norm{x-m_0 + m_0 - m}^2 \\
& = \norm{x-m_0}^2 + \norm{m_0 - m} \\
& < \norm{x-m_0}^2, \forall m\neq m_0
\end{align*}	
\end{proof}

\begin{theorem}[projection theorem in Hilbert space]\label{ch:functional-analysis:th:ProjectionTheoreInHilbertSpace}
	\cite[51]{luenberger1969optimization} \cite[67]{christensen2010functions}
	Let $X$ be a Hilbert space, let $x$ be a given element in $X$, and let $M$ be a closed subspace of $X$. 
	\begin{itemize}
		\item Then there exists a vector $m_0\in M$ such that $\norm{x-m_0} \leq \norm{x-m},\forall m\in M$, then $m_0$ is unique and $\ip{x,x-m_0} = 0.$
		\item 	Moreover, a necessary and sufficient condition for $m_0\in M$ being the unique minimizing vector in $M$ is that $$(x-m_0)\perp M.$$
	\end{itemize}
\end{theorem}
\begin{proof}
	\textbf{We only need to establish the existence of minimizing vector}, the rest has been proved in the above inner space projection theorem(\autoref{ch:functional-analysis:th:ProjectionTheoremInInnerProductSpace}).
	If $x \in M$, then $m_0=x$ and everything is settled. Let us assume $x\notin M$, and defined $\delta = \inf_{m\in M}\norm{x-m}$.
	
	Let $\{m_i\}$ be a sequence in $M$ that $\norm{x-m_i} \to \delta$. Then
	$$\norm{m_i-m_j}^2 = 2\norm{m_i-x}^2 + 2\norm{m_j-x}^2 - 4\norm{x-\frac{m_i+m_j}{2}}^2 \to 0$$ as $i,j\to 0$
	Because of $M$ is closed subspace, then the Cauchy sequence has a limit in $M$, and the limit is the unique minimizing vector.	
\end{proof}


\begin{theorem}[Orthogonal decomposition in Hilbert space]\cite[68]{christensen2010functions}
	\cite[53]{luenberger1969optimization}Let $S$ be an arbitrary closed subspace of an complete Hilbert space $V$, then $$V=S\oplus S^{\perp},S = S^{\perp\perp};$$
	that is, any element $x\in V$ can be uniquely written $x=a+b,a\in S,b\in S^{\perp}$. 
\end{theorem}
\begin{proof}
If $S = \{0\}$, then $S^\perp = H$ and the result holds. We therefore assume $S\neq \{0\}$.	
	(1)From projection theorem in Hilbert space(\autoref{ch:functional-analysis:th:ProjectionTheoreInHilbertSpace}), any element $x\in V$ has a unique element $u\in S$ satisfying $x \perp u \in S^{\perp}$.
	Then we can write $x = u + (x - u)$ uniquely.
	 Because the decomposition is unique, therefore it is direct sum.  
	(2) We have showed that $S\subset S^{\perp\perp}$(\autoref{ch:functional-analysis:th:OrthogonalityRelationInInnerProductSpace}). To show $S = S^{\perp\perp}$, let $x\in S^{\perp\perp} \subset V$, then $x = a+b,a\in S,b\in S^\perp$(using (1), every $x\in V$ can be written as the sum of components in $S$ and $S^\perp$). Since $a\in S, S\subseteq S^{\perp\perp}$, $a$ is also in $ S^{\perp\perp}$, then $x-a  = b \in S^{\perp\perp}$, therefore $b = 0$(using \autoref{ch:functional-analysis:th:OrthogonalityRelationInInnerProductSpace}, $b\in S^{\perp}\cap S^{\perp\perp} \implies b = 0$). Eventually, we have  $x=a\in S, S = S^{\perp\perp}$.
\end{proof}




\subsection{Separable Hilbert space}
\begin{definition}[separable Hilbert space]\index{separable Hilbert space}
\cite[127]{debnath2005hilbert} A Hilbert space is called \textbf{separable Hilbert space} if it contains a complete orthonormal sequence. Finite dimensional Hilbert spaces are separable.
\end{definition}

\begin{example}
\cite[127]{debnath2005hilbert}Space $L^2[-\pi,\pi]$ is separable since there exist the complete trigonometric Fourier series.
\end{example}


\begin{theorem}
\cite[128]{debnath2005hilbert} Every separable Hilbert space $H$ contains a countable dense subset.
\end{theorem}
\begin{proof}
Let $\{x_n\}$ be a complete orthonormal sequence. Let $C = \{c|a+bi,a,b\in \mathbb{Q}\}$ be the set of coefficients. Then subset $S$ of the linear combination of $\{x_n\}$ using coefficients of $C$ can approximate any element in $H$ in the norm sense(norm of $H$). Also, the set $S$ is countable.(the product of countable sets are countable sets) 
\end{proof}

\begin{theorem}
\cite[128]{debnath2005hilbert}Every orthogonal set in a separable Hilbert space is countable.
\end{theorem}


\begin{remark}
This theorem implies \textbf{the complete orthonormal set is countable}.
\end{remark}



\section{Approximations in Hilbert space}
\subsection{Projection theorem }


\subsection{Approximation via projection}
\begin{theorem}[Hilbert space subspace approximation theorem]\label{ch:functional-analysis:th:HilbertSpaceSubspaceApproximationTheorem}
Let $(S,\norm{\cdot})$ be a Hilbert space. Let $T=\{p_1,p_2,...,p_m\} \subset S$ be a set of linearly independent vectors in $S$. Let $V=span(T)$. 
	Given a vector $x\in S$, the element $\hat{x}$ in $T$ such that 
	$$\norm{x-\hat{x}}\leq \norm{x-y}, \forall y
	\in T$$
	can found via the following procedure. 
	
	\begin{itemize}
		\item Let
		$\tilde{x} = \sum_{i=1}^m c_i p_i $ with unknown parameters $c_1,c_2,...,c_m$.  The coefficients $c_1,c_2,...,c_n$ has to satisfy the orthogonality condition of 
		$$\ip{x-\hat{x},\hat{x}} = 0;$$
		or equivalently,
		$$\ip{x-\sum_{i=1}^m c_ip_i,p_j} = 0,\forall j\in\{1,2,...,m\}.$$
		\item The orthogonal condition gives a system of equation as
			$$G c=p, c,p\in \R^m, G\in \R^{m\times m},$$
		where
		$$G = \begin{bmatrix}
		\ip{p_1,p_1} & \ip{p_1,p_2} & \ip{p_1,p_2} & \cdots & \ip{p_1,p_m}\\ 
		\ip{p_2,p_1}	& \ip{p_2,p_2} & \ip{p_2,p_3} & \cdots & \ip{p_2,p_m} \\ 
		\ip{p_3,p_1}	& \ip{p_3,p_2} & \ip{p_3,p_3} & \cdots & \ip{p_3,p_m}\\ 
		\vdots	& \vdots & \vdots & \ddots & \vdots\\ 
		\ip{p_m,p_1}	& \ip{p_m,p_2} & \ip{p_m,p_3} & \cdots & \ip{p_m,p_m} 
		\end{bmatrix}, p = \begin{bmatrix}
		\ip{x,p_1} \\
		\ip{x,p_2} \\
		\ip{x,p_3} \\
		\vdots \\
		\ip{x,p_n} 
		\end{bmatrix}$$
		that is, $G_{i,j}=\ip{p_i,p_j}$, $c=[c_1,c_2,...,c_m]^T$ and $p=[\ip{x,p_1},\ip{x,p_2},...,\ip{x,p_m}]^T$. 
		
		The matrix $G$ is known as \textbf{Gram} matrix of the set $T$.
		\item  If the element $x$ is in $T$, then $\hat{x} = x$.
	\end{itemize}
\end{theorem}
\begin{proof}
 	
\end{proof}

\begin{remark}[existence of minimizer]
In a Hilbert space(also a normed linear space), any finite-dimensional subspace is closed(\autoref{ch:functional-analysis:th:ClosednessAndCompletenessOfFiniteDimensionalNormedSpace}).This theorem and the projection theorem(\autoref{ch:functional-analysis:th:ProjectionTheoreInHilbertSpace}) guarantees the existence of solution.	
\end{remark}



\begin{theorem}[invertible condition for Gram matrix]
The Gram matrix $G$ is always positive-semidefinite. It is positive-definite if and only if the vectors $p_1,p_2,...,p_m$ are linearly independent. 
\end{theorem}
\begin{proof}
$R$ can be written as $G = J^T J$, where $J$ is matrix formed by the columns of $p_i$s. Then, for any nonzero $x \in H$, 
$$x^TGx = x^TJ^TJx = \norm{Jx}^2 \geq 0.$$
If $J$ is nonsingular, then $Jx \neq 0$ and the equality sign will not hold. 
\end{proof}



\begin{theorem}[condition of finality]\label{ch:functional-analysis:th:conditionoffinality}
	\cite[54]{davis1989fourier}
	Suppose we are given a linearly independent element $u_1,u_2,u_3,...$ in a Hilbert space (a function space) $V$, and we want to minimize the value of $$\norm{f-\alpha_1 u_1 - \alpha_2 u_2 -...- \alpha_n u_n}^2$$ over the coefficients $\alpha$. Then a sequential optimization and all-at-once optimization will give the same result \textbf{if and only if the sequence is orthogonal}. 
\end{theorem}
\begin{proof}
directly from uniqueness of optimal coefficients(due to the full rank nature of Gram matrix)	
\end{proof}
.

\subsection{Application examples}
\subsubsection{Orthogonal projection and normal equations in $\R^n$}
\begin{theorem}\index{normal equation}\label{ch:functional-analysis:th:normalequation}
	Let $H$ be the finite dimension Hilbert space $\R^n$, let $y$ be a vector in $H$ and let $S$ be a closed subspace of $H$ and $dim(S)=k$. Further let $U$ be the matrix with columns consisting of the $k$ linearly independent basis of $S$. It follows that
	\begin{itemize}
		\item The unique solution to the minimization problem:
		$$\min_x \norm{y - Ux}^2_2,x \in \R^k, Ux\in S$$ should satisfy the orthogonality condition $$\ip{U_i, y - Ux^*} = 0, \forall i=1,2,...,k.$$
		The orthogonality condition can be summarized by
		$$U^TUx^* = U^Ty,$$
		which is known as \textbf{normal equation}, with solution
		$$x^* = (U^TU)^{-1}U^Ty.$$
		\item (single coefficient) the coefficient associated with the subspace spanned by column vector of $U_i$ is
		$$x^*_i = \frac{U_i^TN_{-i}}{U_i^TN_{-i}U_i}y,$$
		where $N_{-i} = (I - U_{-i}(U^T_{-i}U_{-i})^{-1}U^T_{-i})U_i$,$U_{-i}$is the matrix without column $i$.
		\item From the perspective of projection, we can view $Ux$ as the vector lying in the subspace $S$ and $Ux = Py = U(U^TU)^{-1}U^Ty$
		where $P = U(U^TU)^{-1}U^T$ is the projection matrix. Moreover, 
		\begin{itemize}
			\item if $U$ consists of orthonormal basis, then $P = UU^T$.
			\item $P$ is orthonormal projection operator since $P = P^T$.
		\end{itemize}
	\end{itemize} 
\end{theorem}
\begin{proof}
	(1)(optimization method) The solution to the minimization can be obtained by expanding the norm and take first derivative to be zero. Note that this optimization problem is strictly convex and therefore the solution is unique.
	$$\ip{Ux^*,y-x^*} = (U(U^TU)^{-1}U^Ty)^T(I-U(U^TU)^{-1}U^T)y = y^T(U(U^TU)^{-1}U^T- U(U^TU)^{-1}U^T)y.$$
	
	We can alternatively use Hilbert space subspace projection theorem(\autoref{ch:functional-analysis:th:HilbertSpaceSubspaceApproximationTheorem}) to prove.
	Note that the system equation is given by 
	$$Gc = Uy, x = Ux,$$
	where $G$ is the Gram matrix given by $G = U^TU$.
	
	(2) Note that we can decompose our orthogonal projector into two orthogonal projectors that are orthogonal to each other(\autoref{ch:linearalgebra:th:decompositionOfOrthogonalProjector})
	$$U(U^TU)^{-1}U^T = U_{-i}(U^T_{-i}U_{-i})^{-1}U^T_{-i} + \frac{N_{-i}U_iU_i^TN_{-i}^T}{U_i^TN_{-i}U_i}.$$
	The coefficient associated with vector $U_i$ must be the same as the coefficient associated with $N_{-i}U_i$, which is orthogonal to the rest of the subspace $U_{-i}$. Therefore, the coefficient can be calculated via projection of $y$ onto $N_{-i}U_i$,i.e., $$\frac{U_i^TN_{-i}}{U_i^TN_{-i}U_i}y$$
	
	Note that the best approximate $\hat{y}$ can be written by
	$$y = x_i U_i + x_{-i}U_{-i} = x_i N_{-i}U_i + x_i P_{-i}U_{i} + x_{-i}U_{-i},$$
	Note that the subspace $N_{-i}U_i$ is orthogonal to the subspace of $[U_{-i} , P_{-i}U_i]$. Therefore, the coefficient of $x_i$ is uniquely determined by  projecting onto $N_{-i}U_i$.   
	(3) since $P^2 = P$, $P$ is a projection.
	(4) notice that $U^TU = I$.
	(5) use the theorem above that every orthogonal projection is self-adjoint.
\end{proof}




\subsubsection{Approximation by continuous polynomials}

\begin{lemma}[approximation via polynomial]\label{ch:functional-analysis:th:ApproximationViaPolynomial}Consider we want to approximate a function $f(x)$ in $C[a,b]$ using basis function $\{1,t,t^2,...,t^{m-1}\}$. The Grammian matrix is given as
$$R_{i,j}=\int_a^b t^{i+j}dt$$
If $[a,b]=[0,1]$, $R$ is known as \textbf{Hilbert matrix}.
Let 
$p\in \R^m$, $$p_i = \int_a^b f(x)t^{i-1}dt,i=1,2,...,m,$$
and solve $c = R^{-1}p$. Then we get the approximation for $f(x)$ given by
$$\hat{f}(x) = \sum_{i=1}^{m}c_i t^{i-1}.$$
\end{lemma}
\begin{proof}

\end{proof}
\begin{remark}[the Hilbert matrix is ill-conditioned]
	Note that the Hilbert matrix is ill-conditioned for large $m$, therefore it is not a good choice for approximation.
\end{remark}

\begin{lemma}[approximate discrete points using polynomial]\cite{moon2000mathematical}
Consider we have $n$ training data $\{(x_i,y_i)\},x_i\in \R^n, y_i\in \R$, then we want to approximate the vector $y=[y_1,y_2,...,y_n]^T$ by using polynomial basis $\{1,t,t^2,...\}$ sampled at values $x_1,x_2,...x_n$ as our basis set $\{p_1,p_2,...\},p_i\in \R^n$.

It follows that
\begin{itemize}
	\item if we have $n$ polynomials given by $\{1,t,t^2,...,t^{n-1}\}$, then denote
	$P=[p_1,...p_n]$, and we have 
	$$y=Pc \implies c = P^{-1}y.$$
	That is, the n points $\{x_i,y_i\}$ can be exactly passed through by the function defined by
	$$f(x) = \sum_{i=1}^{n} c_i x^{n-1}.$$
	\item if we have $m<n$ polynomials given by  $\{1,t,t^2,...,t^{m-1}\}$, then denote
	$P=[p_1,...p_m]$, and we have the least-square best approximation for $y$ given by
	$$\hat{y}=Pc \implies c = (P^TP)^{-1}P^Ty.$$
\end{itemize}
\end{lemma}
\begin{proof}
Note that the columns in $P$ are linearly independent(\autoref{ch:linearalgebra:th:LinearIndependenceOfPolynomials}). In both cases, we have(\autoref{ch:functional-analysis:th:normalequation})
$$c = (P^TP)^{-1}P^Ty.$$
When $P\in \R^{n\times n}$, $$(P^TP)^{-1}P^T = P^{-1}P^{-T}P^T = P^{-1}.$$
\end{proof}


\begin{remark}[connection to Lagrange polynomial approximation finite data set]
This lemma achieve the same conclusion as in \autoref{ch:functional-analysis:th:Lagrangepolynomialapproximation} that there exists a polynomial of degree $k-1$ passing $k$ points.	
\end{remark}

\subsubsection{Legendre polynomial via Gram-Schmidt process}
The set of functions $\{1,t,t^2,...\}$ defined on $[-1,1]$ forms a linearly independent set. Let the inner product be $$\ip{f,g}=\int_{-1}^1 f g dt$$
By Gram-Schmidt process, we will get the Legendre polynomial.




\section{Orthonormal systems}

\subsection{Basic definitions}
\begin{definition}[Orthogonal/orthonormal set]\index{orthonormal system}\index{orthonormal set}
	\cite[101]{debnath2005hilbert}
	Let $V$ denotes an inner product space, a set $S$ of non-zero elements $p_1,p_2,...$ that are orthogonal/orthonormal to each other are called orthogonal/orthonormal system/set.
\end{definition}

\begin{theorem}
	Orthonormal/orthogonal systems in an inner product space are linearly independent.
\end{theorem}

\begin{definition}[orthonormal sequence]
	\cite[101]{debnath2005hilbert} A sequence of vectors in an inner product space that are also orthonormal system is call orthonormal sequence.
\end{definition}


\subsection{Gram-Schmidt process}\index{Gram-Schmidt process}

\begin{mdframed}
	Given a set of linear independent vectors $T=\{p_1,p_2,...,p_n\}$, we want to find a set of vectors $T'=\{q_1,q_2,...,q_n\}$ such that
	$$span\{q_1,q_2,...,q_n\}=span\{p_1,p_2,...p_n\}$$
	
	We can obtained $T'$ using the following process:
	\begin{enumerate}
		\item $q_1 = p_1/\norm{p_1}$
		\item $q_i = p_i - \sum_{j=1}^i \ip{p_i,q_j} p_i, q_i = q_i/\norm{q_i},i=2,3.,n $
	\end{enumerate}
\end{mdframed}


\subsection{Properties of orthonormal systems}
\begin{theorem}[Pythagorean formula]\index{Pythagorean formula}
	If $x_1,x_2,...,x_n$ are orthogonal vectors in an inner product space, then
	$$\norm{\sum_{i=1}^n x_k}^2 = \sum_{k=1}^n \norm{x_k}^2$$
\end{theorem}
Proof: directly expand the left hand side and use orthonormal properties. 


\begin{theorem}[Bessels' inequality]\index{Bessel's inequality}
	\cite[305]{johnsonbaugh2010foundations}\cite[106]{debnath2005hilbert} Given an orthonormal system and an element $x$ in an inner product space $S$, we have
	$$\norm{x}^2 \geq \sum_{i=1}^n \ip{x,x_i}^2$$
	$$\norm{x}^2 \geq \sum_{i=1}^\infty \ip{x,x_i}^2$$
	and the series $\sum_{i=1}^\infty \ip{x,x_i}^2$ converge. 
\end{theorem}
\begin{proof}
	Let $c_i = \ip{x, x_i}$. Expand 
	\begin{align*}
	0 \leq \norm{x-\sum_i^\infty c_i x_i}^2 &= \ip{x-\sum_i^\infty c_i x_i,x-\sum_i^\infty c_i x_i}\\
	&= \ip{x,x}-2\ip{x,\sum_i^\infty c_i x_i} + \ip{\sum_i^\infty c_i x_i,\sum_i^\infty c_i x_i}\\
	&= \ip{x,x}-2\sum_i^\infty c_i \ip{x, x_i} + \sum_i^\infty c_i^2 \\
	&=\ip{x,x} - \sum_i^\infty \ip{x, x_i}^2
	\end{align*}
	
	The convergence can directly from bounded monotone sequence. 
\end{proof}


\begin{remark}
	\textbf{$\sum_{i=1}^\infty \ip{x,x_i}^2$ might not converge to $\norm{x}^2$ unless the orthornomal system is complete}.   
\end{remark}


\begin{remark}
	One important application of Bessel's inequality is to prove weak convergence or orthonormal sequence and Riemann-Lebesgue lemma.
\end{remark}


\begin{corollary}
	\cite[185]{debnath2005hilbert} Orthonormal sequences $\{x_n\}$ in inner product space $E$ will weakly converge to 0. Moreover, \textbf{the convergence cannot be strong convergence}.
\end{corollary}
\begin{proof}
	Because of Bessel's inequality, we have 
	$$\sum_{i=1}^\infty \ip{x,x_i}^2 < \infty$$
	which implies that $\ip{x,x_i}^2 \to 0$ as $i\to \infty$.
	\textbf{The convergence is not strong since $\norm{x_i}$ = 1}.
\end{proof}


\begin{theorem}[Riemann-Lebesgue Lemma for Fourier series]\index{Riemann-Lebesgue Lemma}
	\cite[306]{johnsonbaugh2010foundations} Let $f\in \cR[a,a+2\pi]$(Riemann integrable), then
	$$\lim_{n\to \infty} \int_a^{a+2\pi} f(t) \sin nt dt = 0 \lim_{n\to \infty} =\int_a^{a+2\pi} f(t) \cos nt dt$$
\end{theorem}

\begin{theorem}[general cases]
	\cite[306]{johnsonbaugh2010foundations}
	For any orthonormal sequence $\phi_1,\phi_2, \phi_3, ...$ and any $f$ in a Hilbert space $V$, $$\lim_{k\rightarrow \infty} \ip{f,\phi_k} = 0$$
\end{theorem}
\begin{proof}
	directly from the convergence of $\sum_{k=1}^\infty \ip{f,\phi_k}^2$ due to Bessel's inequality. 	
\end{proof}




\subsection{Orthonormal expansion in Hilbert space}
\begin{mdframed}
	\textbf{Note:}\\
	\begin{itemize}
		\item In finite $n$ dimensional inner product space, given an orthonormal basis $\{e_1,...,e_n\}$,  every element can be written as
		$$v = \ip{v,e_1}e_1 + ... + \ip{v,e_n}e_n$$
		\item In infinite dimensional Hilbert space, given an \textbf{orthonormal sequence},
		we have a series $s = \ip{v,e_1}e_1 + ... $. We want to know (1) the condition for its convergence; and (2) whether it will converge to $v$(completeness property).
	\end{itemize}
\end{mdframed}


\begin{theorem}[convergence condition of orthonormal series]\index{orthonormal expansion}
	Let $\{x_n\}$ be an orthonormal sequence in a Hilbert space $H$ and let $\{a_i\}$ be a sequence of complex numbers. Then the series $\sum_{i=1}^\infty a_n x_n$ converges if and only if $\sum_{i=1}^\infty a_i x_i < \infty$, and in that case
	$$\sum_{i=1}^\infty \abs{a_n}^2  = \sum_{i=1}^\infty a_n x_n$$
\end{theorem}
\begin{proof}
	(1) (forward part)Define $s_m = \sum_{i=1}^m a_n x_n$, we know that $s_m$ will converge if $s_m$ is a Cauchy sequence(since the \textbf{Hilbert space is complete}). And 
	$$\norm{s_m-s_k}^2 = \norm{\sum_{i=k}^m a_i x_i}  = \sum_{i=k}^m \abs{a_n}^2$$
	from Pythagorean formula. Note that the right-hand side if the Cauchy sequence of a convergent series $\sum_{i=1}^\infty \abs{a_n}^2 $; that is $\norm{s_m-s_k}^2$ can be made arbitrarily small by choosing $N$ and requiring $k>m>N$. Therefore $s_m$ will converge.
	(2) (converse part) We define $p_m = \sum_{i=1}^m \abs{a_n}^2$, we know that $p_m$ will converge if $p_m$ is Cauchy sequence(since the real line is complete). And 
	$$\abs{s_m-s_k}^2 = \norm{\sum_{i=k}^m a_i x_i}  = \sum_{i=k}^m \abs{a_n}^2$$
	Note that the middle part can be made arbitrarily small by choosing $N$ and requiring $k>m>N$. Therefore $p_m$ will converge.	
\end{proof}


\begin{corollary}[convergence condition of orthonormal expansion]
	Let $\{x_n\}$ be an orthonormal sequence in a Hilbert space $H$, let $x \in H$, then
	the orthonormal expansion
	$$\sum_{i=1}^\infty \ip{x,x_n} x_n$$
	will converge.
\end{corollary}
Proof: directly from above theorem and Bessel's inequality.





\subsection{Complete orthonormal system}
\begin{definition}[completeness]
	\cite[187]{moon2000mathematical}\cite[109]{debnath2005hilbert}An orthonormal set $\{p_1,p_2,...\}$ in a Hilbert space $S$ is complete if
	$$x = \sum_{i=1}^\infty \ip{x,p_i}p_i$$
	for every $x\in S$
\end{definition}


\begin{remark}[\textbf{meaning of equality}]
	It is important to note that here the equality means
	$$\lim_{n\to \infty} \norm{x - \sum_{k=1}^n \ip{x,x_k}x_k}$$
	which \textbf{usually does not imply pointwise convergence in $L^p,1\leq p <\infty$ norm}.(unless $\infty$ norm is used)
\end{remark}


\begin{mdframed}
	\textbf{An 'incomplete' orthonormal system}\\
	Consider the orthonormal system of $\{\sin(nx)/\sqrt{\pi},n=1,2,...\}$. To approximate the function $\cos(x)$, we have the series 
	$$\sum_{n=1}^\infty \ip{\cos(x),\sin(nx)}\sin(nx)$$
	This series will converge(from above theorem), but will converge to 0, since $\ip{\cos(x),\sin(nx)} = 0$.
	
	\textbf{We can interpret an 'incomplete' orthonormal system as a linearly independent set that cannot span the vector space even if it has infinitely many terms.}
\end{mdframed}


\begin{definition}[orthonormal basis]\textbf{orthonormal basis}
	An orthonormal system $B$ in an inner product space $E$ is called an orthonormal basis if every $x\in E$ has a \textbf{unique} representation
	$$x = \sum_{n=1}^\infty a_n x_n,a_i\in \C, x_i\in B$$
\end{definition}

\begin{theorem}[uniqueness of representation]
	Every complete orthonormal sequence in an inner product space $E$ is an orthonormal basis; that is, it can uniquely represent elements in $E$.
\end{theorem}
\begin{proof}
	Let $x = \sum_{i} a_i x_i = \sum_i b_i x_i$, we have
	$$0 = \sum_i (a_i - b_i)x_i \Rightarrow 0 = \norm{\sum_i (a_i - b_i)x_i}^2 = \sum_i \abs{a_i-b_i}^2$$
	where we use Pythagorean formula.	
\end{proof}




\begin{theorem}[completeness criterion]
	\cite[187]{moon2000mathematical}\cite[109-111]{debnath2005hilbert}A set of orthonormal functions $\{p_i,i=1,2,...\}$ is complete in a Hilbert space $S$ with induced norm if any of the following equivalent statements holds:
	\begin{enumerate}
		\item $\forall x\in S$,
		$$x=\sum_{i=1}^\infty \ip{x,p_i}p_i$$
		\item for any $\epsilon > 0$, there is an $N < \infty$ such that $\forall n \geq N$:
		$$\norm{x-\sum_{i=1}^N\ip{x,p_i}p_i} < \epsilon$$
		\item Parseval's equality holds: $$\norm{x}^2=\sum_{i=1}^\infty \ip{x,p_i}^2$$
		\item if $\ip{x,p_i} = 0$ for all $i$, then $x=0$.
		\item There is no nonzero function $f\in S$ for which the set $\{p_i,i=1,2,...\}\cup f$ forms an orthogonal set.
	\end{enumerate}
\end{theorem}


\begin{remark}\hfill
	\begin{itemize}
		\item Note that in the infinite dimension Hilbert space, not every orthonormal function sequence is complete. \textbf{This is one fundamental difference compared to finite dimensional space:} every orthonormal basis set of size $n$ can span the space. In infinite dimensional space, however, even thought two orthonormal set are both of size $\infty$, they are not equal in their express power. 
		\item Note that if we want to use the completeness property to do approximation, then this approximating is in the mean square sense instead of pointwise.
	\end{itemize}
\end{remark}










\subsubsection{Weierstrass approximation theorem for polynomials}
\begin{theorem}\cite[321]{johnsonbaugh2010foundations}
	Let $f$ be continuous on $[a,b]$ and let $\epsilon > 0$. Then there exists a polynomial $p$ such that 
	$$\abs{f(x)-p(x)} < \epsilon$$
\end{theorem}

\begin{remark}
	This theorem also suggests that polynomials are complete on the Hilbert space of continuous functions on $[a,b]$.
\end{remark}

\subsubsection{Examples of complete orthonormal function set}
For complete description and other exotic function set(Chebyshev polynomials, wavelets...)\cite[187]{moon2000mathematical}\cite[112]{debnath2005hilbert}
\paragraph{Fourier Series}
$$p_n(x)=\frac{1}{\sqrt{2\pi}}e^{i n t}$$
\paragraph{Discrete Fourier transform}
In the vector space $\R^n$, each element is given by $x[i],i=0,1,...,N-1$
$$p_k[t]=\frac{1}{\sqrt{N}}e^{i 2\pi k/N}$$

\paragraph{Legendre polynomials}
For vector space $L^2[-1,1]$, 
$$p_0(t)=1,p_1(t)=1,p_2(t)=t^2-1/3...$$





\section{Theory for trigonometric Fourier Series}
\begin{definition}[Fourier series of a function]
	A trigonometric series 
	$$\frac{1}{2}a_0 + a_1 \cos(x) + b_1 \sin(x) + ...$$
	in which 
	$$a_n = \frac{1}{\pi}\int_{-\pi}^\pi f(x)\cos(nx) dx,n=0,1,2,...$$
	
	$$b_n = \frac{1}{\pi}\int_{-\pi}^\pi f(x)\cos(nx) dx,n=1,2,...$$
	is called the Fourier series of $f(x)$.
\end{definition}

\begin{remark}
	Here we define what a Fourier series of a function is; the existence of such Fourier series only require the existence of above definite integral to calculate $a_n,b_n$.
\end{remark}

\begin{remark}
	Given the existence of the Fourier series, another question we are concerned with is whether such Fourier series will converge to $f(x)$.
\end{remark}

\begin{theorem}
	\cite[470]{kaplan1973advanced}Every uniformly convergent trigonometric series is a Fourier series; Let $f(x)$ be the limit of the trigonometric series, then $f(x)$ is continuous for all $x$, with period of $2\pi$.
\end{theorem}
Proof: By the orthogonal property of trigonometric property, we can verify that $a_n,b_n$ are met by the definitions. Also because trigonometric function is continuous and has period $2\pi$, then the limit function $f(x)$ is continuous and has period $2\pi$ since uniform convergence will preserve continuity.

\begin{corollary}
	If two trigonometric series converge uniformly for all $x$ and have the same sum for all $x$, then the two series are equal in their coefficients. 
\end{corollary}
Proof: Let $f(x)$ be the limits, the coefficients are uniquely determined by the definite integral.

\begin{remark}
	This corollary answer the question of uniqueness of Fourier series. 
\end{remark}


\begin{theorem}[\textbf{fundamental theorem, local convergence}]
	\cite[472]{kaplan1973advanced}
	Let $f(x)$ be piece $C^2$ in the interval $-\pi\leq x \leq \pi$. Then the Fourier series of $f(x)$:
	$$\frac{1}{2}a_0 + a_1 \cos(x) + b_1 \sin(x) + ...$$
	in which 
	$$a_n = \frac{1}{\pi}\int_{-\pi}^\pi f(x)\cos(nx) dx,n=0,1,2,...$$
	
	$$b_n = \frac{1}{\pi}\int_{-\pi}^\pi f(x)\cos(nx) dx,n=1,2,...$$
	\textbf{converges uniformly} to $f(x)$ whenever $f(x)$ is continuous inside the interval. The series converge to 
	$$\frac{1}{2}[\lim_{x\to x_1-} f(x) + \lim_{x\to x_1+} f(x)]$$
	at the point of discontinuity $x_1$ inside the interval, and to 
	$$\frac{1}{2}[\lim_{x\to \pi-} f(x) + \lim_{x\to \pi+} f(x)]$$
	at $x=\pm \pi$
\end{theorem}

\begin{remark}
	The most important aspect of this theorem is that for a piece-wise $C^2$ function, its Fourier series will converge uniformly to it. 
\end{remark}




\begin{remark}[\textbf{uniform convergence vs mean-square convergence}]\hfill
	\begin{itemize}
		\item uniform convergence is a much stronger convergence than the mean-square convergence; We usualy call uniform convergence as local convergence, and mean-square convergence as global convergence; \cite[69]{stein2011fourier}
		\item For approximation applications, we usually only concerns mean-square error, i.e., mean-square convergence;
		\item The completeness concept is defined based on mean-square convergence rather than local convergence. 
	\end{itemize}
\end{remark}

\begin{remark}
	\cite[83]{stein2011fourier} \textbf{Continuity cannot guarantee the convergence of Fourier series}, except that other conditions are allowed 
\end{remark}


\begin{lemma}
	\cite[41]{stein2011fourier} Suppose that $f$ is continuous function on $[-\pi,\pi]$, and the Fourier coefficients satisfy $\sum_{i=1}^\infty a_i^2 + b_i^2 < \infty$, i.e., convergent. Then the Fourier series converges uniformly to $f$. 
\end{lemma}
\begin{proof}
	the continuity guarantees the integrability. And then use M-test for uniformly convergence.	
\end{proof}

\subsection{Sufficient condition for pointwise convergence: Dirichlet conditions}
In mathematics, the Dirichlet conditions are sufficient conditions for a real-valued, periodic function f(x) to be equal to the sum of its Fourier series at each point where f is continuous. Moreover, the behavior of the Fourier series at points of discontinuity is determined as well (it is the midpoint of the values of the discontinuity). These conditions are named after Peter Gustav Lejeune Dirichlet.
The conditions are:
\begin{enumerate}
	\item f(x) must be absolutely integrable over a period.
	\item f(x) must have a finite number of extrema in any given bounded interval, i.e. there must be a finite number of maxima and minima in the interval.
	\item f(x) must have a finite number of discontinuities in any given bounded interval, however the discontinuity cannot be infinite.
\end{enumerate}
These three conditions are satisfied if f is a function of bounded variation over a period.\cite{wiki:Dirichlet}

For a piece-wise smooth and periodic function $f$ defined on the interval $[-\pi, \pi]$, then the Fourier series converges to $(f(x+)+f(x-))/2$.

\subsection{Completeness of Fourier series}
\begin{remark}[completeness]
	The completeness theorem says that a continuous periodic function $f$ equals its Fourier series. This means that the set of function $cos(nt),sin(nt)$ form a complete set of basis function, and you do not need any more functions to express any periodic functions as a linear combination.To prove this, we need to show the fourier series $f_1$ of any function $f$ are equal at any point in the domain(we need to construct delta function to prove this).
\end{remark}



\begin{corollary}
	The trigonometric set is complete in $CP[a,a+2\pi]$ (the function space of continuous $2\pi$ periodic functions )\cite{johnsonbaugh2010foundations}
\end{corollary}

\begin{corollary}[Fourier cosine series and sine series]
	The Fourier cosine and sins series are complete for even and odd piecewise continuous functions defined on $[-\pi,\pi]$ respectively.
\end{corollary}

\begin{lemma}[best approximations]
	\cite[78]{stein2011fourier} Let $S_N$ be the partial sum of the Fourier series of $f(x)$(assuming that the Fourier coefficient in $S_N$ exists), then
	$$\int_{-\pi}^\pi \norm{f(x) - S_N(x)}dx \leq \int_{-\pi}^\pi \norm{f(x) - \sum_{k=0}^N c_k \cos(kx) + d_k \sin(kx)}dx$$
	for any $c_i$ and $d_i$.
\end{lemma}

\begin{remark}
	Note that we did not impose any condition on $f$ except that it is Riemann integrable when calculating the Fourier coefficients.  
\end{remark}

\subsection{Complex representation}

\begin{lemma}[complex representation of Fourier series]\cite[271]{prosperetti2013advanced}
	Consider a real-value Fourier series in the interval $[-\pi,\pi]$ given by
	$$f(x) = \frac{1}{2}a_0 + \sum_{k=1}^\infty a_k \cos(kx) + b_k \sin(kx).$$
	It can also be represented by the following complex form:
	$$f(x) = \sum_{k= -\infty}^\infty c_k \exp(ikx)  ,$$
	where 
	$$c_k = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x)\exp(-ikx)dx .$$
	\begin{itemize}
		\item $c_0 = \frac{1}{2}a_0$
		\item for $k\neq 0$,$$c_k = a_k + ib_k, c_{-k} = a_k - i b_k.$$
		\item $c_k = \conj{c_{-k}}$.
	\end{itemize}
	
\end{lemma}
\begin{proof}
	Use $$cos(kx) = \frac{e^{ikx} + e^{-ikx}}{2},sin(kx) = \frac{e^{ikx} - e^{-ikx}}{2i}.$$
	The match of $cos(kx)$ coefficients requires
	$$(\frac{c_k}{2} + \frac{c_{-k}}{2}) = a_k.$$
	The match of $sin(kx)$ coefficients requires
	$$(\frac{c_k}{2} - \frac{c_{-k}}{2}) = ib_k.$$
	We can solve
	$$c_k = a_k + ib_k, c_{-k} = a_k - i b_k.$$
	The rest is straight forward.
\end{proof}

\section{Fourier transform}
\subsection{Definitions and basic concepts}
\begin{definition}[Fourier transform]\index{Fourier transform}
	\cite[267]{prosperetti2013advanced} The Fourier transform of the function $f \in L^1(\R)$ is given as
	$$\cF[f] = \tilde{f}(k) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{ikx}f(x)dx$$
	and the inverse Fourier transform
	$$\cF^{-1}[\tilde{f}] = f(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{-ikx}\tilde{f}(k)dk.$$
	
	Usually we denote $\conj{f} = F$.
\end{definition}


\begin{remark}\hfill
	\begin{itemize}
		\item Because $\abs{e^{ikx}f(x)} \leq \abs{f(x)}$, limit the domain to $L^1(\R)$ guarantees the existence of Fourier transform.
		\item \cite[194]{debnath2005hilbert} Not all functions in $L^2(\R)$ has the existence of Fourier transform.
	\end{itemize}
\end{remark}



\begin{definition}[Fourier transform basis]
	The Fourier transform basis function with frequency $k \in  (-\infty,\infty)$ is defined as
	$$e_k(x) = 1/\sqrt{2\pi}\exp(ikx)$$
	where $x \in  (-\infty,\infty).$
	
	Define $\ip{f,g} = \int_{-\infty}^{\infty} f \conj{g} dx$, we can redefine Fourier transform as
	$$\cF[f](k) = \ip{e_k,f}, f(x) = \ip{e_k,\ip{e_k,f}}.$$
	
\end{definition}

\begin{lemma}[unitary property of basis function]\label{ch:function-sequences-series--approximation:th:unitaryPropertyFourierTransformBasis}
	\cite[268]{prosperetti2013advanced}The basis function of the Fourier transform satisfies:
	$$\ip{e_k,e_{k'}} = \int_{-\infty}^{+\infty} e^{ikx }\conj{e^{ik'x}}dx = 2\pi \delta(k-k')$$
	Some equivalent forms are:
	$$\int_{-\infty}^{+\infty} e^{i k x}dx = 2\pi\delta(k)$$
	or
	$$\int_{-\infty}^{+\infty} e^{i (k-k')x}dx = 2\pi\delta(k-d')$$
\end{lemma}
\begin{proof}
	$$\lim_{k\to \infty} \frac{1}{2\pi} \int_{-k}^k \exp[ik(\eta - x)] dk = \lim_{k\to \infty} \frac{\sin K(\eta - x)}{\pi (\eta - x)} = \delta(\eta - x)$$
	here we use the definition of $\delta$ function.
\end{proof}

\begin{remark}[real inner product vs unitary inner product]
	Note that the basis is unitary instead of orthonormal: $\int_{-\infty}^{+\infty}e^{ik_1 x}e^{i k_2 x} dx =2\pi \delta(k_1-(-k_2)) = 2\pi \delta(k_1 + k_2)  \neq 2\pi \delta(k_1-k_2)$,i.e., they are not orthogonal(in the sense of real inner product) for different $k$.
\end{remark}

\begin{remark}
	This property is the key of inverse Fourier transform to hold:
	\begin{align*}
	f(x) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} e^{-i k x} F(k) dk \\
	&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} e^{-i k x}\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} f(x')  e^{i k x'}dx'dk \\
	&=\frac{1}{2\pi}\int_{-\infty}^{+\infty} e^{-i k (x-x')} dx'dk = f(x)
	\end{align*}
\end{remark}


\begin{corollary}[Fourier transform pair of 1 and $\delta$]
	We have
	$$\int_{-\infty}^{\infty} 1 e^{ikx}dx = 2\pi \delta(k)$$
	and
	$$\frac{1}{2\pi} \int_{-\infty}^{\infty} 2\pi \delta(k) e^{-ikx}dk = 1$$
\end{corollary}



\begin{lemma}[sufficient condition for the existence of Fourier transform]
	
\end{lemma}


\begin{lemma}[basic properties]\cite[269]{prosperetti2013advanced}
	Suppose the Fourier transform of $f$ exists, then we have
	\begin{itemize}
		\item $\cF^2[f] = \cF[\cF[f]] = f(-x)$
		\item $\cF^{-1}[f] = \tilde{f}(-k)$
		\item For a real function, $\conj{\tilde{f}}(k) = \tilde{f}(-k)$.
	\end{itemize}
\end{lemma}
\begin{proof}
	(3)
	
	$$\conj{ \tilde{f}(k)} = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty \conj{e^{ikx}f(x)}dx = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{-ikx}f(x)dx = \tilde{f}(-k)$$
\end{proof}

\begin{lemma}[Parseval equality]\index{Parseval equality}\cite[269]{prosperetti2013advanced}
	\cite[269]{prosperetti2013advanced}Let $f$ be an integrable function, then
	$$\norm{f}_2^2 = \norm{\tilde{f}}_2^2$$
	more generally,
	$$\ip{f,g} = \ip{\tilde{f},\tilde{g}}$$
\end{lemma}
\begin{proof}(informal)
	\begin{align*}
	\norm{f}^2 &= \int_{-\infty}^{\infty} f(x)\conj{f}(x)dx \\ 
	\norm{f}^2 &= \frac{1}{2\pi}\int_{-\infty}^{\infty} \int_{-\infty}^\infty e^{-ikx}F(k)dk\int_{-\infty}^\infty e^{ikx}\conj{F}(k')dk'dx \\ 
	&=\frac{1}{2\pi} \int_{-\infty}^\infty F(k)dk\int_{-\infty}^\infty \conj{F}(k')2\pi(k-k')dk' \\
	&= \int_{-\infty}^\infty F(k)\conj{F}(k')dk = \norm{F}^2
	\end{align*}
\end{proof}



\subsection{Convolution theorem}
\begin{definition}
	The convolution of $f$ and $g$ is defined as
	$$f*g(z) = \int_{-\infty}^{\infty} f(x)g(z-x)dx = \int_{-\infty}^{\infty} f(z-x)g(x)dx.$$
\end{definition}

\begin{remark}[equivalence of two forms]
	\begin{align*}
	\int_{-\infty}^{\infty} f(x)g(z-x)dx &= \int_{\infty}^{-\infty} f(z-y)g(y)d(z-y) \\
	&= -\int_{\infty}^{-\infty} f(z-y)g(y)d(y) \\
	&= \int_{-\infty}^{\infty} f(z-y)g(y)dy\\
	& = \int_{-\infty}^{\infty} f(z-x)g(x)dx
	\end{align*}	
\end{remark}


\begin{lemma}[convolution theorem]\index{convolution theorem}\cite[271]{prosperetti2013advanced}
	$\cF[f*g] = \sqrt{2\pi} \tilde{f}\tilde{g}$
	and
	$\cF^{-1}[f*g] = 1/\sqrt{2\pi} (f*g)(x)$
\end{lemma}
\begin{proof}
	\begin{align*}
	f*g(z) &= \int_{-\infty}^{\infty} f(x)g(z-x) dx \\
	\implies \cF[f*g(z)] &= \int_{-\infty}^{\infty} e^{ikz}\int_{-\infty}^{\infty} f(x)g(z-x) dx dz \\
	&=\int_{-\infty}^{\infty} e^{ikx}\int_{-\infty}^{\infty} f(x)e^{ik(z-x)}g(z-x) dx dz \\
	&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{ikx}\int_{-\infty}^{\infty} f(x)e^{iky}g(y) dx dy \\
	&=\sqrt{2\pi} \cF[f]\cF[g]
	\end{align*}	
\end{proof}



\subsection{Fourier transform and Fourier series}

\begin{lemma}\cite[271]{prosperetti2013advanced}
	Consider a real-value function in $\R$ given by
	$$f(x) = \frac{1}{2}a_0 + \sum_{k=1}^\infty a_k \cos(kx) + b_k \sin(kx) = \sum_{k=-\infty}^{\infty} c_k e^{ikx}.$$	
	
	It follows that
	\begin{itemize}
		\item the Fourier series of $f(x)$ \textbf{on the interval} $[-\pi,\pi]$ is
		$$\sum_{k=-\infty}^{\infty} c_k e^{ikx},$$
		where 
		$$c_k = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x)\exp(-ikx)dx .$$
		\item the Fourier transform of $f(x)$ is given by
		$$\tilde{f}(k) \triangleq \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{ikx} f(x)dx = \sqrt{2\pi} \sum_{k'=-\infty}^\infty c_{k'}\delta(k+k').$$
	\end{itemize}
\end{lemma}
\begin{proof}
	(1) 
	$$\sum_{k'=-\infty}^\infty c_k e^{ikx} = \sum_{k'=-\infty}^\infty \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x)\exp(-ik'x)\exp(ikx) dx  = f(x). $$
	
	(2)	
	$$\tilde{f}(k) \triangleq \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{ikx} f(x)dx  =  \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{ikx}  \sum_{k=-\infty}^{\infty} c_k e^{ikx} dx = \sqrt{2\pi} \sum_{k'=-\infty}^\infty c_{k'}\delta(k+k').$$
\end{proof}

\begin{remark}\hfill
	\begin{itemize}
		\item Note that Fourier series only applies to periodic function defined on finite intervals.
		\item Fourier transform can apply to function defined on $\R$ as long as the integral exists. 
		\item For a periodic function, the Fourier transform will generate spikes. 
	\end{itemize}
\end{remark}



\subsection{Discrete Fourier transform}
\subsubsection{Properties}\index{discrete Fourier transform matrix}
\begin{definition}
	Given an positive integer $N$, let $w = \exp(2\pi i/N)/\sqrt{N}$, a Fourier transform matrix $F$ is given by $$F_{jk} = w^{jk}, j =0,1,...,N-1; k=0,1,...,N-1$$
\end{definition}


\begin{lemma}
	Basic properties of Fourier transform matrix:
	\begin{itemize}
		\item $w^{jN} = 1/\sqrt{N} = w^0,j=0,1,...,N-1$
		\item $F$ is symmetric
		\item $\sum_{j=0}^{N-1} w^{mj} = \delta(m)\sqrt{N}$
		\item columns of $F$ form an orthonormal basis, i.e., each column has unit 1 and different columns are orthogonal.
		\item $F$ is symmetric.
		\item $w^{N+k} = w^k, w^{N/2 + k} = - w^{k}$.
	\end{itemize}
\end{lemma}
\begin{proof}
	(1) and (2) are easy. (3) use
	$$(w^0+w^1+...+w^{N-1})(1-w)=1-w^N = 1-1 = 0$$
	if $w\neq 1$, then $w^0+w^1+...+w^{N-1} = 0$
	We now prove (4). Unit length: $$1/\sqrt{N}^2\sum_{j=0}^{N-1} w^{mj} \conj{w^{mj}} = 1/\sqrt{N}^2 N = 1$$
	where we use $\abs{w^{jm}} = 1$.
	Orthonormal: $$1/\sqrt{N}^2\sum_{j=0}^{N-1} w^{mj} \conj{w^{nj}}=1/\sqrt{N}^2\sum_{j=0}^{N-1} \exp((m-n)ji2\pi/N) = \delta(m-n) $$
	where we have use the property in (3).	
\end{proof}


\begin{theorem}
	The columns of $F$ is a complete orthonormal set in space of complex vectors in $\C^N$.
\end{theorem}
\begin{proof}
	Because $\C^N$ has dimension of $N$ and columns in $F$ is linearly independent and have length $N$, therefore $F$ is a basis of $\C^N$.	
\end{proof}


\begin{lemma}
	The conjugate of matrix $F$ has the same property of $F$
\end{lemma}
\begin{proof}
	this can be directly verified.	
\end{proof}


\begin{mdframed}
	\textbf{Note: Because $F$ and $\conj{F}$ have the same property above, there is no essential differences in using $F$ or $\conj{F}$ in performing discrete Fourier transform.}
\end{mdframed}

\begin{lemma}
	Let $F$ be the Fourier transform matrix, let $\conj{F}$ be the conjugate matrix, then $$F\conj{F} = FF^H = I$$
	That is, $F$ is unitary.
\end{lemma}
\begin{proof}
	directly from the property of $F$ ($F$ is symmetric).	
\end{proof}



\subsubsection{Transform and inverse transform}

\begin{definition}\index{discrete Fourier transform}
	Given a vector $v\in \F^N$, the discrete Fourier transform is given as
	$$\hat{v} = Fv$$
	and the inverse discrete Fourier transform is given as
	$$v = \conj{F}\hat{v}$$
\end{definition}

\begin{remark}
	The inverse transform can recover the original vector is directly from the fact that $F$ is unitary,i.e., $v= \conj{F}Fv$.
\end{remark}





\section{Linear operators in functional analysis}
\subsection{Fundamentals}
\begin{definition}[linear operator]
A linear operator $T$ is an operator such that
\begin{itemize}
    \item the domain $\mathcal{D}(T)$ and the range $\mathcal{R}(T)$ are vector spaces over the same field.
    \item for all $x,y \in \mathcal{D}(T)$ and scalars $\alpha,\beta$, we have 
    $$T(\alpha x +  \beta y )= \alpha T(x) + \beta T(y) $$
\end{itemize}
\end{definition}

\begin{remark}\hfill
\begin{itemize}
    \item linear operator will map zero vector to zero vector $T 0 = 0$
    \item linear operator is a homomorphism from one vector space to another vector space; that is, the two operations of the vector spaces are preserved. 
\end{itemize}
\end{remark}

\begin{example}\hfill
\begin{itemize}
    \item Differentiation. Let $X$ be the vector space of all polynomials defined on $[a,b]$. We define $$Tx(t) =x'(t)$$
    \item Integration. A linear operator $T$ from $C[a,b]$ into itself can be defined as
    $$Tx(t) =\int_a^t x(t')dt'$$
\end{itemize}
\end{example}

\begin{theorem}[range and null space]
\cite[85]{kreyszig1989introductory}Let $T$ be a linear operator, then
\begin{enumerate}
\item The range $\cR(T)$ is a vector space
\item If $dim(\cD(T))=n < \infty$, then $dim(\cR(T)) \leq n$
\item The null space $\cN(T)$ is a vector space.
\end{enumerate}
\end{theorem}
\begin{proof}
(1) The key is to show $\cR(T)$ is nonempty and closed under addition and scalar multiplication based on the subspace theorem. $T(x+y) = Tx + Ty, Tx,Ty \in \cR(T), T(x+y)\in \cR(T)$ and $aT(x) = T(ax), Tx \in \cR(T), aTx \in \cR(T)$ show closeness. And $\cR(T)$ is nonempty, it will always contain zero element. Then $\cR(T)$ is a subspace hence a vector space. \\(2) Consider a set of $n$ basis $x_1,x_2,...,x_n$ in $\cD(T)$, then for any $y \in \cR(T)$, there is a $x \in \cD(T)$ with $Dx = y$ and $x = \sum_{i=1}^n c_i x_i$, hence $y = \sum_{i=1}^n Tx_i = \sum_{i=1}^n y$, hence the dimensionality of $\cR(T)$ must be less than $n+1$.\\(3) similar to the proof of (1).	
\end{proof}


\begin{remark}
(2) can also be understood as linear operators preserve linear dependence, not necessarily linear independence.
\end{remark}

\begin{theorem}[inverse operator]
\cite[88]{kreyszig1989introductory} Let $T$ be a linear operator, then:
\begin{enumerate}
\item the inverse $T^{-1}: \cR(T)\to \cD(T)$ exists if and only if $$Tx = 0 \Rightarrow x = 0$$
\item If $T^{-1}$ exists, it is a linear operator.
\item If $dim(\cD(T))=n < \infty$, then $dim(\cD(T))=dim(\cR(T)) = n$
\end{enumerate}
\end{theorem}
Proof: (1) Since $T$ is onto $\cR(T)$, we only need to prove the mapping is 1-1. Suppose $x_1\neq x_2$, but $T(x_1)=T(x_2) \Rightarrow T(x_1-x_2)=0 \Rightarrow x_1=x_2$, contradict. Therefore, it must be 1-1 and the inverse exists. \\
(2) $T^{-1}(a+b) = T^{-1}(a) + T^{-1}b$,because $T(T^{-1}a + T^{-1}b) = a + b$;  $T^{-1}(\lambda a) = \lambda T^{-1}(a)$ because $T(\lambda T^{-1}(a)) = \lambda TT^{-1}a = \lambda a$\\
(3) Use inequality in both directions.  

\begin{remark}\hfill
\begin{itemize}
    \item Bijective linear operators will preserve linear independence.
    \item Bijective linear operator is also called isomorphism. 
\end{itemize} 
\end{remark}

\begin{lemma}[Inverse of composition]
Let $T$ and $S$ be bi-jective linear operators, then $(ST)^{-1} = T^{-1}S^{-1}$.
\end{lemma}


\subsection{Bounded and continuous linear operator}
\subsubsection{Operator norm}
\begin{definition}[operator norm]\index{operator norm}
The \emph{operator norm} of $T$, denoted $\norm{T}$, is defined as:
$$\norm{T} = \sup_{\norm{v}=1}\norm{Tv},\forall v\in V$$
\end{definition}
Simple example is linear operator represented by matrix. For operator norm can be obtained based on the norm defined on the vector space. 


\begin{example}\hfill
\begin{itemize}
    \item unbounded differentiation operator: Let $Tx=x'$, for polynomials defined on $[0,1]$, we have $T(x^n) = nx^{n-1}$. Let $\norm{x}=1$, we have $\norm{T} = n$, which can be arbitrarily large.
    \item bounded integral operator $y(t)=Tx=\int_0^1 k(t,s) x(s)ds$ for $x \in C[0,1]$, when $k$ is continuous on $[0,1]\times[0,1]$. Because $k$ is continuous, then it is bounded, therefore $\sup_{\norm{x}}\norm{Tx}$ is bounded when we define $\norm{x} = \max{\abs{x}}$.
    \item bounded linear mapping via matrix. Let $y=Ax$, then $\norm{Ax}^2 = x^TA^TAx$, which is bounded by the largest eigenvalue of semi-positive definite matrix $A^TA$.
\end{itemize}
\end{example}

\begin{remark}
(3) shows that finite dimensional linear mapping is always bounded.
\end{remark}


\begin{lemma}[properties of operator norm]\label{ch:functional-analysis:th:operatornormproperties}
\cite[143]{debnath2005hilbert}Let $A,B$ be the linear operator on normed space $V$, then
\begin{itemize}
    \item $\norm{A+B} \leq \norm{A} + \norm{B}$
    \item If $A,B$ are bounded, then $\norm{AB}\leq \norm{A}\norm{B}$
    \item $\norm{A^n} \leq \norm{A}^n$
    \item $\norm{\lambda A} = \abs{\lambda}\norm{A},\lambda\in \F$
    \item $\norm{Ax} \leq \norm{A}\norm{x}$
\end{itemize}
\end{lemma}
\begin{proof}
 (1) 
\begin{align*}
\norm{A+B} &= \sup_{\norm{x}=1}\norm{(A+B)x} = \sup_{\norm{x}=1}\norm{Ax + Bx} \\
& \leq \sup_{\norm{x}=1}(\norm{Ax} + \norm{Bx}) \leq  \sup_{\norm{x}=1}\norm{Ax}\\
& + \sup_{\norm{x}=1}\norm{Bx}) = \norm{A} + \norm{B} 
\end{align*}
(2) 
$$\norm{AB} = \sup_{\norm{x}=1}\norm{ABx} \leq \sup_{\norm{x}=1}\norm{A}\norm{Bx} = \norm{A}\norm{B}$$
where we use the result of (5).
(3) from (2);\\
(4) use the norm definition in $V$ that $\norm{\lambda x} = \abs{\lambda}\norm{x}$
(5) $\norm{Ax} = \norm{Ax/\norm{x} \norm{x}} \leq \sup_{\norm{y}=1} \norm{Ay}\norm{x}=\norm{A}\norm{x}$ 
where $y = x/\norm{x}$	
\end{proof}



\begin{theorem}[linear operators form a normed vector space]
\cite[23]{debnath2005hilbert} If $E_1$ and $E_2$ are normed spaces, then the set of all bounded linear operators from $E_1$ to $E_2$, denoted by $\cB(E_1,E_2)$
, is a normed vector space with norm defined by
$$\norm{L} = \sup_{\norm{x}=1}\norm{L(x)},\forall L\in \cB(E_1,E_2)$$
\end{theorem}
Proof: (1) it is obviously a vector space; (2) the norm satisfies triangle inequality, and homogeneity; moreover, $\norm{L} = 0$ if and only if $L$ is a zero operator. Therefore $\cB(E_1,E_2)$ form a normed vector space.


\subsubsection{Bounded and continuous operators}
\begin{definition}[bounded linear operator]\index{bounded linear operator}
Let $V,W$ be two normed vector spaces. An operator $T: V\rightarrow W$ is called bounded operator if there is some positive constant $C$ such that:
$$\norm{Tv} \leq C\norm{v},\forall v\in V$$
\end{definition}


\begin{definition}[continuous operator]
Let $T$ be an operator between normed space $X$ and $Y$ (not necessarily linear operator), then $T$ is continuous at $x_0$ if for any $\epsilon > 0$, there exist a $\delta > 0$, such that 
$$\norm{Tx - Tx_0} < \epsilon, \forall \abs{x-x_0} <\delta$$
\end{definition}

\begin{theorem}[continuity and boundedness]
Let $T$ be a linear operator, then:
\begin{itemize}
    \item $T$ is continuous if and only if $T$ is bounded.
    \item If $T$ is continuous at a single point, it is continuous every where
\end{itemize}
\end{theorem}
\begin{proof}
(1)If $T$ is bounded, then for $x_0 \in x$, $$\norm{Tx-Tx_0} \leq M\norm{x-x_0}$$ Given $\epsilon > 0$, there exist a $\delta = \epsilon/M$, such that $\forall x, \norm{x-x_0} \leq \epsilon/M, \norm{Tx-Tx_0} \leq \epsilon$. If $T$ is continuous, assume continuous at $0$, then for $\epsilon = 1$, there exist $\delta > 0$, such that $\norm{Tx}\leq 1,\forall \norm{x}\leq \delta$. For any $x \in X$, $\norm{Tx} = \norm{T \delta x/\norm{x}} \norm{x}/\delta = \norm{x}/\delta$, therefore it is bounded by $1/\delta$.	
\end{proof}



\begin{theorem}[boundedness of linear operator on finite dimension vector space]\cite[96]{kreyszig1989introductory} If a normed space $X$ is finite dimensional, then every linear operator on $X$ is bounded, and thus continuous 
\end{theorem}
\begin{proof}
$\norm{Tx} = \norm{T(\sum_{i=1}^n c_i e_i)} = \norm{\sum_{i=1}^n c_i Te_i}\leq \sum_{i=1}^n\abs{c_i} \norm{Te_i} < \max_k \norm{Te_k} \sum_{i=1}^n \abs{c_i}$
And later it can be showed that $\sum_{i=1}^n \abs{c_i}$ is bounded. 	
\end{proof}

\begin{theorem}
\cite[141]{debnath2005hilbert}Any linear operator on $\C^n$ is bounded, and thus continuous.
\end{theorem}
\begin{proof}
Because any linear operator in $\C^n$ can be represented by a matrix $A$. Then for any $x = \sum_{i=1}^n \lambda_i e_i \in \C^n$, we have
$$Ax = \sum_{i=1}^n \lambda_i Ae_i = \sum_{i=1}^n\sum_{j=1}^n \lambda_i a_{ij} e_j$$
where $a_{ij} = \ip{Ae_i,e_j}$
Then 	
\end{proof}


\subsubsection{Linear functionals}
\begin{definition}[linear functional]
A linear functional $f$ is a linear operator with domain in a vector space $X$ and range in the scalar field $K$ of $X$: thus
$$f:\cD(f)\to K$$
where $K=\R$ or $K=\C$.
\end{definition}

\begin{example}\hfill
\begin{itemize}
    \item Norm as nonlinear operator. The norm on a normed vector space is not a linear functional.
    \item Dot product. The dot product $f(x)=\sum_{i=1}^n c_i x_i,x\in X$ is a linear functional.
\end{itemize}
\end{example}




\begin{definition}[algebraic dual space]
The set of all linear functionals on the vector space $X$ will form a vector space $V$ under the following addition and scalar multiplication:
$$(\alpha f_1 + \beta f_2 ) (x) = \alpha f_1(x) + \beta f_2(x),\forall \alpha,\beta \in \F, x\in X,f_1,f_2 \in V$$
This vector space $V$ is called algebraic dual space of $X$.
\end{definition}

\subsubsection{Dual space}
\begin{definition}\cite[106]{luenberger1969optimization}
Let $X$ be a normed linear space. The space of all \textbf{bounded} linear functionals on $X$ is called normed dual of $X$ and is denoted as $X^*$. The norm of an element $f\in X^*$ is
$$\norm{f} = \sup_{\norm{x}=  1} \abs{f(x)}$$
\end{definition}

\begin{theorem}
\cite[107]{luenberger1969optimization}$X^*$ is a Banach space.
\end{theorem}
\begin{proof}
 $X^*$ is known to be a normed linear space, thus we need to show $X^*$ is complete. 	
\end{proof}


\begin{mdframed}
Notation: Given $x\in X,x^* \in X^*$, $\ip{x,x^*}$ denotes $x^*(x)$. And we have
$$\ip{x,x^*} = x^*(x) \leq \norm{x^*}\norm{x}$$
by the definition of norm of $X^*$.
\end{mdframed}

\begin{definition}
\cite[116]A vector $x^* \in X^*$ is said to be \textbf{aligned} with a vector $x\in X$ if $$\ip{x,x^*} = \norm{x^*}\norm{x}$$
\end{definition}

\begin{remark}
Alignment is a relation between vectors in \textbf{two distinct vector spaces}: a normed space and its normed dual.
\end{remark}


\begin{definition}[hyperplanes in linear space]\index{hyperplane}
\cite[129]{luenberger1969optimization}A hyperplane $H$ in a linear vector space $X$ is a maximal \textbf{proper} linear variety, that is, a linear variety $H$ such that $H\neq X$, and if $V$ is any linear variety containing $H$, then either $V=X$ or $V=H$.
\end{definition}

\begin{theorem}
\cite[129]{luenberger1969optimization}Let $H$ be a hyperplane in a linear vector space $X$. Then, there is a linear functional $f$ on $X$ and a constant $c$ such that $H = \{f(x) = c\}$. Conversely, if $f$ is a nonzero linear functional on $X$, the set $x:f(x) = c$ is a hyperplane in $X$.
\end{theorem}



\section{Linear operators on Hilbert spaces}\label{ch:functional-analysis:sec:linear-operators-on-hilbert-spaces}
\begin{theorem}[Riesz representation theorem]\index{Riesz representation theorem}
\cite[126]{debnath2005hilbert} Let $f$ be a bounded linear functional on a Hilbert space $H$. There exists exactly one $x_0 \in H$ such that $f(x) = \ip{x,x_0}$ for all $x \in H$. Moreover, we have $\norm{f} = \norm{x_0}$
\end{theorem}




\subsection{Adjoint operators}
\begin{definition}[adjoint operator]
\cite[151]{debnath2005hilbert}
Let $V,W$ be Hilbert spaces. The adjoint (also called Hermitian conjugate) of a linear operator $T:V \rightarrow W$, denoted $T^H$ is the unique linear operator $T^H:W\rightarrow V$ that satisfies $$\ip{Tv,w}=\ip{v,T^Hw},\forall v\in V, w\in W$$
\end{definition}

\begin{remark}
Note that (1)  for linear operators on finite dimensional vector space, we  can also define the adjoint of a linear operators as its transpose complex conjugate of the representing matrix; (2) For general Hilbert space, when the linear operator cannot be explicitly represented by matrix, we define the adjoint using inner product, which is a \textbf{generalization}. 
\end{remark}

\begin{lemma}[elementary property of adjoint]
\cite[151]{debnath2005hilbert}
Let $A,B$ be the linear operator on Hilbert space $H$, let $\alpha \in \F$, then
$$(A + B)^* = A^* + B^*$$
$$(\alpha A)^* = \conj{\alpha} A^*$$
$$(A^*)^* = A$$
$$I^* = I$$
$$(AB)^* = B^*A^*$$
\end{lemma}



\begin{remark}[adjoint matrix]\index{adjoint matrix}
Consider $V=\mathbb{C}^n,W=\mathbb{C}^m$, a linear operator $T:V\rightarrow W$ can be represented as a matrix $A\in \mathbb{C}^{m\times n}$. The adjoint operator of $T$ is $T^H=A^*$(* is the complex conjugate)
\end{remark}

\subsection{Self-adjoint operator}
\begin{definition}[self-adjoint operator]\index{self-adjoint operator}
Let $V$ be a Hilbert space. A linear operator $T:V\rightarrow V$ is self-adjoint is $T^H=T$. Thus, a self-adjoint operator satisfies
$$\ip{Tv,w}=\ip{v,Tw},\forall v,w \in V$$
\end{definition}

\begin{theorem}[eigenvalues of self-adjoint operator]
\cite[182]{debnath2005hilbert}All eigenvalues of self-adjoint operator on a Hilbert space are real.
\end{theorem}
\begin{proof}
$$\lambda\ip{v,v}=\ip{\lambda v,v}=\ip{Tv,v} =\ip{v,Tv}=\lambda^*\ip{v,v}$$
therefore $\lambda = \lambda^*$.	
\end{proof}


\begin{definition}[self-adjoint and symmetric matrices]
A square matrix $A$ is called self-adjoint or Hermitian if it is equal to its own conjugate transpose, i.e., $A = A^H$.
In the real number matrix, the self-adjoint matrix is symmetric matrix. 
\end{definition}



\subsection{Projection theorem in Hilbert space}\index{projection}

\begin{definition}[orthogonal projections]\index{orthogonal projection}
	Let $P$ be a projection on the vector space $S$. $P$ is called a orthogonal projection if the null space and range space of $P$ are orthogonal complements. That is, for any $x,y\in P$, we have 
	$$\ip{Px,(y-Py}  = \ip{x-Px,Py} = 0$$
	or equivalently, 
	$$\ip{Px,y} = \ip{Px,Py} = \ip{x,Py}$$
\end{definition}

\begin{theorem}[self-adjoin operator, orthogonal projection]
	A projection $P$ is orthogonal projection if and only if it is self-adjoin.
\end{theorem}
\begin{proof}
	(1) (self-adjoin implies orthogonal)$$\ip{Px,y-Py} = \ip{P^2x,y-Py} = \ip{Px,Py-P^2y} = 0$$ where we use $P^2=P$ and $\ip{Px,y} = \ip{x,Py}$ for self-adjoin property.(2) (orthogonal projection implies self-adjoin) $\ip{Px,y} = \ip{Px,Py} = \ip{x,Py}$
\end{proof}



\begin{remark}
	Common self-adjoin operator can be real symmetric matrix, and Hermitian matrix.  
\end{remark}


\begin{theorem}[symmetric matrix as orthogonal projection]
	\cite[114]{moon2000mathematical}Any symmetric matrix $P$ such that $P^2=P$ is a orthogonal projection.
\end{theorem}
\begin{proof}
	$P$ is a projection by definition. Then an arbitrary element in range space is $Pc,c\in \F^n$, an arbitrary element in null space $(I-P)d,d\in \F^n$, $(Pc)^T(I-P)d = 0$.
\end{proof}




\subsection{Unitary and orthogonal operator}
\begin{definition}[unitary and orthogonal operator]
Let $V,W$ be two Hilbert spaces. A \textbf{unitary operator} is an isomorphism $T:V\rightarrow W$ that preserves inner product 
$$\ip{Tv,Tw}_W = \ip{v,w}_V,\forall v,w\in V.$$
When $V$ is a real inner product space, unitary operator are often referred to as \textbf{orthogonal operators}.
\end{definition}

\begin{definition}[unitary and orthogonal matrices]
For any square matrix $U$, we say $U$ is unitary if $U^HU=UU^H=I$. For a real square matrix, $U$ is said orthogonal if $U^TU=U^TU=I$.
\end{definition}

\begin{remark}
Geometrically, multiplying a orthogonal matrix can be interpreted as a rigid rotation.
\end{remark}

\begin{example}\hfill
\begin{itemize}
    \item the noramlized discrete Fourier transform matrix $U \in \mathbb{C}^N\times N$, with 
    $$U_{k,n}=\frac{1}{\sqrt{N}}e^{-i2\pi k n/N},k,n=0,1,...,N-1$$
    is a unitary matrix.
\end{itemize}
\end{example}


\begin{lemma}
An unitary operator an isometry(i.e., norm preserving operator)
$$\norm{Tv}=\ip{Tv,Tv}^{0.5}=\ip{v,v}^{0.5}=\norm{v},\forall v\in V$$
\end{lemma}




\subsection{Compact operators}
\begin{definition}[compact operator]\index{compact operator}\cite[173]{debnath2005hilbert}\cite[98]{krim2015geometric} Let $V$ be a Hilbert space. A linear operator $T:V\to V$ is \textbf{compact (or completely continuous operator)} if for every bounded sequence $\{v_n\}$ in $V$, the sequence $\{Tv_n\}$ has a convergent subsequence. 
\end{definition}



\begin{remark}[about compact set]
Recall that a metric space is compact if every sequence in the set has a convergent subsequence.
\end{remark}


\begin{definition}[finite dimensional operator]\index{finite dimensional operator}
\cite[175]{debnath2005hilbert} An linear operator is called finite dimensional operator if its range is of finite dimension.
\end{definition}



\begin{lemma}[boundedness of compact operator]
\cite[173]{debnath2005hilbert}A compact operator is bounded. 
\end{lemma}
\begin{proof}
If an operator $A$ is not bounded, then there exist a sequence $\{x_n\}$ in $H$ such that $\norm{Ax_n} \geq n$, then this sequence $\{Ax_n\}$ does not have a convergent sequence, which contradict that $A$ is compact operator.	
\end{proof}


\begin{remark}
\textbf{An operator is bounded might not be compact.}\cite[173]{debnath2005hilbert} For example, consider the identity operator $I$ on an infinite dimensional Hilbert space $H$ is not compact, it is bounded. Consider an orthonormal sequence $\{e_n\}$ in $H$. Then the sequence $Ie_n = e_n$ does not contain a convergent subsequence. 
\end{remark}


\begin{theorem}[compact operators form a vector space]
\cite[174]{debnath2005hilbert} If $A,B$ are both compact operators on $H$, then $aA+bB,a,b\in \F$ is compact operator on $H$. Moreover, compact operators on $H$ form a vector space
\end{theorem}


\begin{lemma}\cite[98]{krim2015geometric}
Every linear finite dimension operator is compact.
\end{lemma}


\begin{remark}
A direct consequence is \textbf{every matrix as a linear operator on $\C^n$ is compact}. 
\end{remark}

\section{Eigenvalues and eigenvectors of linear operators}
Let $V$ be a vector space over a field $\mathbb{F}$. An eigenvector and eigenvalue of a linear operator $T: V \rightarrow V$ are a scalar $\lambda \in \mathcal{F}$ and nonzero vector $v\in V$ such that
$$Tv=\lambda v$$
For a given eigenvalue $\lambda$, the set of all vectors such that $Tv = \lambda v$ is called the eigenspace of $T$, denoted $V_\lambda:$
$$V_\lambda = \{v\in V | Tv=\lambda v\} = ker(T-\lambda I)$$


\begin{theorem}
\cite[183]{debnath2005hilbert} For every eigenvalue $\lambda$ of a bounded operator $A$, we have $\abs{\lambda} \leq \norm{A}$
\end{theorem}
\begin{proof}
let $u$ be eigenvector, we have
$$\norm{Au} = \norm{\lambda u} = \abs{\lambda}\norm{u} \leq \norm{A}\norm{u}$$	
\end{proof}


\begin{theorem}\cite[183]{debnath2005hilbert}
If $A$ is a non-zero, compact, self-adjoint operator on a Hilbert space $H$, then it has an eigenvalue $\lambda$ equal to either $\norm{A}$ or $-\norm{A}$
\end{theorem}






\begin{theorem}
\cite[182]{debnath2005hilbert} Eigenvectors corresponding to distinct eigenvalues of self-adjoint operator on a Hilbert space are orthogonal.
\end{theorem}
\begin{proof}
Let $A$ be a self-adjoint operator, let $u_1,u_2 \in H$ be the eigenvectors associated with distinct eigenvalues, we have
\begin{align*}
\ip{Au_1,u_2} = \ip{\lambda_1 u_1,u_2} = \lambda \ip{u_1,u_2} = \ip{u_1, Au_2}= \conj{\lambda_2} \ip{u_1,u_2}\\
\Rightarrow (\lambda_1 - \lambda_2)\ip{u_1,u_2} = 0 
\end{align*}
where we use the fact all eigenvalues of a self-adjoint operator is real.	
\end{proof}




\begin{theorem}[eigenvalue property of unitary operator]
\cite[182]{debnath2005hilbert} All eigenvalues of a unitary operator on a Hilbert space are complex numbers with modulus 1
\end{theorem}
\begin{proof}
Let $A$ be an unitary operator on $H$, let $\lambda$ be the eigenvalue, let $u$ be the corresponding eigenvector, then
$$\ip{Au,Au} =\ip{\lambda u,\lambda u} = \abs{\lambda}^2 \ip{u,u}= \ip{u,A^HAu} = \ip{u,u}$$
then $\abs{\lambda}^2 =1$.	
\end{proof}



\begin{theorem}[eigenvalue property of unitary operator]
\cite[182]{debnath2005hilbert} Eigenvectors corresponding to distinct eigenvalues of a unitary operator on a Hilbert space are orthogonal.
\end{theorem}
\begin{proof}
Let $A$ be an unitary operator on $H$, let $\lambda_1,\lambda_2$ be the eigenvalue, let $u_1,u_2$ be the corresponding eigenvector, then
$$\ip{Au_1,Au_2} = \ip{u_1,u_2} = \ip{\lambda_1u_1,\lambda_2u_2} = \lambda_1\conj{\lambda_2}\ip{u_1,u_2}$$
then $(\lambda_1\conj{\lambda_2}-1)\ip{u_1,u_2} = 0$
note that $1 = \lambda_1\conj{\lambda_1}$,then $$(\lambda_1\conj{\lambda_2}-1) = \conj{\lambda_2}(\lambda_1-\lambda_2) \neq 0$$
note that we use the fact that all eigenvalues of unitary operator has modulus 1.
Therefore, $\ip{u_1,u_2} = 0$.	
\end{proof}



\begin{theorem}
\cite[185]{debnath2005hilbert} The set of distinct non-zero eigenvalues $\{\lambda_i\}$ of a self-adjoint compact operator is either finite \emph{or} infinite and $\lim_{i\to \infty} \lambda_i = 0$
\end{theorem}



\section{Spectral decomposition}\label{ch:functional-analysis:sec:spectral-decomposition}
\begin{theorem}[Hilbert-Schmidt Theorem]\index{Hilbert-Schmidt Theorem}
\cite[188]{debnath2005hilbert}
For every self-adjoint, compact operator $A$ on an infinite dimensional Hilbert space $H$, there exists an orthonormal system of eigenvectors $\{u_n\}$ corresponding to non-zero eigenvalues $\{\lambda_n\}$ such that every element $x\in H$ has a unique representation in the form
$$x = \sum_{n=1}^\infty a_nu_n + v$$
where $a_n \in \C$ and $v$ satisfies $Av = 0$. If $A$ has infinitely many distinct eigenvalues, then $\lambda_n\to 0$ as $n \to 0$.
\end{theorem}




\begin{theorem}[spectral theorem for self-adjoint compact operator]\index{spectral theorem for self-adjoint compact operator} \cite[190]{debnath2005hilbert}Let $T$ be a self-adjoint compact operator on a Hilbert space $V$. Then there exists an orthonormal basis $\{u_l\}$  of $V$ consisting of eigenvectors of $T$ such that
$$Tv = \sum_{l=1}^\infty \lambda_l \ip{v,u_l}u_l$$
for all $v\in V$, where $\lambda_l$ is the eigenvalue associated to $u_l$. Moreover, if the spectrum squence $\{\lambda_l\}$ is infinite, then $\abs{\lambda_l} \to 0$ as $l \to \infty$.
\end{theorem}





\section{Notes on bibliography}
For functional analysis, see \cite{kreyszig1989introductory}\cite{lax2014functional}\cite{duren2012invitation}.

For comprehensive treatment on both theory and applications of linear algebra and functional analysis on signal processing, see \cite{moon2000mathematical}.

For treatment on contraction mapping and fixed point theorem with applications, see \cite{griffel2002applied}\cite{ok2007real}\cite{debnath2005hilbert}.\\

For linear operator theory, see \cite{naylor2000linear}.

For treatment on Hilbert space and appliction, see \cite{debnath2005hilbert}.


\printbibliography


\end{refsection}
