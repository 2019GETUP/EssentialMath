
\begin{refsection}
	\startcontents[chapters]	

\chapter{Statistical methods and analysis}\label{ch:statistical-models}
%\minitoc
	\printcontents[chapters]{}{1}{}
\section{Multivariate statistical methods}
\subsection{Fundamentals}
\subsubsection{Sample statistics}
\begin{mdframed}
	\textbf{notations:}
	\begin{itemize}
		\item $\bm{1}$ is the vector of all 1.
		\item $J$ is a square matrix with all 1.
	\end{itemize}
\end{mdframed}


\begin{lemma}[sample mean and sample variance operator]\label{ch:statistical-models:th:SampleMeanSampleCovarianceOperator}
Let $\bm{X}$ be the data matrix such that $\bm{X} = [X_1^T X_2^T \dots X_n^T]^T$. It follows that	
\begin{itemize}
	\item (sample mean)$$\bar{X} \triangleq \frac{1}{n}\sum_{i=1}^n X_i = \frac{1}{n}\bm{X}^T$$
	\item $$I - \frac{1}{n}J$$
	is an orthogonal projector.
	\item (sample covariance) 
	$$S \triangleq \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})(X_i - \bar{X})^T = \frac{1}{n}\bm{X}^T(I- \frac{1}{n}J)\bm{X}.$$
	\item (sample correlation)
	$$R = D^{-1/2}SD^{-1/2},$$
	where $D = diag(S).$
\end{itemize}		
\end{lemma}



\begin{lemma}[affine transformation of sample statistics]
Let $Y = AX$ and $Z = BX$. 
\begin{itemize}
	\item $$\bar{Y} = A\bar{X}.$$
	\item $$S_{Y} = AS_XA^T.$$
	\item $$S_{Y,Z} = AS_XB^T.$$
\end{itemize}	
\end{lemma}
\begin{proof}
(1)
$$\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i = \frac{1}{n}\sum_{i=1}^n AX_i =  A\bar{X}.$$
(2)
$$S_{Y} = \frac{1}{n-1}\sum_{i=1}^n (Y_i - \bar{Y})(Y_i - \bar{Y})^T=AS_XA^T.$$
(3)
$$S_{Y,Z} = \frac{1}{n-1}\sum_{i=1}^n (Y_i - \bar{Y})(Z_i - \bar{Z})^T=AS_XB^T.$$

\end{proof}

\begin{lemma}
Let a $p$ dimensional random vector $X\sim MN(\mu,\Sigma)$ with $\Sigma$ being nonsingular. Then
\begin{itemize}
	\item $(X-\mu)^T \Sigma^{-1}(X - \mu)$ is distributed as $\chi_p^2$, where $\chi_p^2$ denote the chi-square distribution with $p$ degrees of freedom.
	\item The $MN(\mu,\Sigma)$ distribution assigns probability $1-\alpha$ to the solid ellipsoid $\{x: (x-\mu)^T\Sigma^{-1}(x-\mu) \leq F(1-\alpha)\}$, where $F(x)$ denote the cdf for chi-square distribution with $p$ degrees of freedom.	
\end{itemize}
\end{lemma}
\begin{proof}
\end{proof}


\subsubsection{Spherical and elliptical distribution}
\begin{definition}[spherical distribution]\index{spherical distribution}\cite[89]{mcneil2015quantitative}
	A random vector $Y=(Y_1,Y_2,...,Y_d)$ has a spherical distribution if for every orthonormal matrix $U\in \R^{d\times d}$, we have
	$$Y = UY ~in~distribution,$$
	that is, $Y$ and $UY$ have the same joint cdf. (Note that $Y(\omega) \neq UY(\omega), \omega \in \Omega$.)	
\end{definition}


\begin{lemma}[characterization of a spherical distribution]\cite[89]{mcneil2015quantitative}
	The following are equivalent:
	\begin{enumerate}
		\item $Y = (Y_1,Y_2,...,Y_d)$ is spherical.
		\item There exists a function $\psi$ of a scalar variable such that
		$$\phi_{Y}(t) \triangleq E[e^{it^TY}] = \psi(\norm{t}^2),\forall t\in \R^d$$
		\item For every $a\in \R^d$, 
		$$a^TY = \norm{a}Y_1 ~in~distribution$$
	\end{enumerate}
	We call $\phi$ the characteristic generator of the spherical distribution, denoted as $Y\sim S_d(\psi)$.
\end{lemma}
\begin{proof}
	(1)to (2): Because $Y$ and $UY$ have the same distribution, then the characteristic function must be equal at the same $t\in \R^d$. That is
	$$\phi_Y(t) = \phi_{UY}(t) = E[\exp(i t^TY)] = E[\exp(i t^TUY)]=\phi_Y(U^Tt).$$
	Note that $\norm{U^Tt} = \norm{t}$;therefore $\phi_Y(t)$ must be a function only depends on the length of $t$. 
	(2)to (3): Note that$$t\in \R,\phi_{Y_1}(t) = E[\exp(itY_1)] = \phi_Y(te_1)= \psi(t^2),$$
	and from(2)
	$$\phi_{a^TY}(t) = \psi(t^2\norm{a}^2) = \phi_{\norm{a}Y_1}(t).$$
	(3)to (1):
	$$\phi_{UY}(t) = E[\exp(i t^T(UY))] = E[\exp(i (U^Tt)^TY)] = E[\exp(i \norm{U^Tt}Y_1)]= E[\exp(i \norm{t}Y_1)],$$
	where we use the fact that $$a^TY = \norm{a}Y_1 ~in~distribution~\implies E[\exp(i (U^Tt)^TY)] = E[\exp(i \norm{U^Tt}Y_1)].$$
	
	Further from (3), we have
	$$\phi_{UY}(t) = E[\exp(i \norm{t}Y_1)] = E[\exp(i t^TY)] = \phi_Y(t).$$
\end{proof}








\begin{example}\hfill
	\begin{itemize}
		\item $X\sim MN(0,I_d)$ is spherical, with $\phi_{X}(t) \triangleq E[\exp(it^TX)] = \exp(-\frac{1}{2}t^Tt).$ Then, the generator is $\psi(t) = \exp(-\frac{1}{2}t)$.
		\item $X\sim MN(\mu,I_d), \mu\neq 0$ is not spherical.
		\item $X\sim MN(0,\Sigma), \Sigma\neq I_d$ is not spherical.
	\end{itemize}
\end{example}

\begin{definition}[elliptical distribution]\cite[93]{mcneil2015quantitative}
	A random vector $(X_1,X_2,...,X_d)$ is called elliptical if it is an affine transform of a \textbf{spherical} random vector $(Y_1,Y_2,...,Y_k)$ such that
	$$X \triangleq \mu + AY$$	
	where $\mu\in\R^d$ and $A\in \R^{d\times k}$.
\end{definition}


\begin{remark}[geometric interpretation in low dimensions]
	For 2d or 3d elliptical distributions, the iso-density lines of joint density distribution forms an ellipse or an ellipsoid. Let $\Sigma = AA^T$, the eigenvalues/eigenvectors of $\Sigma$ determine how a sphere is stretched towards an ellipse/ellipsoid.
\end{remark}

\begin{example}\hfill
	\begin{itemize}
		\item $X\sim MN(\mu,\Sigma)$ is a elliptical distribution.
	\end{itemize}
\end{example}

\subsection{Moment estimator}

\subsection{Maximum likelihood estimation}

\begin{lemma}[likelihood function]
Assume $x_1,x_2,...,x_n, \in x_i\in \R^p$ are independent random samples drawn from a multivariate normal distribution $(\mu,\Sigma)$. Then likelihood function is given by
$$L(\mu,\Sigma) = \frac{1}{(2\pi)^{np/2}\abs{\Sigma}^{n/2}}\exp(-Tr(\Sigma^{-1}(\sum_{i=1}^n (x_i - \mu)(x_i-\mu)^T))).$$
\end{lemma}
\begin{proof}
\begin{align*}
L(\mu,\Sigma) &= \frac{1}{(2\pi)^{np/2}\abs{\Sigma}^{n/2}}\exp(-((\sum_{i=1}^n (x_i - \mu)\Sigma^{-1}(x_i-\mu)^T)))\\
& = \frac{1}{(2\pi)^{np/2}\abs{\Sigma}^{n/2}}\exp(-Tr(\Sigma^{-1}(\sum_{i=1}^n (x_i - \mu)(x_i-\mu)^T)))
\end{align*}
Note that we use $Tr(x^TAx) = Tr(Axx^T)$(\autoref{appendix:th:matrixtraceproperty}).
\end{proof}


\begin{lemma}[maximum likelihood estimator]\cite[172]{johnson2007applied}
Let $X_1,X_2,...,X_n$ be a random sample from a multivariate normal distribution with mean $\mu$ and covariance $\Sigma$. Then, 
$$\hat{\mu} = \bar{X}, \hat{\Sigma} = \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})(X_i - \bar{X})^T = \frac{n-1}{n}S,$$
are the \textbf{maximum likelihood estimator} of $\mu$ and $\Sigma$. That is
$$(\hat{\mu},\hat{\Sigma}) = \arg\max L(\mu,\Sigma) = \arg\max \frac{1}{(2\pi)^{np/2}\abs{\Sigma}^{n/2}}\exp(-Tr(\Sigma^{-1}(\sum_{i=1}^n (x_i - \mu)(x_i-\mu)^T))). $$ 
\end{lemma}
\begin{proof}
(1) Note that
\begin{align*}
\sum_{i=1}^n (X_i - \mu)(X_i - \mu)^T &=\sum_{i=1}^n (X_i - \bar{X} + \bar{X} - \mu)(X_i - \bar{X} + \bar{X} - \mu)^T\\
&=\sum_{i=1}^n (X_i - \bar{X})(X_i - \bar{X})^T + \sum_{i=1}^n (\bar{X} - \mu)(\bar{X} - \mu)^T \\
&=\sum_{i=1}^n (X_i - \bar{X})(X_i - \bar{X})^T + n (\bar{X} - \mu)(\bar{X} - \mu)^T 
\end{align*}
Note that we use $$\sum_{i=1}^n (X_i - \bar{X})(\bar{X} - \mu)^T =  (\sum_{i=1}^n X_i - n\bar{X})(\bar{X} - \mu)^T = 0$$
to eliminate the cross terms.
Then, for the exponent in $L$, we have
$$Tr(\Sigma^{-1}(\sum_{i=1}^n (X_i - \mu)(X_i-\mu)^T)) = Tr(\Sigma^{-1}(\sum_{i=1}^n (X_i - \bar{X})(X_i-\bar{X})^T))+ n (\bar{X} - \mu)\Sigma^{-1}(\bar{X} - \mu).$$
It is easy to see that when $\mu = \bar{X}$, $L$ is maximized \textbf{given any positive definite matrix $\Sigma$}.
(2) See reference.
\end{proof}


\subsection{Simultaneous inference }


\begin{lemma}\cite[258]{johnson2007applied}
Let the constant $c > 0$ and positive definite $p\times p$ matrix $A$ determine the ellipsoid $E=\{z: z^T A^{-1} z \leq c^2\}$. For a given nonzero vector $u\in \R^p$, the projection(shadow) of $E$ on $u$ is given by
$$Proj_{u} E = c\frac{\sqrt{u^TAu}}{u^Tu}u.$$
If $u$ is a unit vector, the length of the projection is $c\sqrt{u^TA^{-1}u}$  
\end{lemma}
\begin{proof}
Finding the projection length is equivalent to the following optimization problem:
$$\max_{x\in\R^p} \frac{x^Tuu^Tx}{u^Tu}, ~st~ x^TA^{-1}x \leq c^2.$$
It is easy to see that the constraint will be tight. Therefore, the Lagrange will be $$L(x) = \frac{x^Tuu^Tx}{u^Tu} - \lambda(x^T A^{-1}x - c^2).$$
The first order KKT condition gives
$$2uu^T x - 2\lambda A^{-1}x = 0 \implies x = \frac{1}{\lambda} Au(u^T x), $$
or optimal $x$  is the in the direction of $Au$. Set $x = kAu$, we have
$$x^TA^{-1}x = k^2 u^T A u  = c^2 \implies k^2 = \frac{c^2}{u^TuA}.$$

Finally, $$\frac{x^Tuu^Tx}{u^Tu} = \frac{c^2 u^T A u}{u^T u}.$$ 
\end{proof}


\begin{lemma}\cite[259]{johnson2007applied}
Suppose that the ellipsoid $\{z: z^TA^{-1}z \leq c^2\}$ is given and that $U = [u_1 u_2]$ is arbitrary but of rank two. Then
$$z^T A^{-1} z \leq c^2 \implies (U^T z)^T(U^T A U)^{-1}(U^Tz)\leq c^2, \forall U.$$	
\end{lemma}



\begin{lemma}\cite[259]{johnson2007applied}
	Given the ellipsoid $\{z: z^TA^{-1}z \leq c^2\}$ and two perpendicular unit vector $u_1$ and $u_2$. Then the projection of $\{z:z^TA^{-1}z\leq c^2\}$ on the $u_1,u_2$ plane results in the two-dimensional ellipse $\{(U^T z)^T(U^T A U)^{-1}(U^Tz)\leq c^2 \}$ is given and where $U = [u_1 u_2]$ is arbitrary but of rank two.
\end{lemma}

\subsection{Parameter estimation}


\paragraph{M-estimator of location and dispersion for ellipsoid distribution}

\begin{algorithm}[H][]\
	\SetAlgoLined
	\KwIn{Sample set consists of $X_1,X_2,...,X_N$}
	
	Initial estimate $\hat{\mu}^(1) = \bar{X}, \hat{\Sigma}^{(1)} = S$.\\
	Set $k=1$.\\
	
	\Repeat{terminal condition is met}{
		For $i=1,2,...,N$, set $$D_i^2 = (X_i - \hat{\mu}^{(1)})^T [\hat{\Sigma}^{(1)}]^{-1}(X_i - \hat{\mu}) .$$\\
		
		Update location estimation using
		$$\hat{\mu}^{(k+1)} = \frac{\sum_{i=1}^N w_1(D_i)X_i}{\sum_{i=1}^N w_1(D_i)},$$
		where $w_1$ is a weight function.\\
		
		Update dispersion matrix estimation using
		$$\hat{\Sigma}^{(k+1)} = \frac{1}{N-1}\sum_{i=1}^N w_2(D_i^2)(X_i - \hat{\mu}^{(k+1)})(X_i - \hat{\mu}^{(k+1)})^T$$
		where $w_2$ is a weight function.\\
		set $k=k+1$
	}
	\KwOut{$\hat{\mu},\hat{\Sigma}$}
	\caption{M-estimation algorithm of location and dispersion for ellipsoid distribution}
\end{algorithm}

\begin{remark}\hfill
	\begin{itemize}
		\item Reference \cite[97]{mcneil2015quantitative}.
		\item The weight function is usually a decreasing function in order to downweight outliers.
	\end{itemize}	
	
\end{remark}



\begin{note}[robust estimation of correlation via Kendall's tau]\cite[97]{mcneil2015quantitative}
	From \autoref{ch:statistical-models:th:RankCorrelationGaussianCopula}, the linear correlation is connected to the Kendall's tau via
	$$\rho_\tau(X_1,X_2) = \frac{2}{\pi} \arcsin \rho. $$
	
	The quantity $\rho_\tau(X_1,X_2)$ can be calculated via \autoref{ch:statistical-models:th:KendallTauForObservations}. 	
\end{note}

\subsection{Inference in jointly Gaussian distributions}

\begin{remark}[background and motivation]
Given a multivariate Gaussian joint distribution, $p(x_1,x_2),x_1\in \R^m, x_2\in \R^n$, it is useful to be able to compute marginals $p(x_1)$ and conditionals $p(x_1|x_2)$. 	
\end{remark}


\begin{theorem}[Marginals and conditionals of an MVN]. Suppose $X=(x_1,x_2)$is jointly Gaussian with parameters
	\begin{equation}
	\mu=\left(\begin{array}{c}\mu_1 \\
	\mu_2\end{array}\right),
	\Sigma=\left(\begin{array}{cc}
	\Sigma_{11} & \Sigma_{12} \\
	\Sigma_{21} & \Sigma_{22} \end{array}\right),
	\Lambda=\Sigma^{-1}=\left(\begin{array}{cc}
	\Lambda_{11} & \Lambda_{12} \\
	\Lambda_{21} & \Lambda_{22} \end{array}\right),
	\end{equation}
	
	Then the marginals are given by
	\begin{equation}
	\begin{split}
	p(x_1)= \mathcal{N}(x_1|\mu_1,\Sigma_{11})\\
	p(\vec{x}_2)= \mathcal{N}(x_2|\mu_2,\Sigma_{22})
	\end{split}
	\end{equation}
	and the posterior conditional is given by
	\begin{equation}\label{eqn:Marginals-and-conditionals-of-an-MVN}
	\boxed{\begin{split}
		p(x_1|x_2)& =\mathcal{N}(x_1|\mu_{1|2},\Sigma_{1|2}) \\
		\mu_{1|2}& = \mu_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2) \\
		& = \mu_1-\Lambda_{11}^{-1}\Lambda_{12}(x_2-\mu_2) \\
		& = \Sigma_{1|2}(\Lambda_{11}\mu_1-\Lambda_{12}(x_2-\mu_2)) \\
		\Sigma_{1|2}& = \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}=\Lambda_{11}^{-1}
		\end{split}}
	\end{equation}
\end{theorem}
\begin{proof}
	
\end{proof}

\subsubsection{Examples}
Below we give some examples of these equations in action, which will make them seem more intuitive.


\paragraph{Marginals and conditionals of a 2d Gaussian}



\section{Covariance structure and factor analysis}\label{ch:statistical-models:sec:FactorAnalysis}
\subsection{The orthogonal factor model}
\subsubsection{The model}
\begin{definition}[orthogonal factor model]\index{factor model}\cite[482]{johnson2007applied}
Consider a $p$ dimensional observable random vector $X$ and a $m$ dimensional observable random vector $F$, called common factors. Let $E[X] = \mu, \mu\in \R^p, Cov(X) = \Sigma,\Sigma\in\R^{p\times p}$. 
In the orthogonal factor model, \textbf{we assume $X$ is related to $F$} via
$$X - \mu = L F + \epsilon,$$
where 
\begin{itemize}
	\item $L\in \R^{p\times m}$ is called matrix of \textbf{factor loadings};
	\item $\epsilon$ is a $p$ dimensional random vector such that $E[\epsilon] = 0_{p\times 1}, Cov(\epsilon) = \psi, Cov(\epsilon)_{ij} = \psi_{i}, i = j, Cov(\epsilon)_{ij} = 0, i\neq j$.
	\item $E[F] = 0_{m\times 1}, Cov(F) = I_{m\times m}.$
	\item $F$ and $\epsilon$ are independent such that $Cov(\epsilon, F) = 0$.
\end{itemize}
\end{definition}


\begin{remark}[parameter summary in factor models]
The original covariance structure $\Sigma\in \R^{p\times p}$ has $p(p+1)/2$ parameters. In the new covariance structure resulted from factor model, we have in total $p(m+1)$ parameters:
\begin{itemize}
	\item $mp$ factor loadings in the matrix $L$.
	\item $p$ specific variance in the matrix $\psi$.
\end{itemize} 	
\end{remark}


\subsubsection{Covariance structure implied by factor model}
\begin{lemma}[covariance structure implied by factor model]\cite[483]{johnson2007applied}
In the factor model, the covariance of $X$  is given by
$$E[(X-\mu)(X-\mu)^T] = LL^T + \psi.$$	

In addition, the covariance matrix between $X$ and $F$ is given by
$$Cov(X,F) \triangleq E[(X - \mu)F^T] = L.$$
\end{lemma}
\begin{proof}
(1)	
\begin{align*}
E[(X-\mu)(X-\mu)^T] &= E[(LF + \epsilon)(LF + \epsilon)^T] \\
&=E[LFF^TL^T + \epsilon (LF)^T + LF\epsilon^T + \epsilon \epsilon^T] \\
&= LIL^T + 0 + 0 + \psi. 
\end{align*}	
(2)
\begin{align*}
E[(X-\mu)F^T] &= E[(LF + \epsilon)F^T] \\
&=E[LFF^T + \epsilon F^T] \\
&= LI + 0 = L. 
\end{align*}
\end{proof}


\begin{remark}[factor model as a low dimensional approximation to high-dimensional covariance matrix]
In the factor model, we use a $L$ matrix of $pm$ elements and $\psi$ matrix of $p$ elements to approximate/reproduce the original covariance matrix $\Sigma$ of $X$, containing $p(p+1)/2$ elements. 	
\end{remark}

\begin{remark}[$L = 0$ if there is no correlation between components of $X$]
Consider the situation that $\Sigma = I$, i.e., there is no correlation between components of $X$. Then, we can set $\psi = \Sigma, L = 0$.	
\end{remark}

\begin{lemma}[non-uniqueness of factor model]\cite[573]{anderson2009introduction}\cite[488,504]{johnson2007applied}
Let $\Sigma, L,F, \psi$ be the results of factorization such that
$$X-\mu = LF + \epsilon,$$
and
$$\Sigma = LL^T + \psi. $$
Let $U \in \R^{m\times m}$ be a orthonormal matrix such that $UU^T = I$. Then the factors and the associated factor loadings can also take form 
$\hat{L} = LU, \hat{F} = U^TF$ also satisfy
$$X-\mu = \hat{L}\hat{F} + \epsilon,$$
and
 	$$\Sigma = \hat{L}\hat{L}^T + \psi. $$
\end{lemma}
\begin{proof}
Note that	
$$\hat{L}\hat{F} = LUU^TF = LUU^TL^T = LL^T$$
and
$$\hat{L}\hat{L}^T = LU(LU)^T = LUU^TL^T = LL^T.$$	
\end{proof}

\subsubsection{Factor model for correlation structure}

\begin{lemma}[condition for correlation structure via factor model]\label{ch:statistical-models:th:ConditionForCorrelationStructureFactorModel}
Consider a $p$ dimensional observable random vector $X$ and a $m$ dimensional observable random vector $F$, called common factors. Let $E[X] = \mu, \mu\in \R^p, Cov(X) = \Sigma,\Sigma\in\R^{p\times p}$. 
In the orthogonal factor model, \textbf{we assume $X$ is related to $F$} via
$$X - \mu = L F + \epsilon,$$
where 
\begin{itemize}
	\item $L\in \R^{p\times m}$ is called matrix of \textbf{factor loadings};
	\item $\epsilon$ is a $p$ dimensional random vector such that $E[\epsilon] = 0_{p\times 1}, Cov(\epsilon) = \psi, Cov(\epsilon)_{ij} = \psi_{i}, i = j, Cov(\epsilon)_{ij} = 0, i\neq j$.
	\item $E[F] = 0_{m\times 1}, Cov(F) = I_{m\times m}.$
	\item $F$ and $\epsilon$ are independent such that $Cov(\epsilon, F) = 0$.
\end{itemize}

It follows that
\begin{itemize}
	\item If 
	$$\sum_{j=1}^m L_{ij}^2 + \psi_i = 1, \forall i,$$
	then $$E[(X-\mu)(X-\mu)^T] = LL^T + \psi$$ will produce a correlation matrix.
	\item In particular for a one factor model, if
	$$ L_{i1}^2 + \psi_i = 1, \forall i,$$
	then $$E[(X-\mu)(X-\mu)^T] = LL^T + \psi$$ will produce a correlation matrix.
\end{itemize}	
\end{lemma}
\begin{proof}
Note that
	$LL^T + \psi_i$ will always be a positive-definite matrix. Therefore, we need to impose the condition of diagonal entry to be 1 to ensure that it is a correlation matrix.
\end{proof}


\begin{example}
	Consider a one factor model given by
$$V_i = a_i Y + \sqrt{1-a_i^2}Z_i, i=1,2,...,n,$$
where $(F,Z_1,Z_2,...,Z_n)$ are independent standard normal variables. 

Then the covariance structure implied by the factor model is given by(\autoref{ch:statistical-models:th:ConditionForCorrelationStructureFactorModel})
\begin{align*}
Cov &= \begin{bmatrix}
a_1\\
a_2\\
\cdots \\
a_n
\end{bmatrix}[a_1, a_2, ..., a_n] + \begin{bmatrix}
1-a_1^2 &  &  & \\ 
& 1-a_2^2 &  & \\ 
&  & \ddots & \\ 
&  &  & 1-a_n^2
\end{bmatrix} \\
&=\begin{bmatrix}
a_1^2 & a_1a_2 & \cdots & a_1a_n\\ 
a_2a_1 & a_2a_2 & \cdots & a_2a_n\\ 
\vdots & \cdots & \ddots & \vdots\\ 
a_na_1 & a_na_2 & \cdots & a_na_n
\end{bmatrix}\begin{bmatrix}
1-a_1^2 &  &  & \\ 
& 1-a_2^2 &  & \\ 
&  & \ddots & \\ 
&  &  & 1-a_n^2
\end{bmatrix}\\
&=\begin{bmatrix}
1 & a_1a_2 & \cdots & a_1a_n\\ 
a_2a_1 & 1 & \cdots & a_2a_n\\ 
\vdots & \cdots & \ddots & \vdots\\ 
a_na_1 & a_na_2 & \cdots & 1
\end{bmatrix}
\end{align*}
which is also a valid correlation matrix.		
\end{example}


\subsection{Parameter estimation}

\subsubsection{Data collection and preparation}

\begin{remark}\hfill
\begin{itemize}
	\item Suppose each data vector $x_i$ has $p$ components. Then we can form a $n\times p$ data matrix $X$.
	\item We can transform the data matrix to sample covariance matrix (\autoref{ch:statistical-models:th:SampleMeanSampleCovarianceOperator})via
	$$S = \frac{1}{n-1}X^T(I-\frac{1}{n}J)X.$$
	\item The sample covariance matrix and sample correlation matrix will then be used to build factor models.
\end{itemize}	
\end{remark}


\subsubsection{PCA method}
\begin{lemma}[PCA method for estimation of factor loadings]\cite[488]{johnson2007applied}
Let $S\in \R^{p\times p}$ be the sample covariance matrix of data matrix $X$. Let $S$ adopt eigendecomposition $$S = \sum_{i=1}^p \lambda_i u_i u_i^T, $$where $\lambda_1\leq \lambda_2 \dots \leq \lambda_p$. 

It follows that
\begin{itemize}
	\item Then an estimation of column $i$ of factor loading matrix $L$ is given by
	$$\hat{L}_i = \sqrt{\lambda_i}u_i$$
	such that
	$$\hat{L}\hat{L}^T = \sum_{i=1}^m \lambda_i u_i u_i^T.$$
	\item An estimation of $\psi$ is given by
	$$\hat{\psi} = diag(S - \hat{L}\hat{L}^T).$$
	\item The factors $F_1,F_2,...,F_m$ are assumed to be independent standard normal random variables estiamted to have samplings given by
	$$$$ 
\end{itemize}

\end{lemma}
\begin{proof}
Note that
$$\hat{L}_i\hat{L}_j^T = 0, \forall i\neq j.$$
Then
$$LL^T = \sum_{i=1}^{m} \hat{L}_i\hat{L}_i^T = \sum_{i=1}^{m} \lambda_i u_iu_i^T.$$
\end{proof}

\begin{remark}[how to determine the number of factors]
We can empirically determine the number of factors based on the following considerations:
\begin{itemize}
	\item $m$ is chosen to explain most of the variance.
	\item $m$ is chosen such that the number of parameters $p(m+1)$ is smaller than the number of parameters $p(p+1)/2$ in the full covariance matrix.
\end{itemize}	
	
\end{remark}

\subsubsection{Maximum likelihood method}

\begin{remark}
Using the Maximum Likelihood Estimation Method we must assume that the data are independently sampled from a multivariate normal distribution with mean vector $\mu$ and variance-covariance structure take the form of
$$\Sigma = LL^T + \psi,$$
where $L\in \R^{p\times m}$ is the factor loadings and $\psi$ is the diagonal matrix of specific variances.
\end{remark}



\begin{lemma}\cite[496]{johnson2007applied}
	
Suppose we have data vectors from $n$ observations $x_1,x_2,...,x_n \in \R^p$. The maximum likelihood estimation involves estimating the mean $\mu$, the matrix of factor loadings, and the specific variance matrix $\psi$.

The likelihood function is given by	
	$$L(\mu,\Sigma)=(2\pi)^{-\frac{np}{2}}\abs{\Sigma}^{-\frac{n}{2}}\exp(-\frac{1}{2}Tr(\Sigma^{-1}(\sum_{j=1}^{n}(x_j-\mu)(x_j-\mu)^T))) $$
where $\Sigma = LL^T + \psi.$
and the log likelihood function is given by
	$$l(\mu,\Sigma)\triangleq \ln L =\frac{np}{2}\ln (2\pi)-\frac{n}{2}\ln \abs{\Sigma} -\frac{1}{2}Tr(\Sigma^{-1}(\sum_{j=1}^{n}(x_j-\mu)(x_j-\mu)^T)).$$	
\end{lemma}

\subsection{Hypothesis testing}

\begin{definition}\cite[495]{johnson2007applied}

\begin{itemize}
	\item Null hypothesis
	$$H_0: \Sigma = LL^T + \psi$$
	\item Alternative hypothesis
	$H_1$: $\Sigma$ is any other positive definite matrix. 
\end{itemize}	
\end{definition}


\subsection{Factor score estimation}

\begin{lemma}[weighted least square method to find factor score]
Suppose we have data vectors from $n$ observations $x_1,x_2,...,x_n \in \R^p$. Further suppose that we have already had the factor model given by
$$X = \mu + Lf + \epsilon,$$
with $\mu\in \R^p, L\in \R^{p\times m}$ being given.
Then the factor scores $f_1,f_2,...,f_n \in \R^m$ associated with data vector can be estimated by minimizing the following optimization problem
$$\min_{f_i\in\R^m} (x_i - \mu - Lf_i)^T\psi^{-1}(x_i - \mu - Lf_i).$$

Furthermore, the minimizer is given by
$$\hat{f}_i = (L^T\psi^{-1}L)^{-1}L^T\psi^{-1}(x_i-\mu).$$
\end{lemma}
\begin{proof}
See the results in the generalized least square(\autoref{ch:statistical-models:th:GeneralizedLeastSquareSolution}).	
\end{proof}

\section{Copulas and dependence modeling}
\subsection{Definitions and properties}
\begin{definition}
A \textbf{copulas} $C:[0,1]^d\to [0,1]$ is a multivariate CDF of a $d-$dimensional random vector on the unit cube whose univariate marginal distributions are all $U(0,1)$.

The \textbf{density of a copulas}, denoted by $c$, is given by,
$$c(u_1,u_2,...,u_d) = \frac{\Pa^d}{\Pa u_1\Pa u_2... \Pa u_d}C(u_1,u_2,...,u_d).$$  
\end{definition}

\begin{note}[marginal distribution function from joint distribution function]
	
	$$F_1(y_1) = \lim_{y_2,...,y_d\to \infty} F(y_1,y_2,...,y_d)$$
\end{note}

\begin{lemma}[basic properties of Copulas]\label{ch:statistical-models:th:basicpropertiesofcopulas}\hfill
\begin{itemize}
	\item 	$$C(u_1,...,u_{i-1},0,u_{i+1},...,u_d) = 0$$
	\item $$C(1,...,1,u,1,...,1) = u$$
	\item $C$ is nondecreasing in each of the $d$ variables.
	\item (marginalization property) Let $C_{1:d}(u_1,u_2,...,u_d)$ be copula(i.e., joint cdf) associated with uniform random variables $U_1,U_2,...,U_d$, then 
	$$C_{1:k}(u_1,u_2,...,u_k) \triangleq C_{1:d}(u_1,u_2,...,u_k,1,...,1),k < d,$$
	is the marginal cdf associated with uniform random variables $U_1,U_2,...,U_k$.
\end{itemize}
\end{lemma}
\begin{proof}
Directly from the property of multivariate cdf.
\end{proof}

\begin{corollary}
	Let $C:[0,1]\times[0,1]\to [0,1]$ be a bivariate copula. Then
	$$C(0,u) = C(u,0) = 0, C(1,u) = C(u,1) = u$$
	and
	$$C(u_2,v_2) - C(u_2,v_1) - C(u_1,v_2) + C(u_1,v_1)\geq 0,\forall 0\leq u_1\leq u_2 \leq 1, 0\leq v_1\leq v_2\leq 1.$$
\end{corollary}

\begin{lemma}[Probability integral transform for a random variable]\label{ch:statistical-models:th:probabilityintegraltransform}
Let $X$ be a random variable with support $\R$ and let $F_X: \R\to [0,1]$ be its cdf. 

It follows that
\begin{itemize}
	\item The new random variable $Y = F_X(X)$ has an uniform distribution.
	\item Let $U$ be a uniform random variable on $[0,1]$, then
	$$X = F^{-1}_X(U).$$   
	\item Let $\phi$ be the cdf of a standard normal, then the random variable defined by
	$$Z = \phi^{-1}(F_X(X)).$$
	is a Gaussian random variable.
\end{itemize} 
\end{lemma}
\begin{proof}
(1)Note that
$$P(Y\leq y) = Pr(F_X(X) \leq y) = Pr(X \leq F_X^{-1}(y)) = F_X[F_X^{-1}(y)] = y.$$
(2) Note that
$Pr(X < x) = Pr(F^{-1}_X(U) < x) = Pr(U < F_X(x)) = F_X(x).$
where we use the fact that $Pr(U < y) = y, \forall y\in [0,1]$.
(3) use (1) and (2).
\end{proof}




\begin{example}
If $X$ has an exponential distribution with unit mean, then 
$$F_X(x) = 1 - \exp(-x).$$
It follows that the random variable $Y$, defined as
$$Y = 1 - \exp(-X)$$
has a uniform distribution.
\end{example}


\begin{lemma}[probability transform for a random vector and its properties]\label{ch:statistical-models:th:ProbababilityTransformForRandomVectorAndProperties}
	Consider a random vector $(X_1,X_2,...,X_d)$. Suppose its marginal cdfs $F_i(x) = P(X_i\leq x)$ are continuous functions. Then the random vector
	$$(Y_1,Y_2,...,Y_d) \triangleq (F_1(X_1),F_2(X_2),...,F_d(X_d))$$
	has uniformly distributed marginals. 
	Moreover, the joint cdf of $(Y_1,Y_2,...,Y_d)$ is the copulas associated with the joint cdf of $(X_1,X_2,...,X_d)$.  
\end{lemma}
\begin{proof}
(1)	Note that from the definition of marginal cdf, we have
	$$P(Y_1 < y_1,Y_2 < \infty, ...,Y_d < \infty) = F_{Y_1,Y_2,...,Y_d}(y_1,\infty,...,\infty) = F_{Y_1}(y_1).$$
	$F_{Y1}(y_1)$ is a uniform distributed cdf, as showed in \autoref{ch:statistical-models:th:probabilityintegraltransform}.
(2)	(a)To show the copula is the same we can use monotone transformation invariance \autoref{ch:statistical-models:th:copulasinvarianceundermonotonetransform} since $F_i$ is increasing;(b) 
\begin{align*}
&Pr(F_1(x_1) < u_1 ,...,F_d(x_d)<u_d) \\
=&Pr(x_1 < F_1^{-1}(u_1),...,x_d < F_d^{-1}(u_d)) \\
=&F_X(F_1^{-1}(u_1),...,F_d^{-1}(u_d))
\end{align*}
That is, the joint cdf of $(Y_1,Y_2,...,Y_d)$is the copulas associated with the joint cdf of $(X_1,X_2,...,X_d)$(\autoref{ch:statistical-models:th:constructCopulaForJointDistribution}).
\end{proof}

\begin{lemma}[Frechet-Hoeffding bounding]\label{ch:statistical-models:th:FrechetHoeffdingBounding}\cite[189]{mcneil2015quantitative}
	$$W(u_1,u_2,...,u_d)\leq C(u_1,u_2,...,u_d) \leq M(u_1,...,u_d)$$
	where
	$$W(u_1,u_2,...,u_d) = \max(1 - d + \sum_{i=1}^du_i,0)$$
	and
	$$M(u_1,u_2,...,u_d) = \min(u_1,u_2,...,u_d)$$
	In particular, for $d=2$, we have
	$$\max(u+v-1,0) \leq C(u,v) \leq \min(u,v).$$
\end{lemma}
\begin{proof}
(1) 

(2) For the lower bound, we have
\begin{align*}
C(u_1,...,u_d) &= Pr(\cap_{1\leq i\leq d}\{U_i\leq u_i\}) \\
&= 1 - Pr(\cup_{1\leq i\leq d}\{U_i > u_i\})\\
&= 1 - \sum_{i=1}^d Pr(U_i > u_i) \\
&=1 -  \sum_{i=1}^d(1 - u_i) \\
&=1 - d + \sum_{i=1}^du_i 
\end{align*}
where we use Demorgan's law (\autoref{ch:sets:th:DemorganLaw}) and union bound.
\end{proof}


\begin{corollary}
For a multivariate joint cdf $F$ with margins $F_1,F_2,...,F_d$, we have
$$\max(1 - d + \sum_{i=1}^d F_i(x_i),0) \leq F(x_1,...,x_d) \leq \min(F_1(x_1),...,F_d(x_d))$$	
\end{corollary}
\begin{proof}
Note that
$$W(F_1(x_1),...,F_d(x_d))\leq C(F_1(x_1),...,F_d(x_d)) \leq M(F_1(x_1),...,F_d(x_d)),$$
and 
$$C(F_1(x_1),...,F_d(x_d)) = F(x_1,...,x_d).$$	
\end{proof}

\begin{example}[Gaussian copula]
$$C_{\rho}(u,v) = \int_{-\infty}^{\Phi^{-1}(u)}\int_{-\infty}^{\Phi^{-1}(v)}\frac{1}{2\pi(1-\rho^2)^{1/2}}\exp(-\frac{x^2-2\rho x y + y^2}{2(1-\rho^2)}) dxdy$$
where $\rho$ is the linear correlation coefficient.
\end{example}



\begin{example}[Student's t copula]
	$$C_{\rho,\nu}(u,v) = \int_{-\infty}^{t^{-1}(u)}\int_{-\infty}^{t^{-1}(v)}\frac{1}{2\pi(1-\rho^2)^{1/2}}\exp(1 + \frac{x^2-2\rho x y + y^2}{\nu (1-\rho^2)})^{-(\nu+2)/2} dxdy$$
	where $\rho$ is the linear correlation coefficient.
\end{example}






\subsection{Copulas and distributions}

\begin{note}
In this section we define inverse function for a cdf as
$$F^{-1}(t) = \inf\{x\in\R: F(x)\geq t\}.$$	
\end{note}


\subsubsection{Fundamentals}
\begin{definition}[copula associated with a distribution]\cite[3]{Ruschendorf2013mathematical}
Let $F(x_1,x_2,...,x_n)$ be a cdf of a random vector $(X_1,X_2,...,X_n)$ with support $\R^n$. Let $F_1,F_2, ...,F_n$ be the corresponding marginal cdf. The copula $C(u_1,u_2,...,u_n)$ associated with $F$ is such that 
$$F(x_1,x_2,...,x_n) = C(F_1(x_1),F_2(x_2),...,F_n(x_n)).$$	
\end{definition}




\begin{theorem}[construct a multivariate cdf from a copulas and margins]\label{ch:statistical-models:th:ConstructMultivariateDistributionFromCopulaAndMargins}
	Consider $d$ random variables $X_1,X_2,...,X_d$ with $F_1,F_2,...,F_d$ being the univariate cdf. Let $C$ be a $d$ dimensional copulas, then we can construct a $d$-dimensional multivariate cdf as
	$$F(x_1,x_2,...,x_n) = C(F_1(x_1),F_2(x_2),...,F_d(x_d))$$
	such that the marginal cdf is given by $F_1,F_2,...,F_d$.
\end{theorem}
\begin{proof}
(1)Use the definition of marginal cdf and \autoref{ch:statistical-models:th:basicpropertiesofcopulas}, we have
\begin{align*}
F_1(x_1) &\triangleq F(x_1,\infty,...,\infty) \\
&=C(F_1(x_1),F_2(\infty),...,F_n(\infty)) \\
&=F_1(x_1).
\end{align*}
\end{proof}

\begin{remark}[back out the copula from constructed cdf]
Note that we construct a new cdf $F$ from a given copula $C(u_1,u_2,...,u_d),u_i\in [0,1]$ via
$$F(x_1,x_2,...,x_n) = C(F_1(x_1),F_2(x_2),...,F_d(x_d)).$$

On the other hand, \autoref{ch:statistical-models:th:constructCopulaForJointDistribution} gives a way to back out the original copula from joint cdf via
\begin{align*}
C_n(u_1,u_2,...,u_d) &= F(F_1^{-1}(u_1),F_2^{-1}(u_2),..., F_d^{-1}(u_d)) \\
&=C_o(F_1F_1^{-1}(u_1),F_2F_2^{-1}(u_2),..., F_dF_d^{-1}(u_d)) \\
&=C_o(F_1F_1^{-1}(u_1),F_2F_2^{-1}(u_2),..., F_dF_d^{-1}(u_d)) \\
&=C_o(u_1,u_2,...,u_d)
\end{align*}
where $u_1,u_2,...,u_d\in [0,1]$ and 
$F$ is given by
$$F(x_1,x_2,...,x_n) = C_o(F_1(x_1),F_2(x_2),...,F_d(x_d)).$$	
\end{remark}

\begin{note}[caution!]\hfill
\begin{itemize}
	\item It is \textbf{incorrect} that univariate margins and correlation matrix will allow the construction of the multivariate joint cdf.
	\item It is \textbf{incorrect} that univariate Gaussian margins and correlation matrix will allow the construction of the multivariate joint cdf.
	\item It is \textbf{only correct} that, \textbf{if we know the joint distribution is multivariate Gaussian}, univariate Gaussian margins and correlation matrix will allow the construction of the multivariate joint cdf.
\end{itemize}	
\end{note}





\begin{theorem}[ construct a copulas for a joint distribution; any continuous multivariate cdf has a copula]\label{ch:statistical-models:th:constructCopulaForJointDistribution}
	Consider a random vector $(X_1,X_2,...,X_d)$ with continuous cdf $F$ and marginal cdf $F_1,...,F_d$. The copula for this random vector $(X_1,X_2,...,X_d)$ is defined as
	\begin{align*}
	C(u_1,u_2,...,u_d) = F(F_1^{-1}(u_1),F_2^{-1}(u_2),..., F_d^{-1}(u_d))
	\end{align*}
where $u_1,u_2,...,u_d\in [0,1]$.	
\end{theorem}
\begin{proof}
Based on the definition of a copula associated with a cdf, we want to show that $F(x_1,x_2,...,x_d) = C(F_1(x_1),F_2(x_2),....,F_d(x_d)).$
We have	
\begin{align*}
C(u_1,u_2,...,u_d) &= F(F_1^{-1}(u_1),F_2^{-1}(u_2),..., F_d^{-1}(u_d)) \\
\implies C(F_1(u_1),F_2(u_2),...,F_d(u_d)) &= F(F_1^{-1}(F_1(u_1)),F_2^{-1}(F_2(u_2)),..., F_d^{-1}(F_d(u_d))) \\
&= F(F_1^{-1}(F_1(u_1)),F_2^{-1}(F_2(u_2)),..., F_d^{-1}(F_d(u_d))) \\
&= F(u_1,u_2,...,u_n).
\end{align*}	
\end{proof}

\begin{lemma}[copula of a uniform distribution]\label{ch:statistical-models:th:CopulaOfUniformDistribution}
For $d$-dimensional uniform cdf $F$, its copulas $C = F$.
\end{lemma}
\begin{proof}
Note that
$$C(u_1,u_2,...,u_d) = F(F_1^{-1}(u_1),F_2^{-1}(u_2),..., F_d^{-1}(u_d))
= F(u_1,u_2,...,u_d).$$
since for all marginals, $F_i(x) = x.$
\end{proof}




\begin{remark}
Given a copula, we can obtain different multivariate cdf by selecting different marginal distribution.
\end{remark}



\begin{theorem}[Sklar's theorem, construct joint cdf and pdf from copula and margins]\label{ch:statistical-models:SklarTheorem}
For every multivariate cumulative distribution function
$$H(x_1,...,x_d) \triangleq P(X_1\leq x_1,...,X_d \leq x_d).$$
of a random vector $(X_1,X_2,...,X_d)$, there exists a copula $C$ such that
$$H(x_1,...,x_d) = C(F_1(x_1),...,F_d(x_d)).$$

If $H$ has a density $h$, then
$$h(x_1,...,x_d) = c(F_1(x_1),...,F_d(x_d))\cdot f_1(x_1)\cdots f_d(x_d),$$
or equivalently, 
$$c(x_1,...,x_d) = \frac{h(F_1(x_1),...,F_d(x_d))}{f_1(x_1)\cdots f_d(x_d)},$$
where $f_1,f_2,...,f_d$ are marginal density functions.
\end{theorem}
\begin{proof}
(1) directly from \autoref{ch:statistical-models:th:constructCopulaForJointDistribution}.(2) Use chain rule. Note that
$$h = \frac{\Pa^d H}{\Pa x_1\Pa x_2 ... \Pa x_d},$$
and $$\frac{\Pa H}{\Pa x_1} =\frac{\Pa C}{\Pa x_1} = \frac{\Pa C}{\Pa F_1}\frac{\Pa F_1}{\Pa x_1}.$$
Therefore, 
$$h = \frac{\Pa^d H}{\Pa x_1\Pa x_2 ... \Pa x_d} = \frac{\Pa^d C}{\Pa F_1\Pa F_2 ... \Pa F_d}\frac{\Pa F_1}{\Pa x_1}\frac{\Pa F_2}{\Pa x_2}...\frac{\Pa F_d}{\Pa x_d} = c(F_1,F_2,...,F_d)f_1\cdot f_2\cdots f_d.$$
\end{proof}

\begin{remark}
	The marginal distribution and the copulas can be modeled and estimated separately and independently. 
\end{remark}


\begin{example}[joint density with Gaussian copula]
Let $c(u_1,u_2,...,u_d)$ be a Gaussian copula given by
	$$c(u_1,u_2,...,u_d) = \frac{1}{\sqrt{det(R)}} \exp(-\frac{1}{2}
	\begin{bmatrix}
	\phi^{-1}(u_1) \\
	\vdots\\
	\phi^{-1}(u_d)
	\end{bmatrix}^T \cdot (R^{-1} - I) \begin{bmatrix}
	\phi^{-1}(u_1) \\
	\vdots\\
	\phi^{-1}(u_d)
	\end{bmatrix}
	),$$
where  $\phi$ is the cdf for a standard normal variable. 	
\begin{itemize}
	\item If we have Gaussian margins all given by $\phi$, then we have the joint pdf given by
	\begin{align*}
	h(x_1,x_2,...,x_d) &= c(\phi(x_1),\phi(x_2),...,\phi(x_d))\phi(x_1)\phi(x_2)\cdots \phi(x_d) \\
	&=\frac{1}{\sqrt{det(R)}} \exp(-\frac{1}{2}
	\begin{bmatrix}
	\phi^{-1}(\phi(x_1)) \\
	\vdots\\
	\phi^{-1}(\phi(x_d))
	\end{bmatrix}^T \cdot (R^{-1} - I) \begin{bmatrix}
	\phi^{-1}(\phi(x_1)) \\
	\vdots\\
	\phi^{-1}(\phi(x_d))
	\end{bmatrix}
	) \frac{1}{(2\pi)^{d/2}}\exp(-\frac{1}{2}\sum_{i=1}^{d} x_i^2) \\
	&=\frac{1}{\sqrt{det(R)}} \exp(-\frac{1}{2}x^TR^{-1}x)
	\end{align*}
	\item Let $\phi(x)$ be the standard Gaussian cdf, then a Gaussian random variable $N(m,\sigma^2)$ has pdf given by $\phi(\frac{x-m}{\sigma})$.
	
	Then,
\begin{align*}
h(x_1,...,x_d) &= c(F_1(x_1),...,F_d(x_d))\cdot f_1(x_1)\cdots f_d(x_d) \\
&= \frac{1}{\sqrt{det(R)}} \exp(-\frac{1}{2}
\begin{bmatrix}
\phi^{-1}(\phi(x_1-m_1/\sigma_1)) \\
\vdots\\
\phi^{-1}(\phi(x_d-m_d/\sigma_d))
\end{bmatrix}^T \cdot (R^{-1} - I) \begin{bmatrix}
\phi^{-1}(\phi(x_1-m_1/\sigma_1)) \\
\vdots\\
\phi^{-1}(\phi(x_d-m_d/\sigma_d))
\end{bmatrix})\\
&\frac{1}{(2\pi)^{d/2}}\exp(\begin{bmatrix}
(x_1-m_1/\sigma_1)) \\
\vdots\\
(x_d-m_d/\sigma_d))
\end{bmatrix}^T \cdot \begin{bmatrix}
(x_1-m_1/\sigma_1)) \\
\vdots\\
(x_d-m_d/\sigma_d))
\end{bmatrix}
)
)\\
&= \frac{1}{\sqrt{det(R)}} \exp(-\frac{1}{2}
\begin{bmatrix}
(x_1-m_1/\sigma_1) \\
\vdots\\
(x_d-m_d/\sigma_d)
\end{bmatrix}^T \cdot (R^{-1}) \begin{bmatrix}
(x_1-m_1/\sigma_1) \\
\vdots\\
(x_d-m_d/\sigma_d)
\end{bmatrix})
\frac{1}{(2\pi)^{d/2}}
\\
&= \frac{1}{\sqrt{det(R)}(2\pi)^{d/2}} \exp(-\frac{1}{2}
\begin{bmatrix}
(x_1-m_1/\sigma_1) \\
\vdots\\
(x_d-m_d/\sigma_d)
\end{bmatrix}^T \cdot (\Sigma^{-1}) \begin{bmatrix}
(x_1-m_1/\sigma_1) \\
\vdots\\
(x_d-m_d/\sigma_d)
\end{bmatrix})
\end{align*}
	where $\Sigma^{-1} = D^{-1}R^{-1}D^{-1}$, $D = diag(\sigma_1,\sigma_2,...,\sigma_d)$.
	
	
	\item If we have pdf margins given by $f_1,f_2,...,f_d$ and cdf margins given by $F_1,F_2,...,F_d$, then we have the joint pdf given by
	\begin{align*}
	h(x_1,x_2,...,x_d) &= c(F_1(x_1),F_2(x_2),...,F_d(x_d))f_1(x_1)f_2(x_2)\cdots f_d(x_d) \\
	&=\frac{1}{\sqrt{det(R)}} \exp(-\frac{1}{2}
	\begin{bmatrix}
	\phi^{-1}(F_1(x_1)) \\
	\vdots\\
	\phi^{-1}(F_d(x_d))
	\end{bmatrix}^T \cdot (R^{-1} - I) \begin{bmatrix}
	\phi^{-1}(F_1(x_1)) \\
	\vdots\\
	\phi^{-1}(F_d(x_d))
	\end{bmatrix}
	) f_1(x_1)f_2(x_2)\cdots f_d(x_d) 
	\end{align*}
	 
\end{itemize}	
	
\end{example}




\begin{lemma}[copula density for random vector with independent components]
If a random vector $(X_1,X_2,...,X_d)$ has independent components, then its copula density is given by
$$c = 1.$$
\end{lemma}
\begin{proof}
From \autoref{ch:statistical-models:SklarTheorem}, we know that	
$$c(x_1,...,x_d) = \frac{h(F_1(x_1),...,F_d(x_d))}{f_1(x_1)\cdots f_d(x_d)} = \frac{f_1(x_1)\cdots f_d(x_d)}{f_1(x_1)\cdots f_d(x_d)} = 1.$$	
\end{proof}


\begin{lemma}[copulas invariance under monotone transform]\label{ch:statistical-models:th:copulasinvarianceundermonotonetransform}
	Let $C$ be the copulas associated with random vector $(X_1,X_2,...,X_d)$ and its cdf $F$. 
	It follows that
	\begin{itemize}
		\item 	Let $T_1,T_2,...,T_d$ be increasing functions. Then
		$C$ is also the copulas associated with random vector $(T_1(X_1),T_2(X_2),...,T_n(X_n)).$
		\item Let $T_1,T_2,...,T_d$ be monotone(either increasing or decreasing) functions. \textbf{If $(X_1,X_2,...,X_n)$ have uniform distribution}, then
		$C$ is also the copulas associated with random vector $(T_1(X_1),T_2(X_2),...,T_n(X_n)).$
	\end{itemize}

	
\end{lemma}
\begin{proof}
(1)Denote $(Y_1,Y_2,...,Y_n) = (T_1(X_1),T_2(X_2),...,T_n(X_n))$. 	
	Note that $(Y_1,Y_2,...,Y_n)$ has marginal $$F_{Y_i}(y_i)=F_{X_i}(T_i^{-1}(y_1)).$$
	and cdf
	$$F_Y(y_1,...,y_2) = F_X(T_1^{-1}(y_1),T_2^{-1}(y_2),...,T_n^{-1}(y_n)).$$
	
	Then 
	\begin{align*}
C_Y(u_1,u_2,...,u_d) &= F_Y(F^{-1}_{Y_1}(u_1),F^{-1}_{Y_2}(u_2),...,F^{-1}_{Y_n}(u_n)) \\
&= F_X(T_1^{-1}(F^{-1}_{Y_1}(u_1)),T_2^{-1}(F^{-1}_{Y_2}(u_2)),...,T_n^{-1}(F^{-1}_{Y_n}(u_n))) \\
&=F_X(F_{X_1}^{-1}(u_1),F_{X_2}^{-1}(u_2),...,F_{X_n}^{-1}(u_n))	\\
&=C_X(u_1,u_2,...,u_d)
	\end{align*}
where we use the relation $F_{Y_i} = F_{X_i}\circ T^{-1}$ such that
$$F_{X_1}^{-1} = (F_{Y_1}\circ T)^{-1} = T^{-1}\circ F_{Y_1}^{-1}.$$
Note that $(F_{Y_1}\circ T)$ is invertible only if $T$ is increasing.

(2) We can replace $F_{X_i} = I$ in the above proof and get the result.

\end{proof}

\begin{remark}[caution!]
	This does not hold for decreasing transform.
\end{remark}

\subsubsection{Survival copula}
\begin{definition}[survival copula]\index{survival copula}\hfill
\begin{itemize}
	\item Consider a uniform random vector $(U_1,U_2,...,U_d)$ with joint cdf/copula $C$. The \textbf{survival copula} associated with $C$ is defined by
	$$C^s(u_1,u_2,....,u_d) \triangleq Pr(U_1>u_1,U_2>u_2,...,U_d>u_d).$$
	\item We have relation
	$$1 - F(U_1 < u_1,U_2 < u_2,..., U_d < u_d) = C^s(1 - F_1(u_1),1-F_2(u_2),...,1 - F_d(u_d)).$$
	or equivalently
	$$S(u_1,u_2,..., u_d) = C^s(S_1(u_1),S_2(u_2),...,S_d(u_d)),$$
	where $S\triangleq 1 - F, S_i \triangleq 1 - F_i, i=1,2,...,d $.	
\end{itemize}	
\end{definition}


\begin{lemma}[conversion between copula and survival copula]\hfill
\begin{itemize}
	\item Let $(U_1,U_2,...,U_n)$ be uniform random variables. Let $C$ be its joint cdf/copula, let $C^s$ be its survival copula. 
	Then 
	$$C(u_1,u_2,...,u_n) = 1- C^s(1-u_1,1-u_2,...,1-u_n).$$
	Or equivalently, 
	$$C^s(u_1,u_2,...,u_n) = 1- C(1-u_1,1-u_2,...,1-u_n).$$
	\item  Let $(X_1,X_2,...,X_n)$ be random variables with margins $F_1,F_2,...,F_n$ and copula $C$. Then the joint cdf
	\begin{align*}
	F_{X_1,X_2,...,X_n}(x_1,x_2,...,x_n) &=C(F_1(x_1),F_2(x_2),...,F_n(x_n))\\
						&= 1- C^s(1-F_1(x_1),1-F_2(x_x),...,1-F_n(x_n)) 
	\end{align*}
	Or equivalently, 
		\begin{align*}
	F^s_{X_1,X_2,...,X_n}(x_1,x_2,...,x_n) &\triangleq Pr(X_1>x_1,X_2>x_2,...,X_n>x_n) \\ &=C(F_1(x_1),F_2(x_2),...,F_n(x_n))\\
	&= 1- C^s(1-F_1(x_1),1-F_2(x_x),...,1-F_n(x_n)) 
	\end{align*}
	$$C^s(u_1,u_2,...,u_n) = 1- C(1-u_1,1-u_2,...,1-u_n).$$
	 
\end{itemize}	

\end{lemma}


\subsubsection{Partial differential and conditional distribution}

\begin{theorem}[partial differential of copula gives conditional distribution of uniform random variables]\label{ch:statistical-models:th:partialDifferentialCopulaGivesConditionalDistributionForUniformRandomVariables}\cite[16]{schmitz2003copulas}
\begin{itemize}
	\item (bivariate case)Let $C(u,v)$ be a copula, i.e., the joint cdf  associated with uniform random variables $(U,V)$. Assume $C(u,v)$ can be differentiated. Then
	$$\frac{\Pa}{\Pa v}C(u,v)$$
	is the conditional cdf given by
	$$\frac{\Pa}{\Pa u}C(v,v) = Pr(U<u|V=v).$$
	\item (multivariate case, multiple conditioning on one)Let $C(u_1,u_2,...,u_n)$ be a differentiable copula, i.e., the joint cdf  associated with uniform random variables $(U_1,U_2,...,U_n)$. Then
	$$\frac{\Pa}{\Pa u_k}C(u_1,u_2,...,u_n)$$
	is the conditional cdf given by
	$$\frac{\Pa}{\Pa u_k}C(u_1,u_2,...,u_n) = Pr(U_i\leq u_i, 1\leq i\leq n,i\neq k|U_k=u_k).$$ 
	\item (multivariate case, one conditioning on multiple)Let $C(u_1,u_2,...,u_n)$ be a differentiable copula, i.e., the joint cdf  associated with uniform random variables $(U_1,U_2,...,U_n)$. Then
the conditional cdf given by
	$$\frac{\frac{\Pa^{n-1}}{\Pa u_1 ... \Pa u_{k-1} ... \Pa u_n} C_{1:n}(u_1,...,u_n)}{\frac{\Pa^{k-1}}{\Pa u_1 ... \Pa u_{k-1}} C_{1:k-1}(u_1,...,u_k-1)} = Pr(U_k\leq u_k|U_i=u_i,1\leq i\leq n,i\neq k).$$
\end{itemize}
\end{theorem}
\begin{proof}
(1) 
\begin{align*}
Pr(U\leq u|V = v) &= \lim_{h\to 0} Pr(U\leq u|v\leq V \leq v+h) \\
&= \lim_{h\to 0} \frac{C(u,v+h) - C(u,v)}{C_V(v+h) - C_V(v)} \\
&= \lim_{h\to 0} \frac{C(u,v+h) - C(u,v)}{h} \\
&= \lim_{h\to 0} \frac{C(u,v+h) - C(u,v)}{h} 
& = \frac{\Pa}{\Pa u}C(u,v)
\end{align*}
where $C_V$ is the marginal distribution of $V$ given by(\autoref{ch:statistical-models:th:basicpropertiesofcopulas})
$$C_V(v) = C(1,v) = v.$$
(2)
\begin{align*}
Pr(U_i\leq u_i,1\leq i\leq n,i\neq k|U_k) &= \lim_{h\to 0} Pr(U_i\leq x_i,1\leq i\leq n,i\neq k|u_k\leq U_k \leq u_k+h) \\
&= \lim_{h\to 0} \frac{C(u_1,...,u_k+h,...,u_n) - C(u_1,...,u_k,...,u_n)}{C_{k}(x_k+h) - C_{k}(x_k)} \\
&= \lim_{h\to 0} \frac{C(u_1,...,u_k+h,...,u_n) - C(u_1,...,u_k,...,u_n)}{h} \\
&= \frac{\Pa }{\Pa u_k}C(u_1,...,u_k,...,u_n)
\end{align*}
where $C_k$ is the marginal distribution of $U_k$ given by(\autoref{ch:statistical-models:th:basicpropertiesofcopulas})
$$C_k(u_k) = C(1,...1,,u_k, 1,...,1) = u_k.$$
(3) informally, we can think of the upper as
$$Pr(U_k\leq u_k, U_i=u_i, 1\leq i\leq n,i\neq k);$$
and the lower as
$$Pr(U_i=u_i, 1\leq i\leq n,i\neq k).$$

\end{proof}



\begin{lemma}[partial differential of copula gives conditional distribution for general random variables, bivariate distribution]\label{ch:statistical-models:th:partialDifferentialCopulaGivesConditionalDistributionBivariateDistribution}\cite[16]{schmitz2003copulas}
Let $C(u,v)$ be a copula, i.e., the joint cdf  associated with uniform random variables $(U,V)$. Assume $C(u,v)$ can be differentiated. Then
$$\frac{\Pa}{\Pa u}C(u,v)$$
is the conditional cdf given by
	$$\frac{\Pa}{\Pa u}C(u,v) = Pr(V<v|U=u).$$
\end{lemma}
\begin{proof}
\begin{align*}
Pr(X\leq x|Y = y) &= \lim_{h\to 0} Pr(X\leq x|y\leq Y \leq y+h) \\
&= \lim_{h\to 0} \frac{F_{XY}(x,y+h) - F_{XY}(x,y)}{F_Y(y+h) - F_Y(y)} \\
&= \lim_{h\to 0} \frac{C(F_X(x),F_Y(y+h)) - C(F_X(x),F_Y(y))}{F_Y(y+h) - F_Y(y)} \\
&= \lim_{h\to 0} \frac{C(F_X(x),F_Y(y) + \Delta(h)) - C(F_X(x),F_Y(y))}{\Delta(h)} \\
&= \frac{\Pa }{\Pa q}C(p,q)|_{p=F_X(x),q=F_Y(y)}
\end{align*}
where $\Delta(h) = F_Y(y+h) - F_Y(y).$
\end{proof}


\begin{lemma}[partial differential gives conditional distribution, multiple conditioned on one]\label{ch:statistical-models:th:partialDifferentialCopulaGivesConditionalDistributionMultipleConditionedOnOne}\cite[19]{schmitz2003copulas}
Let $X_1,X_2,...,X_n$ be real-valued random variables with corresponding copula $C$ and continuous marginals $F_1,...,F_n$. Then for any $k\in\{1,2,...,n\},$
$$Pr(X_i\leq x_i,1\leq i\leq n,i\neq k|X_k) = \frac{\Pa }{\Pa F_k(X_k)}C(F_1(x_1),...,F_k(x_k),...,F_n(x_n)),$$
for all $x_1,x_2,...,x_n \in \R$.
\end{lemma}
\begin{proof}
	\begin{align*}
	Pr(X_i\leq x_i,1\leq i\leq n,i\neq k|X_k) &= \lim_{h\to 0} Pr(X_i\leq x_i,1\leq i\leq n,i\neq k|x_k\leq X_k \leq x_k+h) \\
	&= \lim_{h\to 0} \frac{F_{X_1:X_n}(x_1,...,x_k+h,...,x_n) - F_{X_1:X_n}(x_1,...,x_k,...,x_n)}{F_{k}(x_k+h) - F_{k}(x_k)} \\
	&= \lim_{h\to 0} \frac{C(F_1(x_1),...,F_k(x_k+h),...,F_n(x_n)) - C(F_1(x_1),...,F_k(x_k),...,F_n(x_n))}{F_{k}(x_k+h) - F_{k}(x_k)} \\
	&= \lim_{h\to 0} \frac{C(F_1(x_1),...,F_k(x_k) + \Delta(h),...,F_n(x_n)) - C(F_1(x_1),...,F_k(x_k),...,F_n(x_n))}{\Delta(h)} \\
	&= \frac{\Pa }{\Pa F_k(X_k)}C(F_1(x_1),...,F_k(x_k),...,F_n(x_n))
	\end{align*}
	where $\Delta(h) = F_k(x_k+h) - F_k(x_k).$
\end{proof}


\begin{lemma}[partial differential gives conditional distribution, one conditioned on multiple]\label{ch:statistical-models:th:partialDifferentialCopulaGivesConditionalDistributionOneConditionedOnMultiple}\cite[20]{schmitz2003copulas}
	Let $X_1,X_2,...,X_n$ be real-valued random variables with corresponding copula $C$ and continuous marginals $F_1,...,F_n$. Then for any $k\in\{1,2,...,n\},$
	$$Pr(X_i\leq x_i,1\leq i\leq n,i\neq k|X_k) = \frac{\Pa }{\Pa F_k(X_k)}C(F_1(x_1),...,F_k(x_k),...,F_n(x_n)),$$
	for all $x_1,x_2,...,x_n \in \R$.
\end{lemma}
\begin{proof}
	\begin{align*}
	Pr(X\leq x|Y = y) &= \lim_{h\to 0} Pr(X\leq x|y\leq Y \leq y+h) \\
	&= \lim_{h\to 0} \frac{F_{XY}(x,y+h) - F_{XY}(x,y)}{F_Y(y+h) - F_Y(y)} \\
	&= \lim_{h\to 0} \frac{C(F_X(x),F_Y(y+h)) - C(F_X(x),F_Y(y))}{F_Y(y+h) - F_Y(y)} \\
	&= \lim_{h\to 0} \frac{C(F_X(x),F_Y(y) + \Delta(h)) - C(F_X(x),F_Y(y))}{\Delta(h)} \\
	&= \frac{\Pa }{\Pa q}C(p,q)|_{p=F_X(x),q=F_Y(y)}
	\end{align*}
	where $\Delta(h) = F_Y(y+h) - F_Y(y).$
\end{proof}


\subsection{Common copula functions}\label{ch:statistical-models:sec:commonCopulaFunction}
\subsubsection{Gaussian copula}
\begin{definition}[Gaussian copula]\label{ch:statistical-models:def:GaussianCopula}
A Gaussian copula characterized by correlation matrix $R\in [-1,1]^{d\times d}$ is given by
$$C(u_1,u_2,...,u_d; R) = \Phi(\phi^{-1}(u_1),\phi^{-1}(u_2),...,\phi^{-1}(u_d)),$$
where $u_1,u_2,...,u_d \in [0,1]$, $\Phi$ is the cdf for a multivariate normal distribution with zero mean and covariance matrix $R$, $\phi$ is the cdf for a standard normal variable. 

The copula density function is given by
$$c(u_1,u_2,...,u_d) = \frac{1}{\sqrt{det(R)}} \exp(-\frac{1}{2}
\begin{bmatrix}
\phi^{-1}(u_1) \\
\vdots\\
\phi^{-1}(u_d)
\end{bmatrix}^T \cdot (R^{-1} - I) \begin{bmatrix}
\phi^{-1}(u_1) \\
\vdots\\
\phi^{-1}(u_d)
\end{bmatrix}
).$$	
\end{definition}



\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.42\textwidth}
		\includegraphics[width=\textwidth]{figures/statisticalModeling/GaussianCopulaDemo1}
			\end{subfigure}\quad
	\begin{subfigure}[b]{0.42\textwidth}
		\includegraphics[width=\textwidth]{figures/statisticalModeling/GaussianCopulaDemo2}
	\end{subfigure}\quad
	\begin{subfigure}[b]{0.42\textwidth}
		\includegraphics[width=\textwidth]{figures/statisticalModeling/GaussianCopulaDemo3}
		
	\end{subfigure}
		\begin{subfigure}[b]{0.42\textwidth}
		\includegraphics[width=\textwidth]{figures/statisticalModeling/GaussianCopulaDemo4}
		
	\end{subfigure}
	\caption{Gaussian copula with different correlations.}

\end{figure}


\begin{remark}[derivation of copula density function]
From \autoref{ch:statistical-models:SklarTheorem}, we know that	
$$c(x_1,...,x_d) = \frac{h(F_1(x_1),...,F_d(x_d))}{f_1(x_1)\cdots f_d(x_d)}.$$
Note that
$$h(x)= \frac{1}{(2\pi)^{d/2}\abs{det  R}^{1/2}}\exp(-\frac{1}{2}(x)^T R^{-1}(x)),x\in \R^d, $$
and
$$f_1\cdot f_2\cdots f_d= \frac{1}{(2\pi)^{d/2}}\exp(-\frac{1}{2}(x)^T (x)). $$
\end{remark}


\begin{lemma}[the copula associated with multivariate Gaussian distribution is Gaussian copula]\label{ch:statistical-models:th:CopulaOfMultivariateGaussian}\hfill
\begin{itemize}
	\item If $F$ is a multivariate normal distribution $MN(0,R)$, where $R$ is correlation matrix and $\Sigma = R$(that is, the margins are standard normal such that covariance matrix is equal to correlation matrix); then the copula associated with $F$ is the Gaussian copula with correlation matrix $R$
	\item If $F$ is a multivariate normal distribution $MN(0,\Sigma)$, then the copula associated with $F$ is the Gaussian copula with correlation matrix $R = D^{-1/2}\Sigma D^{-1/2}, D = diag(\Sigma).$
	\item If $F$ is a multivariate normal distribution $MN(\mu,\Sigma)$, then the copula associated with $F$ is the Gaussian copula with correlation matrix $R = D^{-1/2}\Sigma D^{-1/2}, D = diag(\Sigma).$
\end{itemize}
\end{lemma}
\begin{proof}
(1)Straight forward(\autoref{ch:statistical-models:th:constructCopulaForJointDistribution}). 
(2)(3) Let $\Phi$ denote the joint cdf $MN(\mu, \Sigma)$ and $\phi_i$ the marginal cdf $N(\mu_i,\sigma_i^2)$.  Let $\Phi^S$ denote the joint cdf $MN(\mu, R)$ and $\phi_i^S$ the marginal cdf $N(0,1)$.

Then,
\begin{align*}
\Phi(u_1,u_2,...,u_d) &= \Phi^S(\frac{u_1 - \mu_1}{\sigma_1^2},\frac{u_2 - \mu_2}{\sigma_2^2},...,\frac{u_d - \mu_d}{\sigma_d}) \\
\phi_i(u_i) &= \phi^S_i(\frac{u_i - \mu_i}{\sigma_i}) \\
\phi_i^{-1}(u_1) &= \sigma_i(\phi^S_i)^{-1} + \mu_i 
\end{align*}

Therefore,
\begin{align*}
&\Phi(\phi^{-1}_i(u_1),\phi^{-1}_2(u_2),...,\phi^{-1}_2(u_d)) \\
=&\Phi^S((\phi^{-1}_i(u_1) - \mu_1)/\sigma_1,(\phi^{-1}_i(u_2) - \mu_2)/\sigma_2,...,(\phi^{-1}_i(u_d) - \mu_d)/\sigma_d) \\
=&\Phi^S((\phi^S_1)^{-1},(\phi^S_2)^{-1},...,(\phi^S_d)^{-1}) 
\end{align*}
\end{proof}


\begin{lemma}[construct a multivariate cdf from Gaussian copulas and margins]\label{ch:statistical-models:th:ConstructMultivariateCDFwithGaussianCopulaAndMargins}
	Consider $d$ random variables $X_1,X_2,...,X_d$ with $F_1,F_2,...,F_d$ being the univariate cdf.
	Let $C$ be a $d$ dimensional Gaussian copulas given by
	$$C(u_1,u_2,...,u_d; R) = \Phi(\phi^{-1}(u_1),\phi^{-1}(u_2),...,\phi^{-1}(u_d)),$$
	where $u_1,u_2,...,u_d \in [0,1]$, $\Phi$ is the cdf for a multivariate normal distribution with zero mean and covariance matrix $R$, $\phi$ is the cdf for a standard normal variable.
	
	Then the joint distribution for $X_1,X_2,...,X_d$ is given by
	$$F(x_1,x_2,...,x_n) = \Phi(\phi^{-1}(F_1(x_1)),\phi^{-1}(F_2(x_2)),...,\phi^{-1}(F_d(x_d)));$$
	and the joint density function is given by
	
	$$f(x)= \frac{1}{(2\pi)^{d/2}\abs{det  R}^{1/2}}\exp(-\frac{1}{2}(x)^T R^{-1}(x))f_1(x_1)f_2(x)\cdots f_d(x_d), $$
	where $$x=(\phi^{-1}(F_1(x_1)),\phi^{-1}(F_2(x_2)),...,\phi^{-1}(F_d(x_d))).$$
	and $f_1,f_2,...,f_d$ are the marginal densities of $X_1,X_2,...,X_n$.
\end{lemma}
\begin{proof}
	(1)See \autoref{ch:statistical-models:th:ConstructMultivariateDistributionFromCopulaAndMargins}.
	(2)See \autoref{ch:statistical-models:SklarTheorem}. 
\end{proof}

\begin{example}[Gaussian margin with Gaussian copula will give multivariate Gaussian]
Note that from \autoref{ch:statistical-models:SklarTheorem}, we know that the joint cdf can be constructed from margins $f_1,f_2,...,f_d$ and $c(u_1,u_2,...,u_d)$ via
$$h(x_1,...,x_d) = c(F_1(x_1),...,F_d(x_d))\cdot f_1(x_1)\cdots f_d(x_d).$$

Let $\phi(x)$ be the standard Gaussian cdf, then a Gaussian random variable $N(m,\sigma^2)$ has pdf given by $\phi(\frac{x-m}{\sigma})$.

Then,
\begin{align*}
h(x_1,...,x_d) &= c(F_1(x_1),...,F_d(x_d))\cdot f_1(x_1)\cdots f_d(x_d) \\
&= \frac{1}{\sqrt{det(R)}} \exp(-\frac{1}{2}
\begin{bmatrix}
\phi^{-1}(\phi(x_1-m_1/\sigma_1)) \\
\vdots\\
\phi^{-1}(\phi(x_d-m_d/\sigma_d))
\end{bmatrix}^T \cdot (R^{-1} - I) \begin{bmatrix}
\phi^{-1}(\phi(x_1-m_1/\sigma_1)) \\
\vdots\\
\phi^{-1}(\phi(x_d-m_d/\sigma_d))
\end{bmatrix})\\
&\frac{1}{(2\pi)^{d/2}}\exp(\begin{bmatrix}
(x_1-m_1/\sigma_1)) \\
\vdots\\
(x_d-m_d/\sigma_d))
\end{bmatrix}^T \cdot \begin{bmatrix}
(x_1-m_1/\sigma_1)) \\
\vdots\\
(x_d-m_d/\sigma_d))
\end{bmatrix}
)
)\\
&= \frac{1}{\sqrt{det(R)}} \exp(-\frac{1}{2}
\begin{bmatrix}
(x_1-m_1/\sigma_1) \\
\vdots\\
(x_d-m_d/\sigma_d)
\end{bmatrix}^T \cdot (R^{-1}) \begin{bmatrix}
(x_1-m_1/\sigma_1) \\
\vdots\\
(x_d-m_d/\sigma_d)
\end{bmatrix})
\frac{1}{(2\pi)^{d/2}}
\\
&= \frac{1}{\sqrt{det(R)}(2\pi)^{d/2}} \exp(-\frac{1}{2}
\begin{bmatrix}
(x_1-m_1/\sigma_1) \\
\vdots\\
(x_d-m_d/\sigma_d)
\end{bmatrix}^T \cdot (\Sigma^{-1}) \begin{bmatrix}
(x_1-m_1/\sigma_1) \\
\vdots\\
(x_d-m_d/\sigma_d)
\end{bmatrix})
\end{align*}
where $\Sigma^{-1} = D^{-1}R^{-1}D^{-1}$, $D = diag(\sigma_1,\sigma_2,...,\sigma_d)$.

\end{example}


\begin{example}[bivariate Gaussian copula]
	$$C_{\rho}(u,v) = \int_{-\infty}^{\Phi^{-1}(u)}\int_{-\infty}^{\Phi^{-1}(v)}\frac{1}{2\pi(1-\rho^2)^{1/2}}\exp(-\frac{x^2-2\rho x y + y^2}{2(1-\rho^2)}) dxdy$$
	where $\rho$ is the linear correlation coefficient.
\end{example}



\subsubsection{t copula}

\begin{definition}[t copula]\label{ch:statistical-models:def:tCopula}\cite[419]{roncalli2016lecture}
A t-copula 	characterized by correlation matrix $R\in [-1,1]^{n\times n}$ and degree of freedom parameter $v$ is the copula associated with the multivariate Student's t probability distribution
$$C(u_1,u_2,...,u_n;\rho,v) = T_n(T_v^{-1}(u_1),T_v^{-1}(u_2),...,T_v^{-1}(u_n);R,v).$$


\end{definition}


\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.42\textwidth}
		\includegraphics[width=\textwidth]{figures/statisticalModeling/StudentTCopulaDemo1}
	\end{subfigure}\quad
	\begin{subfigure}[b]{0.42\textwidth}
		\includegraphics[width=\textwidth]{figures/statisticalModeling/StudentTCopulaDemo2}
	\end{subfigure}\quad
	\begin{subfigure}[b]{0.42\textwidth}
		\includegraphics[width=\textwidth]{figures/statisticalModeling/StudentTCopulaDemo3}
		
	\end{subfigure}
	\begin{subfigure}[b]{0.42\textwidth}
		\includegraphics[width=\textwidth]{figures/statisticalModeling/StudentTCopulaDemo4}
		
	\end{subfigure}
	\caption{Student T copula with different correlations.}
	
\end{figure}


\begin{example}[bivariate t copula]
	$$C(u_1,u_2;\rho,v) = \int_{-\infty}^{T^{-1}_v(u_1)}\int_{-\infty}^{T^{-1}_v(u_2)}\frac{1}{2\pi(1-\rho^2)^{1/2}}(1 + \frac{x_1^2+x_2^2-2\rho x_1x_2}{v(1-\rho^2)})dx_1dx_2$$
	where $\rho$ is the linear correlation coefficient.
\end{example}

\cite{cherubini2004copula}

\subsubsection{Common copula functions: other copula}

\begin{definition}[product copula, co-monotonicity copula]\cite[187]{ruppert2015statistics}\cite[190]{mcneil2015quantitative}
	\begin{itemize}
		\item \textbf{(product copula, independence copula)}The d-dimensional product copula is given by
		$$C(u_1,u_2,...,u_d) = u_1u_2\cdots u_d, u_i\in [0,1],\forall i.$$
		The product copula corresponds to independence and it can be viewed as the cdf of $(U_1,...,U_d)$, where $U_1,...,U_d$ are independent uniform random variables.
		\item \textbf{(co-monotonicity copula)} The joint cdf of the random vector $(U_1,U_2,...,U_d)$, where $U$ is a uniform random variable is called a co-monotonicity copula, which characterizes perfect positive correlation. It is given by
		$$C(u_1,u_2,...,u_d) = \min(u_1,u_2,...,u_d).$$
		\item \textbf{two dimensional counter-monotonicity copula} is defined as the joint cdf of $(U,1-U)$.Therefore,
		\begin{align*}
		C(u_1,u_2) \triangleq &Pr(U\leq u_1,(1-U)\leq u_2) \\
		=& Pr(1-u_2\leq U\leq u_1) \\
		=& \max\{u_1+u_2-1,0\}
		\end{align*}
	\end{itemize}	
\end{definition}

\begin{remark}[interpretation]\cite[185]{ruppert2015statistics}\hfill
\begin{itemize}
	\item The co-monotonicity copula is the joint cdf of $\bm{U}=(U,U,...,U)$;that is, $\bm{U}$ contains $d$ copies of $U(0,1)$. The co-monotonicity copula is the upper bound of all copula functions(\autoref{ch:statistical-models:th:FrechetHoeffdingBounding}). 
	\item The two dimensional counter-monotonicity copula is the lower bound of all copula functions.
\end{itemize}
\end{remark}


\begin{definition}[t-copula]
\end{definition}

\subsection{Dependence and copula}

\subsubsection{Linear correlations}

\begin{example}
Consider discrete-valued random variable $V_1$ and $V_2$ given by: 
\begin{itemize}
	\item $V_1$ equally take three different values $-1,0,+ 1$.
	\item If $V_1 = -1 ~or~ + 1$, $V_2 = 1$. If $V_1= 0$, then $V_2 = 0$. 
\end{itemize}

It is clearly that $E[V_1V_2] = 0, E[V_1] = 0 \implies Cov[V_1,V_2] = 0$; however, it is clearly that $V_1$ and $V_2$ are uncorrelated but they are dependent since 
$$Pr(V_2|V_1 = v) \neq Pr(V_2).$$
where $$Pr(V_2 = 1) = Pr(V_2 = 1|V_1 = 1)Pr(V_1 = 1) + Pr(V_2 = 1|V_1 = 1)Pr(V_1 = 1) = 1\times 1/3 + 1\times 1/3 = 2/3;$$
and
$$Pr(V_2 = 0) = Pr(V_2 = 0|V_1 = 0)Pr(V_1 = 0) = 1\times 1/3= 1/3.$$
\end{example}



\begin{definition}[linear correlation, Pearson correlation]\cite[202]{mcneil2015quantitative}\index{linear correlation}\index{Person correlation}
Given two random variables $X$ and $Y$. The linear correlation is defined by	
	$$\rho(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var[X]Var[Y]}}.$$
\end{definition}



\begin{note}[characteristics of linear correlations]\hfill
\begin{itemize}
	\item sensitive to outliers.
	\item measure the 'average dependence' between $X$ and $Y$.
	\item invariant under strictly \textbf{increasing linear transformation}.
	\item May be misleading when multivariate distribution is not elliptical.
\end{itemize}
\end{note}


\begin{lemma}[invariance of correlation under affine transformation]\label{ch:statistical-models:th:CorrelationInvarianceUnderAffineTransformation}\hfill
\begin{itemize}
	\item Let random variables $X$ and $Y$ have correlation $\rho(X,Y)$. Then
	$$\rho(aX+b, cY + d) = \rho(X,Y), \forall a,c >0, b,d\in \R.$$
	\item Let random variables $X$ and $Y$ have correlation $\rho(X,Y)$. Then
	$$\rho(aX+b, cY + d) = \frac{cd}{\abs{cd}}\rho(X,Y), \forall a,b, b,d\in \R.$$
\end{itemize}	
	
\end{lemma}
\begin{proof}
(1)	
$$\rho(aX+b, cY + d) \triangleq  \frac{Cov(aX + b,cY + d)}{\sqrt{Var[aX+b]Var[cY+d]}} = \frac{aCov(X,Y)d}{\sqrt{a^2Var[X]Var[Y]c^2}} = = \rho(X,Y). $$	
(2) straight forward.
\end{proof}

\begin{remark}[generally not invariant under nonlinear transformation]
Let $T:\R \to \R$ be a nonlinear strictly increasing function. Then generally $$\rho(X,Y) \neq \rho(T(X), T(Y)).$$	
\end{remark}


\begin{lemma}[perfect linear correlation and linear function relationship]\label{ch:statistical-models:th:PerfectLinearCorrelationAndLinearFunctionRelationship}\cite[202]{mcneil2015quantitative} Let $X$ and $Y$ be two random variable defined on the same probability space.  It follows that
\begin{itemize}
	\item $\rho(X,Y) = 1$ implies $Y = \alpha  + \beta X$ almost surely for some $\alpha,\beta \in \R, \beta >0$.
	\item $\rho(X,Y) = -1$ implies $Y = \alpha  + \beta X$ almost surely for some $\alpha,\beta \in \R, \beta <0$.
	\item Conversely, if $Y = \alpha + \beta X$, then $\rho(X,Y) = sign(\beta)$.
\end{itemize}	
\end{lemma}
\begin{proof}
(1)(2) Note that the equality holds only when the equality holds in Cauchy inequality(\autoref{ch:theory-of-probability:th:Cauchy-SchwarzInequality}).Then, $X$ and $Y$ must have linear dependence almost everywhere. 
(3)  Use the affine transformation invariance property of correlation(\autoref{ch:statistical-models:th:CorrelationInvarianceUnderAffineTransformation}),
\end{proof}

\subsubsection{Rank correlations}

\begin{definition}[Kendall's tau for observations]\label{ch:statistical-models:th:KendallTauForObservations}\hfill
\begin{itemize}
	\item Let $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$ be a set of joint observations from two random variables $X$ and $Y$.
	\item A pair of observations $(x_i,y_i)$ and $(x_j,y_j)$ are \textbf{concordant} if both $x_i > x_j$ and $y_i>y_j$ or if both $x_i < x_j$ and $y_i<y_j$; 
	\item They are \textbf{discordant}, if $x_i > x_j$ and $y_i<y_j$ or if $x_i < x_j$ and $y_i>y_j$. 
	\item If $x_i=x_j, y_i=y_j$, the pair is neither concordant nor discordant.
	\item The \textbf{Kendall $\tau$ coefficient} is defined as
	$$\rho_\tau = \frac{num ~concordant~pairs - num ~disconcordant~pairs}{n(n-1)/2}.$$
\end{itemize}	
Note that $n(n-1)/2$ is the total number of pairs to compare.
\end{definition}


\begin{definition}[Kendall's tau for random variables]\index{Kendall's tau}\cite[207]{mcneil2015quantitative}
Let $X_1$ and $X_2$ be two random variables. Then the Kendall's tau is given by
$$\rho_\tau = E[sign((X_1-\tilde{X}_1)(X_2 - \tilde{X}_2))],$$
where $(\tilde{X}_1,\tilde{X}_2)$ is a independent copy of $(X_1,X_2)$; that is,$(\tilde{X}_1,\tilde{X}_2)$ has the same cdf of $(X_1,X_2)$, but they are statistically independent. 	
\end{definition}




\begin{lemma}[Kendall's tau from copula]\cite[207]{mcneil2015quantitative}\label{ch:statistical-models:KendallTauFromCopula}
The $C$ be the copula associated with the joint cdf of $(X,Y)$, then 
\begin{align*}
\rho_{\tau}(X,Y) & = 4\int_0^1\int_0^1 C(u,v)dC(u,v) - 1 \\
&=4E[C(U,V)] - 1
\end{align*}
\end{lemma}
\begin{proof}
From the definition 
\begin{align*}
\rho_\tau &= E[sign((X_1-\tilde{X}_1)(X_2 - \tilde{X}_2))] \\
& =E[\bm{1}((X_1-\tilde{X}_1)(X_2 - \tilde{X}_2)>0)]-E[\bm{1}((X_1-\tilde{X}_1)(X_2 - \tilde{X}_2)<0)] \\
&=Pr((X_1-\tilde{X}_1)(X_2 - \tilde{X}_2)>0)-Pr((X_1-\tilde{X}_1)(X_2 - \tilde{X}_2)<0) \\
&=2Pr((X_1-\tilde{X}_1)(X_2 - \tilde{X}_2)>0) - 1\\
&=2Pr((X_1-\tilde{X}_1)>0,(X_2 - \tilde{X}_2)>0) + 2Pr((X_1-\tilde{X}_1)<0,(X_2 - \tilde{X}_2)<0)- 1 \\
&=4Pr((X_1-\tilde{X}_1)<0,(X_2 - \tilde{X}_2)<0) - 1 \\
&=4\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}Pr((X_1<s_1,X_2 <s_2)f_{\tilde{X}_1,\tilde{X}_2}(s_1,s_2)ds_1ds_2 - 1 \\
&=4\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}F(s_1,s_2)dF(s_1,s_2) - 1 \\
&=4\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}C(F_1(s_1),F_2(s_2))dC(F_1(s_1),F_2(s_2)) - 1 \\
&=4\int_{0}^{1}\int_{0}^{1}C(u_1,u_2)dC(u_1,u_2) - 1
\end{align*}

\end{proof}

\begin{remark}[Kendall's tau is independent of the marginal cdf]
Note that Kendall's tau only depends on the correlation structure characterized by the copula; it is independent of the marginal cdf.	
\end{remark}


\begin{lemma}[Hoffding formula for covariance]\cite[204]{mcneil2015quantitative}\label{ch:statistical-models:th:HoffdingFormuaForCovariance}
If $(X_1,X_2)$ has joint cdf $F$ and marginal cdf $F_1$ and $F_2$, then
\begin{itemize}
	\item $$Cov(X_1,X_2) = \frac{1}{2}E[(X_1-\tilde{X}_1)(X_2 - \tilde{X}_2)],$$
	where $(\tilde{X}_1,\tilde{X}_2)$ is a independent copy of $(X_1,X_2)$; that is,$(\tilde{X}_1,\tilde{X}_2)$ has the same cdf of $(X_1,X_2)$, but they are statistically independent. 
	\item $$Cov(X_1,X_2) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} (F(x_1,x_2) - F_1(x_1)F_2(x_2))dx_1dx_2.$$
\end{itemize}
\end{lemma}
\begin{proof}
(1) Directly expand the rhs. Note that $E[X_1\tilde{X}_2] = E[X_1]E[\tilde{X}_2]$ due to independence.
(2)
A useful identity is for any $a\in \R,b\in R$, we have
$$(a-b) = \int_{-\infty}^{\infty} H(x-b) - H(x-a) dx.$$

We have
\begin{align*}
&E[(X_1-\tilde{X}_1)(X_2 - \tilde{X}_2)] \\
=& E[\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (H(s_1-X_1) - H(s_1-\tilde{X}_1))(  H(s_2-X_2) - H(s_2-\tilde{X}_2)) ds_1ds_2]\\
=&\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} E[(H(s_1-X_1) - H(s_1-\tilde{X}_1))(  H(s_2-X_2) - H(s_2-\tilde{X}_2))] ds_1ds_2 \\
=&2\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} Pr(X_1\leq s_1,X_2\leq s_2) - Pr(X_1\leq s_1)Pr(X_2\leq s_2)ds_1ds_2\\
=&2\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} (F(s_1,s_2) - F_1(s_1)F_2(s_2))ds_1ds_2
\end{align*}
\end{proof}




\begin{definition}[Spearman's rho]\index{Spearman's rho}\cite[207]{mcneil2015quantitative}
Let $X_1$ and $X_2$ be two random variables with marginal cdf $F_1$ and $F_2$. \textbf{Spearman's rho} is defined as
	$$\rho_{S}(X,Y)  = \rho(F_1(X_1),F_2(X_2)).$$
In other words, Spearman's rho is the linear correlation of the transform random variables.	
\end{definition}


\begin{definition}[Spearman's rho for observations]\index{Spearman's rho}\label{ch:statistical-models:def:SpearmanRhoFromObservations}\hfill
\begin{itemize}
	\item Let $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$ be a set of joint observations from two random variables $X$ and $Y$.
	\item Let $R(x_i)$ denote the rank of $x_i$ among $x_1,x_2,...,x_n$, where $R(x_i)$ will take value from 1 to n. Similarly we denote $R(y_i)$ as the rank of $y_i$. 
	\item The \textbf{Spearman's $\rho$ coefficient} is defined as
	$$\rho_S = \frac{\frac{1}{n}\sum_{i=1}^n (R(x_i) - \bar{R}(x))(R(y_i) - \bar{R}(y))}{\sqrt{\frac{1}{n}\sum_{i=1}^n (R(x_i) - \bar{R}(x))^2\frac{1}{n}\sum_{i=1}^n (R(y_i) - \bar{R}(y))^2 }},$$
	where $\bar{R}(y) = \frac{1}{n}\sum_{i=1}^n R(y_i)$.
	\item It can be showed that 
	$$\rho_S = 1 - \frac{\sum_{i=1}^{n}(R(x_i) - R(y_i))^2}{n^3-n}.$$
\end{itemize}	
\end{definition}

\begin{lemma}[properties of Spearman's rho for observations]
Let $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$ be a set of joint observations from two random variables $X$ and $Y$. It follows that
\begin{itemize}
	\item $$n\bar{R}(y) = \sum_{i=1}^n R(y_i) = \frac{1}{2}n(n+1).$$
	\item $$\sum_{i=1}^n R(y_i)^2 = \sum_{i=1}^n R(x_i)^2 = \frac{n(n+1)(2n+1)}{6}.$$
	\item $$\rho_S = 1 - \frac{\sum_{i=1}^{n}(R(x_i) - R(y_i))^2}{n^3-n}.$$
\end{itemize}	
\end{lemma}
\begin{proof}
(1)straight forward.(2) this is the sum of squares from 1 to n. (3)
Note that
$$\sum_{i=1}^n (R(x_i) - \bar{R}(x))^2 = \sum_{i=1}^n R(x_i)^2 - (\sum_{i=1}^n R(x_i))^2 = \frac{n(n+1)(2n+1)}{6} - (\frac{1}{2}n(n+1))^2.$$
and
$$\sum (R(x_i) - R(y_i))^2 = \sum R(x_i)^2 + \sum R(y_i)^2 - 2\sum R(x_i)R(y_i)$$
implies 
$$2\sum R(x_i)R(y_i) =\sum R(x_i)^2 + \sum R(y_i)^2 - \sum (R(x_i) - R(y_i))^2. $$ 	
\end{proof}

\begin{lemma}[Spearman's rho from copula]\cite[207]{mcneil2015quantitative}\label{ch:statistical-models:th:SpearmanRhoFromCopula}
\begin{align*}
\rho_{S}(X,Y) = 12\int_0^1\int_0^1 C(u,v)dudv - 3
\end{align*}	
\end{lemma}
\begin{proof}
From definition and use \autoref{ch:statistical-models:th:HoffdingFormuaForCovariance}, we have
\begin{align*}
\rho_{S}(X,Y)  &= \rho(F_1(X_1),F_2(X_2)) \\
& = Cov(F_1(X_1),F_2(X_2))/(\sqrt{Var[F_1[X_1]]Var[F_2[X_2]]}) \\
& = 12 Cov(F_1(X_1),F_2(X_2)) \\
& = 12 \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \hat{F}(F_1(x_1),F_2(x_2)) - \hat{F}_1(F_1(x_1))\hat{F}_2(F_2(x_2)) dx_1dx_2\\
& = 12 \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \hat{F}(F_1(x_1),F_2(x_2)) - x_1x_2 dx_1dx_2 \\
& = 12(\int_0^1\int_0^1 C(u,v)dudv - \frac{1}{4})
\end{align*}	
where we use the property $F_1(X_1),F_2(X_2)$ are uniform random variable with variance 1/12, and the joint cdf of $(F_1(X_1),F_2(X_2))$ is the copula associated with joint cdf of $(X_1,X_2)$( 
\autoref{ch:statistical-models:th:ProbababilityTransformForRandomVectorAndProperties}).
\end{proof}


\begin{lemma}[Spearman's rho for monotonic relation]
Let $X$ be a random variable and let $Y$ be a monotonic function of $X$, denoted by $Y = f(X)$. It follows that
\begin{itemize}
	\item If $f$ is a monotonically increasing function, the $\rho_S(X,Y) = 1.$
	\item If $f$ is a monotonically decreasing function, the $\rho_S(X,Y) = -1.$
\end{itemize}	
\end{lemma}
\begin{proof}
(1)Consider the Spearman's rho for observations(\autoref{ch:statistical-models:def:SpearmanRhoFromObservations}). For each sample $(x_i,y_i)$, each component has the same rank.	
(2)For each sample $(x_i,y_i)$, each component has ranks satisfying
$$R(x_i) - \bar{R}(x_i) = -(R(y_i) - \bar{R}(y_i)).$$	

\end{proof}



\begin{lemma}[first quadrant probability of bivariate Gaussian distribution]\cite[215]{mcneil2015quantitative}\label{ch:statistical-models:th:FirstQuadrantProbabilityBivariateGaussain}
Let $(X_1,X-2)$ be a random vector with joint multivariate Gaussian distribution $MN(0,\Sigma)$. Let $\rho = \rho(X_1,X_2)$. Then
$$Pr(X_1>0, X_2>0) = \frac{1}{4}+ \frac{\arcsin \rho}{2\pi}.$$	
\end{lemma}
\begin{proof}
See reference. 
\end{proof}

\begin{lemma}[rank correlation for Gaussian copula]\cite[215]{mcneil2015quantitative}\label{ch:statistical-models:th:RankCorrelationGaussianCopula}
Let $(X_1,X_2)$ be a bivariate random vector with Gaussian copula characterized by correlation coefficient $\rho$ and continuous margins. Then
\begin{itemize}
	\item $$\rho_\tau(X_1,X_2) = \frac{2}{\pi}\arcsin \rho$$	
	\item $$\rho_S(X_1,X_2) = \frac{6}{\pi}\arcsin \frac{1}{2}\rho$$	
\end{itemize}	
\end{lemma}
\begin{proof}
(1) Note that Kendall's tau only depends on the copula; therefore we can assume $(X_1,X_2)$ has bivariate normal distribution($MN(0,2\Sigma)$, correlation $\rho$). From \autoref{ch:statistical-models:KendallTauFromCopula},
\begin{align*}
\rho_\tau &= 4Pr((X_1-\tilde{X}_1)>0,(X_2 - \tilde{X}_2)>0) - 1 \\
&=4Pr(Y_1>0,Y_2>0) - 1 \\
&=4(\frac{1}{4}+ \frac{\arcsin \rho}{2\pi}) - 1\\
&=\frac{2}{\pi}\arcsin \rho
\end{align*}
where $(\tilde{X}_1,\tilde{X}_2)$ is the independent copy of $(X_1,X_2)$, and $Y_1 = X_1-\tilde{X_2}\sim MN(0,2\Sigma)$($Y_1$ has the same correlation $\rho$), we use \autoref{ch:statistical-models:th:FirstQuadrantProbabilityBivariateGaussain}.
(2) From \autoref{ch:statistical-models:th:SpearmanRhoFromCopula}, we have
\begin{align*}
\rho_S(X_1,X_2) &= 12\int_0^1\int_0^1 C(u,v)dudv - 3 \\
&=12\int_0^1\int_0^1 \Phi(\phi^{-1}(u),\phi^{-1}(v))dudv - 3 \\
&=12\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \Phi(s_1,s_2)d\phi(s_1)d\phi(s_2) - 3 \\
&=12\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \Phi(s_1,s_2) f(s_1)f(s_2)ds_1ds_2 - 3 \\
&=12 Pr(X_1-S_1<0, X_2-S_2<0) -3 \\
&=12 Pr(Y_1<0,Y_2<0) -3 \\
&=12(\frac{1}{4}+ \frac{\arcsin \rho/2}{2\pi}) - 3\\
&=\frac{6}{\pi}\arcsin \frac{1}{2}\rho
\end{align*}
where $(\tilde{X}_1,\tilde{X}_2)$ is the independent copy of $(X_1,X_2)$, and $Y_1 = X_1-S_1\sim MN(0,\Sigma + I_2)$($Y_1$ has the correlation $\rho/2$), we use \autoref{ch:statistical-models:th:FirstQuadrantProbabilityBivariateGaussain}.
\end{proof}


\begin{remark}[applications in robust correlation estimation for multivariate Gaussian random variables]
Note that multivariate Gaussian distribution has Gaussian copula(\autoref{ch:statistical-models:th:CopulaOfMultivariateGaussian}). Therefore, we can estimate $\rho_\tau, \rho_S$ first(which is robust) and then convert them to linear correlation coefficients.	
\end{remark}



\subsubsection{Tail dependence}

\begin{definition}[tail dependence]
Let $X$ and $Y$ be random variables with marginal cdf $F_X$ and $F_Y$.
\begin{itemize}
	\item The \textbf{coefficient	of upper tail dependence} of $X$ and $Y$ is
	\begin{align*}
	\lambda_u(X,Y) \triangleq &= \lim_{\alpha\to 1} Pr(F_Y(Y) > \alpha | F_X(X) > \alpha) \\
	&= \lim_{\alpha\to 1} Pr(Y > F^{-1}_Y(\alpha)|X > F^{-1}_X(\alpha)).
	\end{align*}
	\item The \textbf{coefficient	of lower tail dependence} of $X$ and $Y$ is
	\begin{align*}
	\lambda_l(X,Y) \triangleq &= \lim_{\alpha\to 0} Pr(F_Y(Y) \leq \alpha | F_X(X) \leq \alpha) \\
	&= \lim_{\alpha\to 0} Pr(Y > F^{-1}_Y(\alpha)|X > F^{-1}_X(\alpha)).
	\end{align*}
\end{itemize}
 

Tail dependence is the probability of observing a large(small) $Y$ given that $X$ is large(small). If $\lambda_u > 0(\lambda_l > 0)$, then we say $(X,Y)$ has an upper(lower) tail dependence. 
\end{definition}

\begin{lemma}[tail dependence from copula]\hfill
\begin{itemize}
	\item 
	$$\lambda_l = \lim_{q\to 0^+} \frac{ Pr(F_Y(Y) \leq q , F_X(X) \leq q)}{Pr(F_X(X) \leq q)} = \lim_{q\to 0^+} \frac{C(q,q)}{q}.$$
	\item 
	$$\lambda_u = \lim_{q\to 1^-} \frac{ Pr(F_Y(Y) > q , F_X(X) > q)}{Pr(F_X(X) > q)} = \lim_{q\to 1^-} \frac{C^s(q,q)}{1-q},$$
	where $C^s$ is the survival copula.
\end{itemize}	
\end{lemma}

\begin{remark}[tail dependence is independent of the marginal cdf]\hfill
\begin{itemize}
	\item 	Note that tail dependence only depends on the correlation structure characterized by the copula; it is independent of the marginal cdf.
	\item The existence of tail will depend on margins.
\end{itemize}	
	
\end{remark}


\begin{lemma}[tail independence of Gaussian copula]
Let $(X,Y)$ be a bivariate random vector with Gaussian copula characterized by correlation coefficient $\rho$ and continuous margins.	Then
	$$\lambda_u=\lambda_l = 2\lim_{x\to \infty} \Phi(x\sqrt{1-\rho}/\sqrt{1+\rho}) = 0.$$
\end{lemma}
\begin{proof}
From definition, we have
\begin{align*}
\lambda_l &= \lim_{q\to 0^+} \frac{ Pr(F_Y(Y) \leq q , F_X(X) \leq q)}{Pr(F_X(X) \leq q)} \\
&=\lim_{q\to 0^+} \frac{ Pr(Y \leq f^{-1}(q) , X \leq \phi^{-1}(q))}{Pr(X \leq f^{-1}(q))} \\
&=\lim_{q\to -\infty} \frac{ Pr(Y \leq q , X \leq q)}{Pr(X \leq q)} \\
&=\lim_{q\to -\infty} \frac{ Pr(Y \leq q , X \leq q)}{Pr(X \leq q)} \\
&= \lim_{q\to -\infty} \frac{ \int_{-\infty}^{q}\int_{-\infty}^{q} f(x,y)dxdy}{\int_{-\infty}^{q} f(x)dx} \\
&=\lim_{q\to -\infty} \frac{ \int_{-\infty}^{q}f(x,q)dx}{ f(q)} + \frac{ \int_{-\infty}^{q}f(q,y)dy}{ f(q)} \\
&=2\lim_{q\to -\infty} \frac{ \int_{-\infty}^{q}f(x,q)dx}{ f(q)} \\
&=2\lim_{q\to -\infty} f(x|y=q)dx
\end{align*}
where $f(x,y)$ is the density of $(X,Y)$, $f(x)$ is the marginal density, and we use L'hospital rule in the derivation.  
Note that $X|y=q \sim N(\rho q, 1-\rho^2)$(\autoref{ch:theory-of-statistics:th:multivariatenormalconditionaldistribution}); therefore, 
$$\lim_{q\to -\infty} f(x|y=q)dx = \Phi(\frac{q-\rho q}{\sqrt{1-\rho^2}}).$$
\end{proof}

\subsection{Estimating copula function}
\subsubsection{Empirical copula method}


\begin{definition}[empirical copula]\cite[424]{roncalli2016lecture}
Suppose we have observation of $n$ iid $d$ dimensional random vectors, $$X^i = (X_1^{(i)},X_2^{(i)},...,X_d^{(i)}),i=1,2,...,n,$$
We can construct its empirical copula associated with the joint distribution of $X$ via the following procedures:	
	\begin{itemize}
		\item (construct empirical marginal cdf)
		$$\hat{F}_k(x) = \frac{1}{n}\sum_{i=1}^{n}\bm{1}(X^{(i)}_k\leq x), k=1,2,...,d$$
		\item (construct transformed uniform sample)
		$$(\hat{U}_1^{(i)},...,\hat{U}_d^{(i)}) = (\hat{F}_1(X_1^i),...,\hat{F}_d(X_d^{(i)})),i=1,...,n$$
		\item (construct empirical copula)$$\hat{C}(u_1,u_2,...,u_d) = \frac{1}{n}\sum_{i=1}^n \bm{1}(\hat{U}_1^{(i)} \leq u_1,...,\hat{U}_1^{(i)} \leq u_d)$$
	\end{itemize}	
\end{definition}


\begin{remark}
	The nature of copula is the cdf of the transformed uniform random vector $(F_1(X_1),F_2(X_2),...,F_n(X_n))$(\autoref{ch:statistical-models:th:ProbababilityTransformForRandomVectorAndProperties}).
\end{remark}


\subsubsection{Maximum likelihood method}


\begin{lemma}[maximum likelihood function and two-stage estimation method]\cite[429]{roncalli2016lecture}
Suppose we have observation of $n$ iid $d$ dimensional random vectors, $$X^i = (X_1^{(i)},X_2^{(i)},...,X_d^{(i)}),i=1,2,...,n.$$
Assume the joint cdf for $X$ is given by
$$F(x_1,x_2,...,x_n) = C(F_1(x_1;\theta_1),...,F_d(x_d;\theta_d)),$$
such that we have two sets of parameters given by
\begin{itemize}
	\item $(\theta_1,...,\theta_d)$ for univariate distribution function $F_1,F_2,...,F_d$;
	\item $\theta_c$ for the copula function $C(u_1,...,u_d)$.
\end{itemize}	
\end{lemma}

The maximum log likelihood function is given by
\begin{align*}
&l(\theta_1,...,\theta_d,\theta_c) \\
&
= \sum_{i=1}^{n} \ln c(F_1(x^{(j)}_1;\hat{\theta}_1),...,F_1(x^{(j)}_d;\hat{\theta}_d) + 
\sum_{i=1}^{n}\sum_{j=1}^{d} 
\end{align*}
The first stage is to estimate univariate parameter $\theta_1,...,\theta_d$ via
$$\hat{\theta}_i = \arg\max \sum_{j=1}^{N} \ln f_i(x^{(j)}_i;\theta_i).$$

The second stage is to estimate the copula parameters $\theta_c$ with the estimated univariate parameters $\hat{\theta}_1,\hat{\theta}_2,...,\hat{\theta}_d$ fixed via
$$\hat{\theta}_c = \arg\max \sum_{j=1}^{N} \ln c(F_1(x^{(j)}_1;\hat{\theta}_1),...,F_1(x^{(j)}_d;\hat{\theta}_d);\theta_c).$$

\subsection{Applications of copula}

\subsubsection{Generating correlated uniform random number}

\begin{lemma}[conditional method for generating bivariate uniform random number with arbitrary joint distribution]\cite[96]{schmitz2003copulas}
Suppose that we want to generate a pair of random variables with marginal uniform $U(0,1)$ and joint cdf given by copula $C(u,v)$. 

We use the following procedures:
\begin{itemize}
	\item Generate $U$ and $T$ independently from $U(0,1)$; 
	\item Set $V = C_u^{-1}(T)$, where
	$C_u = \frac{\Pa}{\Pa u}C(u,v).$
	\item The desired pair is $(U,V)$.
\end{itemize}	
\end{lemma}
\begin{proof}
Note that (\autoref{ch:statistical-models:th:partialDifferentialCopulaGivesConditionalDistributionForUniformRandomVariables})
$$C_u = \frac{\Pa}{\Pa u}C(u,v) = Pr(V<v|U=u),$$
which is the conditional cdf. Then we use inverse transformation method to get $V$.
\end{proof}


\begin{lemma}[conditional method for generating bivariate uniform random number with arbitrary joint distribution]\cite[96]{schmitz2003copulas}
	
	Suppose that we want to generate a set of $n$ random variables with marginal uniform $U(0,1)$ and joint cdf given by copula $C_n(u_1,u_2,...,u_n)$.
	
	Further denote $C_{1:k}(u_1,...,u_k)$ be the copula of $(U_1,U_2,...,U_k), 2\leq k\leq n$ and set $C_1(u_1) = u_1$. 
	
	We use the following procedures:
	\begin{itemize}
		\item Simulate $u_1$ from $U(0,1)$.
		\item Simulate $u_2$ from the conditional distribution function $C_2(u_2|u_1)$.
		\item Simulate $u_3$ from the conditional distribution $C_3(u_3|u_1,u_2)$.
		\item $\cdots$
		\item Simulate $u_n$ from the conditional distribution $C_n(u_n|u_1,u_2,...,u_{n-1})$.
	\end{itemize}	
where
\begin{align*}
C_k(u_k|u_1,...,u_{k-1}) &\triangleq Pr(U_k\leq u_k|U_1=u_1,...,U_{k-1}=u_{k-1}) \\
&=\frac{\frac{\Pa^{k-1}}{\Pa u_1 ... \Pa u_{k-1}} C_{1:k}(u_1,...,u_k)}{\frac{\Pa^{k-1}}{\Pa u_1 ... \Pa u_{k-1}} C_{1:k-1}(u_1,...,u_k-1)}
\end{align*}
\end{lemma}
\begin{proof}
See \autoref{ch:statistical-models:th:partialDifferentialCopulaGivesConditionalDistributionForUniformRandomVariables} for how partial derivative is associated with conditional distribution.	
\end{proof}


\begin{lemma}[generate correlated uniform random variables with Gaussian copula correlation structure]
	Given a Gaussian copula $C$ characterized by correlation matrix  $\Sigma$. 	
	We can generate random vector $(X_1,X_2,...,X_n)$ with uniform distribution and Gaussian copula correlation structure  using the following procedures:
	\begin{itemize}
		\item First generate $(Y_1,Y_2,...,Y_n) \sim MN(0,\Sigma)$.
		\item Then transform $X_1 = \phi(Y_1),X_2 = \phi(Y_2),...,X_n = \phi(Y_n)$, where $\phi$ is standard normal cdf.
	\end{itemize}

\end{lemma}
\begin{proof}
	Note that $Y_1,Y_2$ alone is standard normal variable. Therefore $\phi(Y_1)$ and $\phi(Y_2)$ are uniform random variables(\autoref{ch:statistical-models:th:probabilityintegraltransform}). 
	To show $C$ is the copula associated with the cdf $F$, we have
	\begin{align*}
	F(x_1,x_2) &= Pr(X_1\leq x_1, X_2\leq x_2) \\
	&= Pr(\phi(Y_1) \leq x_1, \phi(Y_2) \leq x_2) \\
	&=Pr(\phi(Y_1) \leq x_1, \phi(Y_2) \leq x_2) \\
	&=Pr(Y_1 \leq \phi^{-1}(x_1),Y_2 \leq \phi^{-1}(x_2)) \\
	&=\Phi(\phi^{-1}(x_1),\phi^{-1}(x_2)) \\
	&= C(x_1,x_2)
	\end{align*}
	where $C(u_1,u_2) = \Phi(\phi^{-1}(u_1),\phi^{-1}(u_2))$,$\Phi$ is the cdf for a multivariate normal distribution with zero mean and covariance matrix $\Sigma$, $\phi$ is the cdf for a standard normal variable. Note that the copula of a uniform distribution is itself(\autoref{ch:statistical-models:th:CopulaOfUniformDistribution}).
\end{proof}

\begin{lemma}[generate correlated uniform random variables with t copula correlation structure]\cite[420]{roncalli2016lecture}
Suppose we have n marginal distribution $F_i:\R\to [0,1], i=1,2,...,n$. Given a $T$ copula $C$ characterized by correlation matrix  $\Sigma$ and degree of freedom $v$. 	
We can generate samples of random vector $(X_1,X_2, ...,X_n)$ characterized by margins $F_1,F_2,...,F_n$ and copula $C$ using the following procedures:
\begin{itemize}
	\item Generate $Z_1,Z_2,...,Z_n$ as iid $N(0,1)$, and let $Z = (Z_1,Z_2,...,Z_n).$
	\item Generate a random $W\sim \chi^2(n)$ independent of $Z$.
	\item Return $X=\sqrt{\frac{v}{W}}CZ$, where $C$ is the Cholesky decomposition of $\Sigma$ such that $\Sigma = CC^T$.
	\item Return $U_i = T_v(X_i),i=1,2,...,n$, where $T_v$ is the univariate student $t$ distribution with $v$ degrees of freedom.
\end{itemize}	
\end{lemma}
\begin{proof}
From 	
\autoref{ch:MonteCarlo-methods--optimization:th:generateMultivariateStudentTRandomVector}, we discuss how to generate multivariate $T$ distribution.
\end{proof}

\subsubsection{Generating correlated random number with Gaussian copula}


\begin{theorem}[generate pair correlated random variables with Gaussian copula correlation structure]
	Suppose we have two univariate marginal distribution $F_1:\R\to [0,1]$ and $F_2:\R\to [0,1]$. Given a Gaussian copula $C$ characterized by correlation matrix  $\Sigma$. 	
	We can generate samples of random vector $(X_1,X_2)$  characterized by margins $F_1,F_2$ and copula $C$ using the following procedures:
	\begin{itemize}
		\item First generate $(Y_1,Y_2) \sim MN(0,\Sigma)$.
		\item Then transform $X_1 = F_1^{-1}(\phi(Y_1)),X_2 = F_2^{-1}(\phi(Y_2))$, where $\phi$ is standard normal cdf.
		\item The random vector $(X_1,X_2)$ has marginal distribution  $(F_1,F_2)$ and cdf
		$$F = C(F_1,F_2).$$
		
	\end{itemize}
\end{theorem}
\begin{proof}
	(1)Note that $Y_1,Y_2$ alone is standard normal variable. Therefore $\phi(Y_1)$ and $\phi(Y_2)$ are uniform random variables(\autoref{ch:statistical-models:th:probabilityintegraltransform}). Therefore $X_1$	and $X_2$ have marginal distribution $(F_1,F_2)$.
	To show $C$ is the copula associated with the cdf $F$, we have
	\begin{align*}
	F(x_1,x_2) &= Pr(X_1\leq x_1, X_2\leq x_2) \\
	&= Pr(F_1^{-1}(\phi(Y_1)) \leq x_1, F_2^{-1}(\phi(Y_2)) \leq x_2) \\
	&=Pr(\phi(Y_1) \leq F_1(x_1), \phi(Y_2) \leq F_2(x_2)) \\
	&=Pr(Y_1 \leq \phi^{-1}(F_1(x_1)),Y_2 \leq \phi^{-1}(F_2(x_2))) \\
	&=\Phi(\phi^{-1}(F_1(x_1)),\phi^{-1}(F_2(x_2))) \\
	&= C(F_1(x_1),F_2(x_2))
	\end{align*}
	where $C(u_1,u_2) = \Phi(\phi^{-1}(u_1),\phi^{-1}(u_2))$,$\Phi$ is the cdf for a multivariate normal distribution with zero mean and covariance matrix $R$, $\phi$ is the cdf for a standard normal variable.
	(2) Alternatively, we can use \autoref{ch:statistical-models:th:transformCorrelatedUniformToArbitraryDistributionWithSameCopula}.
\end{proof}



\begin{corollary}[generate correlated random variables with Gaussian copula correlation structure]\label{ch:statistical-models:th:GenerateCorrelatedRandomNumberWithGaussianCopula}
	Suppose we have n marginal distribution $F_i:\R\to [0,1], i=1,2,...,n$. Given a Gaussian copula $C$ characterized by correlation matrix  $\Sigma$. 	
	We can generate samples of random vector $(X_1,X_2, ...,X_n)$ characterized by margins $F_1,F_2,...,F_n$ and copula $C$ using the following procedures:
	\begin{itemize}
		\item First generate $(Y_1,Y_2,...,Y_n) \sim MN(0,\Sigma)$.
		\item Then transform $X_1 = F_1^{-1}(\phi(Y_1)),X_2 = F_2^{-1}(\phi(Y_2)), ...,X_n  = F_n^{-1}(\phi(Y_n)),$, where $\phi$ is standard normal cdf.
		\item The random vector $(X_1,X_2,...,X_n)$ has marginal distribution  $(F_1,F_2,...,F_n)$ and cdf
		$$F = C(F_1,F_2,...,F_n).$$	
	\end{itemize}
\end{corollary}





\subsubsection{Generating correlated random number with t copula}



\begin{lemma}[generating correlated random number with t copula]\label{ch:statistical-models:th:GenerateCorrelatedRandomNumberWithTCopula}
	Suppose we have n marginal distribution $F_i:\R\to [0,1], i=1,2,...,n$. Given a $T$ copula $C$ characterized by correlation matrix  $\Sigma$ and degree of freedom $v$. 	
We can generate samples of random vector $(X_1,X_2, ...,X_n)$ characterized by margins $F_1,F_2,...,F_n$ and copula $C$ using the following procedures:
	\begin{itemize}
		\item Generate $Z_1,Z_2,...,Z_n$ as iid $N(0,1)$, and let $Z = (Z_1,Z_2,...,Z_n).$
		\item Generate a random $W\sim \chi^2(n)$ independent of $Z$.
		\item Return $X=\sqrt{\frac{v}{W}}CZ$, where $C$ is the Cholesky decomposition of $\Sigma$ such that $\Sigma = CC^T$.
		\item Set $U_i = T_v(X_i),i=1,2,...,n$, where $T_v$ is the univariate student $t$ distribution with $v$ degrees of freedom.
		\item Return $Y_i = F_i^{-1}(U_i),i=1,2,...,n$.
	\end{itemize}	
\end{lemma}
\begin{proof}
From 	
\autoref{ch:MonteCarlo-methods--optimization:th:generateMultivariateStudentTRandomVector}, we discuss how to generate multivariate $T$ distribution.
\end{proof}





\subsubsection{Generating correlated random number with general copula}

\begin{lemma}[transform correlated uniform distribution to arbitrary distribution with same copula structure]\label{ch:statistical-models:th:transformCorrelatedUniformToArbitraryDistributionWithSameCopula}
Let  $(U_1,U_2,...,U_n)$ be a uniform random vector with cdf $F$(or equivalently copula$C$(\autoref{ch:statistical-models:th:CopulaOfUniformDistribution})). Let $F_1,F_2,...,F_n$ be the marginal cdf of a target cdf $F^{target}$.  It follows that
the random vector $(F^{-1}_1(U_1),F^{-1}_2(U_2),...,F^{-1}_n(U_2))$ has marginal cdf $F_1,F_2,...,F_n$ and copula structure $C$; that is, the cdf of $(F^{-1}_1(U_1),F^{-1}_2(U_2),...,F^{-1}_n(U_2))$ is $F^{target}$.
\end{lemma}
\begin{proof}
Let $U_i\sim U(0,1)$, then
$$Pr(F^{-1}_i(u_i) < x_i) = Pr(u_i < F_i(x_i)) = F_i(x_i) ,$$
which implies that $F^{-1}_i(U_i)$ has marginal $F_i$.

The show the cdf of $(F^{-1}_1(U_1),F^{-1}_2(U_2),...,F^{-1}_n(U_2))$ has the same copula as $(U_1,U_2,...,U_n)$, we use the monotone transform invariance property of copula(\autoref{ch:statistical-models:th:copulasinvarianceundermonotonetransform}).
\end{proof}









\subsubsection{Multivariate distribution approximation with Gaussian copula}

\begin{theorem}
Given random variable $X_1,X_2,...,X_n$ with margins $F_{X_1},F_{X_2},...,F_{X_n}$ and pair-correlation matrix $R$. We can construct a multivariate distribution such that recovers the margins and correlation via
$$F(x_1,x_2,...,x_n) = \Phi(\phi^{-1}(F_1(x_1)),\phi^{-1}(F_2(x_2)),...,\phi^{-1}(F_n(x_n))),$$
$\Phi$ is the cdf for a multivariate normal distribution with zero mean and covariance matrix $R$, $\phi$ is the cdf for a standard normal variable.	
\end{theorem}
\begin{proof}
From the theorem of constructing multivariate distribution from margins(\autoref{ch:statistical-models:th:ConstructMultivariateDistributionFromCopulaAndMargins}), we have
	$$F(x_1,x_2,...,x_n) = C(F_1(x_1),F_2(x_2),...,F_d(x_d)).$$
Note that the Gaussian copula with correlation matrix $R$(\autoref{ch:statistical-models:def:GaussianCopula}) is given by
	$$C(u_1,u_2,...,u_d; R) = \Phi(\phi^{-1}(u_1),\phi^{-1}(u_2),...,\phi^{-1}(u_d)),$$
	where $u_1,u_2,...,u_d \in [0,1]$, $\Phi$ is the cdf for a multivariate normal distribution with zero mean and covariance matrix $R$, $\phi$ is the cdf for a standard normal variable. 
\end{proof}

\begin{remark}[implications]\hfill
\begin{itemize}
	\item To fully determine the multivariate distribution, we usually need all the margins and all cross-term moments. 
	\item With only margins and correlations given, we can construct a multivariate Gaussian distribution as an approximation.  
\end{itemize}	
	
\end{remark}



\section{Extreme value theory}
\subsection{Basic order statistics}
\subsubsection{Independent random variables}


\begin{lemma}[distribution of max and min for independent random variables having same distribution]\label{ch:statistical-models:th:distributionMaxMinofRandomVariableOfSameDistribution}
	Let $X$ be a random variable with cdf $F_X(x)$. Let $X_{1:n} = \min(X_1,...,X_n)$ and $X_{n:n} = \max(X_1,...,X_n)$, where $X_1,...,X_n$ are $n$ iid random sample of $X$. Then
	\begin{itemize}
		\item (cdf)			
		$$ F_{1:n}(x) = 1 - (1 - F_X(x))^{n}$$
		and
		$$ F_{n:n}(x) = (F_X(x))^{n}.$$
		\item (density)
			$$ f_{1:n}(x) = nf_X(x)(1 - F_X(x))^{n-1}$$
		and
		$$ f_{n:n}(x) = nf_X(x)(F_X(x))^{n-1}.$$
	\end{itemize}
\end{lemma}
\begin{proof}
	$$P(X_{1:n}\geq x) = (P(X_i\geq x))^n \implies 1 - F_{1:n}(x) = (1 - F_X(x))^n \implies f_{1:n}(x) = nf_X(x)(1 - F_X(x))^{n-1}$$
	and
	$$P(X_{n:n}\leq x) = (P(X_i\leq x))^n \implies  F_{n:n}(x) = (F_X(x))^n \implies f_{n:n}(x) = nf_X(x)(F_X(x))^{n-1}.$$
\end{proof}

\begin{lemma}[distribution of max and min for independent random variables having different distribution]\label{ch:statistical-models:th:distributionMaxMinofRandomVariableOfDifferentDistribution}
	Let $X_1,X_2,...,X_n$ be independent random variables with cdf $F_1(x),F_2(x),...,F_n(x)$. Let $X_{1:n} = \min(X_1,...,X_n)$ and $X_{n:n} = \max(X_1,...,X_n)$, where $X_1,...,X_n$ Then
	\begin{itemize}
		\item (cdf)			
		$$ F_{1:n}(x) = 1 - \prod_{i=1}^n(1 - F_i(x))$$
		and
		$$ F_{n:n}(x) = \prod_{i=1}^n (F_i(x)).$$
	\end{itemize}
\end{lemma}
\begin{proof}
	$$P(X_{1:n}\geq x) = \prod_{i=1}^n(P(X_i\geq x)) \implies 1 - F_{1:n}(x) = \prod_{i=1}^n(1 - F_i(x))$$
	and
	$$P(X_{n:n}\leq x) = \prod_{i=1}^n P(X_i\leq x) \implies  F_{n:n}(x) = \prod_{i=1}^n F_X(x).$$
\end{proof}


\begin{corollary}[order statistics of uniform random variables]
	Let $X$ be a uniform random variable at [0,1]. Let $X_{1:n} = \min(X_1,...,X_n)$ and $X_{n:n} = \max(X_1,...,X_n)$, where $X_1,...,X_n$ are $n$ iid random sample of $X$. Then
	\begin{itemize}
		\item $$F_{1:n}(x) = (1-x)^n, F_{n:n}(x) = x.$$
		\item $$ f_{1:n}(x) = nf_X(x)(1 - F_X(x))^{n-1} = n(1-x)^{n-1}$$
		\item $$ f_{,:n}(x) = nf_X(x)(F_X(x))^{n-1} = nx^{n-1}$$
		\item $$f_j = \frac{n!}{(j-1)!(n-j)!} [x]^{j-1}[1-x]^{n-j} = Beta(j,n-j+1)$$
	\end{itemize}
\end{corollary}
\begin{proof}
	Note that we use the fact that for $U(0,1)$ distribution, $F_X(x) = x, f_X(x) = 1$.
\end{proof}




\subsubsection{Dependent random variables}

\begin{lemma}[distribution of max and min for dependent random samples]\cite[446]{roncalli2016lecture}
	Let $X$ be a random variable with cdf $F_X(x)$. Let $X_{1:n} = \min(X_1,...,X_n)$ and $X_{n:n} = \max(X_1,...,X_n)$, where $X_1,...,X_n$ have joint distribution $F$ and margins $F_1,F_2,...,F_n$. Then
	\begin{itemize}
		\item 			
		$$ F_{1:n}(x) = 1 - C^s(1-F_1(x),1-F_2(x),...,1-F_n(x)).$$
		or equivalently,
		$$ F_{1:n}(x) = 1 - C^s(S_1(x),S_2(x),...,S_n(x)) .$$
		where $S_i$ and $C^s$ are survival function and survival copula.
		
		\item 
		
		$$ F_{n:n}(x) = C(F_1(x),F_2(x),...,F_n(x)).$$
		\end{itemize}
\end{lemma}
\begin{proof}
(1)
\begin{align*}
F_{n:n}(x) &= Pr(X_{n:n} \leq x) \\
&= Pr(X_1 \leq x, X_2\leq x,...,X_n \leq x) \\
&= F(x,x,...,x) \\
&= C(F_1(x),F_2(x),...,F_n(x))
\end{align*}	
(2)	
\begin{align*}
1-F_{1:n}(x) &= 1 - Pr(X_{1:n} \leq x) \\
&= Pr(X_{1:n} \geq x) \\
&= Pr(X_1 \geq x, X_2\geq x,...,X_n \geq x) \\
&= C(1-F_1(x),1-F_2(x),...,1-F_n(x)) \\
&= C^s(S_1(x),S_2(x),...,S_n(x)) 
\end{align*}
\end{proof}

\begin{remark}[reduction to independent case]
The independence copula(\autoref{ch:statistical-models:sec:commonCopulaFunction}) is given by $C(u_1,u_2,...,u_n) = \prod_{i=1}^n u_i$. Then
$$F_{n:n}(x) = (1-x)^n,,F_{n:n}(x) = x^n. $$
 	
\end{remark}



\subsection{Univariate extreme value theory}



\begin{note}[limit distributions of maximum and minimums among iid samples]\cite[442]{roncalli2016lecture}
Note that
\begin{itemize}
	\item $$\lim_{n\to \infty} F_{1:n}(x) = \lim_{n\to \infty} 1 - (1 - F(x))^n= \begin{cases*}
	0, if ~ F(x) = 0\\
	1, if ~ F(x) > 0
	\end{cases*}$$
	\item $$\lim_{n\to \infty} F_{n:n}(x) = \lim_{n\to \infty} 1 - (1 - F(x))^n= \begin{cases*}
	0, if ~ F(x) < 1\\
	1, if ~ F(x) = 1
	\end{cases*}$$	
	
\end{itemize}
	



\end{note}


\section{Graphical models}\label{ch:theory-of-statistics:sec:graphical-models}

\begin{remark}[motivation]\hfill
\begin{itemize}
	\item Graphs are an intuitive way of representing and visualizing the relationships between many variables. 
	\item A graph can help extract the conditional independence relationships among random variables. Thus we can answer questions like "Is A independent from B given that we know the value of C?"
\end{itemize}	
\end{remark}


\begin{definition}[Graph terminology]\cite[309]{murphy2012machine}
	\begin{itemize}
		\item A graph $G = (\cV,\cE)$ consists of a set of nodes or vertices $\cV=\{1,2,...,V\}$, and a set of edges $\cE=\{(s,t):s,t\in \cV \}$. The connectivity between nodes can be represented by a matrix $G$, where $G(s,t) = 1$ if $(s,t)\in \cE$.
		\item directed acyclic graph(DAG) is a directed graph with no directed cycles.
		\item Parent of a node: $Pa(s) = \{t|G(t,s)=1\}$
		\item Child of a node: $ch(s) = \{t|G(s,t)=1\}$
		\item ancestors of a node: $anc(t)$ is the set of nodes $s$ that has a directed path from $s$ to $t$.
		\item descendants of a node: $anc(t)$ is the set of nodes $s$ that has a directed path from $t$ to $s$.
	\end{itemize}
\end{definition}

\begin{lemma}[joint distribution decomposition,chain rule]\index{chain rule}\label{ch:theory-of-statistics:th:jointdistributionchainrule}
	Let $P$ be the joint distribution on random variables $X_1,X_2,...,X_V$, then we can decompose the joint distribution as
	$$P(X_{1:V}) = P(X_1)P(X_2|X_1)P(X_3|X_{1:2}...P(X_V|X_{1:V-1}) = \prod_{i=1}^V P(X_i|X_{1:i-1})$$
	This decomposition still holds if we permute the index of the $X_i$.
\end{lemma}
\begin{proof}
	Use the definition of conditional distribution $$P(X,Y) = P(X)P(Y|X)$$.
\end{proof}

\subsection{Fundamentals}

\begin{definition}[graphical model]\index{graphical model}
	\cite[308]{murphy2012machine}A graphical model is a way to represent a joint distribution with conditional independence assumptions between random variables implicitly encoded. Specifically, the nodes in the graph represent random variable, and the lack of edges represent conditional independence assumptions.
\end{definition}

\begin{definition}[set of independences]
	\cite[60]{koller2009probabilistic}\cite[324]{murphy2012machine}Let $P$ be a distribution over a set of random variables $\cX$. Random variables $X$ and $Y$ are conditional independent given $Z$, denoted as $X\perp Y | Z$, if
	$$P(X,Y|Z) = P(X|Z)P(Y|Z$$
	the set of all such conditional independencies is denoted as $I(P)$.
\end{definition}

\begin{definition}[Independence map,I-map]
	\cite[60]{koller2009probabilistic}\cite[324]{murphy2012machine} A graph model $G$ is an independence map for a joint distribution $P$ if $$I(G)\subseteq I(P)$$
	where $I(G)$ the set of all conditional independence assumptions encoded by $G$.
\end{definition}

\begin{lemma}[fully connected graph]
	A fully connected graph $G$ is the I-map for all distributions $P$ defined over the same random variables. 
\end{lemma}
\begin{proof}
	$I(G) = \emptyset$
\end{proof}


\begin{definition}[minimal I-map]
	We say a graph model $G$ is the minimal I-map for joint distribution $P$ if $I(G)\subseteq I(P)$ and if $G'\subset G$ implies $G'$ is not I-map of $P$.
\end{definition}


\begin{definition}[directed graphical model]\index{directed graphical model}
	\cite[309]{murphy2012machine}
	A directed graphical model $G=(\cV,\cE)$ is a type of graphical model whose graph is a directed acyclic graph(DAG).And the encoded assumptions are: for each $s\in V, X_s \perp X_{ance(s)}\\pa(s)|pa(s)$
	that is, each node is conditional independent of all its ancestor nodes except the parent nodes when conditioned on the parent nodes of $s$.
\end{definition}

\begin{remark}
	Directed graphical model is also called \textbf{Bayes network}(the parameter as a random variable can be represented by a node), \textbf{brief network}, and \textbf{causal network}(because the directed arrows can be interpreted as causal relations.)
\end{remark}


\begin{definition}[d-separation]\cite[324]{murphy2012machine}
In a directed acyclic graphic model, a set of nodes	
$\cV$ d-separates $X$ from $Y$ if \textbf{every undirected path} between $X$ and $Y$ is blocked by $\cV$. A path is blocked by $\cV$ if there is a node $W$ on the path such that either
\begin{itemize}
	\item $W$ has converging arrows along the path $(\rightarrow W \leftarrow)$ and neither $W$ nor its descendants are in $\cV$.
	\item $W$ does not have converging arrows along the path $(\rightarrow W \rightarrow)$ or $\leftarrow W \rightarrow$ and $W\in \cV$.
\end{itemize}	
\end{definition}


\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../figures/statisticalModeling/d-separationScheme}
		\caption{$X$ and $Y$ are d-separated by $W$}
	\end{subfigure}\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../figures/statisticalModeling/d-separationSchemeVstructure}
		\caption{$X$ and $Y$ are not d-separated by $W$ and $Z$.}
	\end{subfigure}
	\caption{d-separation examples.}
	\label{ch:statistical-models:fig:d-SeparationExamples}
\end{figure}


\begin{theorem}[d-sepration and conditional independence]
In a directed acyclic graphic model, $X$ is conditional independent from $Y$ given $\cV$ if $\cV$ d-separates $X$ from $Y$.	
\end{theorem}





\begin{theorem}[factorization theorem]\index{factorization theorem}
	If graph $G$ is an I-map of $P$, then
	$$P(X_1...X_n) = \prod_{i}^n P(X_i|Pa(X_i))$$
\end{theorem}
\begin{proof}
	by the chain rule lemma, we have
	$$P(X_1...X_n) = \prod_{i}^n P(X_i|X_1...X_i))$$
\end{proof}


\begin{example}	Consider the following graph models in ch:statistical-models:fig:graphModelExamples.
\begin{itemize}
	\item  For graphical model (A), we can have the following factorization
	$$P(A,B,C,D,E) = P(A)P(B)P(C|A,B)P(D|B,C)P(E|C,D).$$
	\item Consider the graphical model (B). A data set of $N$ points are generated iid from a Gaussian distribution with parameters $\mu$ and $\sigma$. The joint probability is given by
	$$P(X_1,X_2,...,X_N, \mu,\sigma) = P(\mu)P(\sigma) \prod_{n=1}^{N}P(X_n|\mu,\sigma).$$
	\item Consider the graphical model (C) representing a Markov chain.  The joint probability can be decomposed by
		$$P(X_1,X_2,...,X_N) = P(X_N|X_{N-1})P(X_{N-1}|X_{N-2})\cdots P(X_1).$$
	\item Consider the graphical model (D) representing a second-order Markov chain.  The joint probability can be decomposed by
	\begin{align*}
	&P(X_1,X_2,...,X_N) \\
	&= P(X_N|X_{N-1},X_{N-2})P(X_{N-1}|X_{N-2},X_{N-3})\cdots P(X_3|X_1,X_2)P(X_1)P(X_2).
	\end{align*}
	\item Consider the graphical model (E) representing a hidden Markov chain.  The joint probability can be decomposed by
	\begin{align*}
	&P(X_1,...,X_N,Z_1,...,Z_N) \\
	&= P(Z_{N}|Z_{N-1})P(Z_{N-1}|Z_{N-2})\cdots P(Z_1) \prod_{n=1}^N P(X_i|Z_i).
	\end{align*}
\end{itemize}	
\end{example}



\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../figures/statisticalModeling/graphModelExample}
		\caption{Graphicial model A.}
	\end{subfigure}\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{../figures/statisticalModeling/graphModelExample2}
		\caption{Graphicial model B. $\mu,\sigma$ are model parameters.}
	\end{subfigure}\quad
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{../figures/statisticalModeling/graphModelExample3}
			\caption{Graphical model C for a Markov chain model.}
		\end{subfigure}\\
		\begin{subfigure}[b]{0.5\textwidth}
			\includegraphics[width=\textwidth]{../figures/statisticalModeling/graphModelExample4}
			\caption{Graphical model D for a second-order Markov chain model.}
		\end{subfigure}\\
		\begin{subfigure}[b]{0.45\textwidth}
			\includegraphics[width=\textwidth]{../figures/statisticalModeling/graphModelExample5}
			\caption{Graphical model E for a hidden Markov chain model.}
		\end{subfigure}
	\caption{Graphical model examples.}
	\label{ch:statistical-models:fig:graphModelExamples}
\end{figure}


\begin{example}[efficient inference via factorization]
Consider a joint distribution on random variables $A,B,C,D,E$ has the following factorization given by
	$$P(A,B,C,D,E) = P(A)P(B)P(C|A,B)P(D|B,C)P(E|C,D).$$

To calculate $$P(A|C=c) = \frac{P(A,C=c)}{P(C=c)},$$
we have
\begin{align*}
P(A,C=c) &= \sum_{B,D,E} P(A)P(B)P(C=c|A,B)P(D|B,C=c)P(E|C=c,D) \\
&=\sum_{B} P(A)P(B)P(C=c|A,B)\sum_{D}P(D|B,C=c)\sum_{E}P(E|C=c,D) \\
&=\sum_{B} P(A)P(B)P(C=c|A,B) 
\end{align*}		
	
\end{example}

\subsection{Undirected graphical model}


\begin{theorem}[Hammersley-Clifford theorem, fundamental theorem of random fields]\index{Hammersley-Clifford theorem}\index{fundamental theorem of random fields}
	A positive distribution $P >0$ satisfies the conditional independence of an undirected graph $G$ if and only if $P$ can be represented as a product of factors, one per maximal clique, such that
	$$p(\cX|\theta) = \frac{1}{Z(\theta)}\prod_{c\in \cC} \phi_c(\cX_c|\theta_c)$$
	where $\cC$ is the set of all the maximal cliques of $G$, and $Z(\theta)$ is the partition function given by 
	$$Z(\theta) = \int \prod_{c\in \cC}\phi_c(\cX_c|\theta_c) dP(\cX)$$
\end{theorem}


\begin{remark}
	This theorem guarantee that a distribution $P$ can be factored as distributions on cliques on the graph.
\end{remark}




see \cite{ankan2015mastering}
Risk Assessment and Decision Analysis with Bayesian Networks


\subsubsection{Correlation vs. causation}\index{correlation vs. causation}
\begin{remark}\hfill
	\begin{itemize}
		\item causation will imply correlation.
		\item correlation will \textbf{not} imply causation.
	\end{itemize}
	
For any two correlated events, A and B, the following relationships are possible:
\begin{itemize}
	\item A causes B; (direct causation)
	\item B causes A; (reverse causation)
	\item A and B are \textbf{consequences of a common cause of C}, but do not cause each other; graphically, we have $A\leftarrow C \rightarrow B$;
	\item A causes B and B causes A (bidirectional or cyclic causation);
	\item A causes C which causes B (indirect causation); graphically, we have $A\rightarrow C \rightarrow B$;
\end{itemize}	

	The Bayesian network assumes causation relations. Markov random field assumes correlation relations.	
\end{remark}






\section{Notes on Bibliography}

For linear regression models, see \cite{kutner2003applied}\cite{seber2012linear}. For, linear models with $R$ resources, see \cite{faraway2014linear}.


For multivariate statistical analysis, see \cite{johnson2007applied}\cite{anderson2009introduction}.

For copula, see \cite{Ruschendorf2013mathematical}\cite{lindskog2000modelling}\cite{mcneil2015quantitative}\cite{cherubini2004copula}.

\printbibliography
\end{refsection}
